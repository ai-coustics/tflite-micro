diff --git a/algo/common/include/xa_nn_common.h b/algo/common/include/xa_nn_common.h
index 5a45d27..4fa7215 100644
--- a/algo/common/include/xa_nn_common.h
+++ b/algo/common/include/xa_nn_common.h
@@ -64,8 +64,10 @@
 #define STRINGIZE(A) STRINGIZE_NX(A)    /*  Turn A into a string literal after macro-expanding it. */
 //#include STRINGIZE(PPCAT(cstub,XTENSA_CORE).h)
 //#include STRINGIZE(PPCAT(PPCAT(cstub,XTENSA_CORE),c.h))
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include "xtensa/tie/xt_hifi3.h"
 #include "xtensa/config/core-isa.h"
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 #endif
 
 //-----------------------------------------------------
@@ -89,6 +91,7 @@
 #define INV_TBL_BITS 7
 extern const int32_t tab_invQ30[128];
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #if XCHAL_HAVE_NSA
   #define NSA(n) XT_NSA(n)
 #else
@@ -100,6 +103,7 @@ extern const int32_t tab_invQ30[128];
     return AE_NSAQ56S(t)-8;
   }
 #endif
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 
 #ifdef COMPILER_XTENSA
   #define ATTRIBUTE_ALWAYS_INLINE __attribute__((always_inline))
@@ -123,11 +127,13 @@ extern const int32_t tab_invQ30[128];
 #define return_int64(x) {  union {ae_int64  ai;int64_t   i; } r; r.ai = x;  return r.i; }
 #endif
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #if  defined (__cplusplus) || defined(COMPILER_XTENSA)
 
 #else
 #error sorry, C compiler is not supported excluding the XCC
 #endif
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 
 
 #ifdef COMPILER_MSVC
diff --git a/algo/common/include/xa_nnlib_common.h b/algo/common/include/xa_nnlib_common.h
index fcf379a..d66f2a0 100644
--- a/algo/common/include/xa_nnlib_common.h
+++ b/algo/common/include/xa_nnlib_common.h
@@ -21,12 +21,16 @@
 ******************************************************************************/
 #ifndef __XA_NNLIB_LEGACY_COMPAT_H__
 #define __XA_NNLIB_LEGACY_COMPAT_H__
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include <xtensa/config/core-isa.h>
 #include "xtensa/tie/xt_hifi2.h"
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 #include "xa_nnlib_api.h"
 #include "xa_nnlib_standards.h"
 #include "xa_nnlib_err_chk.h"
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include "xa_nnlib_hifi_isa_compat.h"
+#endif
 #include "xa_nn_common.h"
 #include "xa_nnlib_common_internal.h"
 #endif /* __XA_NNLIB_LEGACY_COMPAT_H__ */
@@ -36,4 +40,4 @@
 #define XA_HAVE_HIFI3_CORE 1
 #else
 #define XA_HAVE_HIFI3_CORE 0
-#endif
\ No newline at end of file
+#endif
diff --git a/algo/common/include/xa_nnlib_common_macros_hifi5.h b/algo/common/include/xa_nnlib_common_macros_hifi5.h
index b2ac8e3..9ad396d 100644
--- a/algo/common/include/xa_nnlib_common_macros_hifi5.h
+++ b/algo/common/include/xa_nnlib_common_macros_hifi5.h
@@ -28,6 +28,8 @@
 #define NULL (void *)0
 #endif /* NULL */
 
+#define MAX(a, b)   (((a) > (b)) ? (a) : (b))
+
 /* Macros for memcpy */
 #define MEMCPY_8b(out, inp, N) \
 { \
diff --git a/algo/kernels/activations/hifi5/xa_nn_softmax_sym16s_16.c b/algo/kernels/activations/hifi5/xa_nn_softmax_sym16s_16.c
new file mode 100644
index 0000000..ae14ab3
--- /dev/null
+++ b/algo/kernels/activations/hifi5/xa_nn_softmax_sym16s_16.c
@@ -0,0 +1,428 @@
+/*******************************************************************************
+* Copyright (c) 2018-2024 Cadence Design Systems, Inc.
+*
+* Permission is hereby granted, free of charge, to any person obtaining
+* a copy of this software and associated documentation files (the
+* "Software"), to use this Software with Cadence processor cores only and
+* not with any other processors and platforms, subject to
+* the following conditions:
+*
+* The above copyright notice and this permission notice shall be included
+* in all copies or substantial portions of the Software.
+*
+* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+* IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+* SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+******************************************************************************/
+#include "xa_type_def.h"
+#include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common.h"
+#include "xa_nnlib_common_macros_hifi5.h"
+WORD16 exp_lut[513] = {
+      2,     2,     2,     2,     2,     2,     2,     2,
+      2,     2,     2,     2,     2,     2,     2,     2,
+      2,     2,     2,     2,     2,     2,     2,     2,
+      2,     2,     2,     3,     3,     3,     3,     3,
+      3,     3,     3,     3,     3,     3,     3,     3,
+      3,     3,     3,     3,     4,     4,     4,     4,
+      4,     4,     4,     4,     4,     4,     4,     4,
+      4,     5,     5,     5,     5,     5,     5,     5,
+      5,     5,     5,     6,     6,     6,     6,     6,
+      6,     6,     6,     7,     7,     7,     7,     7,
+      7,     7,     7,     8,     8,     8,     8,     8,
+      8,     9,     9,     9,     9,     9,     9,    10,
+     10,    10,    10,    10,    11,    11,    11,    11,
+     11,    12,    12,    12,    12,    13,    13,    13,
+     13,    14,    14,    14,    14,    15,    15,    15,
+     16,    16,    16,    17,    17,    17,    18,    18,
+     18,    19,    19,    19,    20,    20,    21,    21,
+     21,    22,    22,    23,    23,    24,    24,    25,
+     25,    26,    26,    27,    27,    28,    28,    29,
+     29,    30,    30,    31,    32,    32,    33,    34,
+     34,    35,    36,    36,    37,    37,    38,    39,
+     40,    40,    42,    42,    43,    44,    45,    45,
+     46,    47,    48,    49,    50,    51,    52,    53,
+     54,    55,    56,    57,    59,    60,    60,    62,
+     63,    65,    65,    67,    68,    69,    71,    73,
+     74,    75,    77,    78,    80,    81,    83,    85,
+     86,    88,    90,    92,    93,    95,    97,    99,
+    101,   103,   105,   107,   109,   112,   114,   116,
+    118,   121,   123,   126,   128,   131,   133,   135,
+    139,   141,   144,   147,   149,   152,   155,   158,
+    162,   165,   168,   171,   174,   178,   181,   185,
+    189,   192,   196,   200,   204,   208,   212,   217,
+    221,   225,   230,   234,   239,   243,   248,   253,
+    258,   263,   268,   273,   279,   284,   290,   296,
+    302,   308,   314,   320,   327,   333,   340,   346,
+    353,   360,   366,   374,   381,   389,   397,   404,
+    413,   421,   429,   437,   446,   455,   464,   473,
+    482,   492,   501,   511,   522,   532,   543,   553,
+    564,   575,   586,   598,   610,   622,   634,   646,
+    659,   672,   685,   699,   713,   727,   741,   756,
+    771,   786,   801,   817,   833,   850,   866,   884,
+    901,   919,   937,   955,   974,   993,  1013,  1033,
+   1053,  1074,  1095,  1117,  1139,  1161,  1184,  1207,
+   1232,  1256,  1281,  1306,  1332,  1358,  1385,  1412,
+   1440,  1468,  1497,  1527,  1557,  1587,  1619,  1651,
+   1683,  1716,  1750,  1785,  1820,  1856,  1892,  1930,
+   1968,  2006,  2046,  2087,  2128,  2170,  2212,  2256,
+   2300,  2346,  2392,  2439,  2488,  2537,  2587,  2638,
+   2690,  2743,  2796,  2852,  2908,  2966,  3024,  3084,
+   3145,  3207,  3270,  3334,  3400,  3467,  3535,  3605,
+   3677,  3749,  3822,  3898,  3975,  4053,  4133,  4214,
+   4297,  4383,  4469,  4557,  4647,  4739,  4833,  4927,
+   5024,  5124,  5225,  5328,  5433,  5541,  5649,  5761,
+   5875,  5991,  6109,  6230,  6352,  6477,  6605,  6736,
+   6868,  7004,  7141,  7282,  7427,  7572,  7722,  7874,
+   8030,  8188,  8350,  8514,  8683,  8854,  9028,  9206,
+   9387,  9572,  9762,  9954, 10151, 10351, 10555, 10763,
+  10976, 11191, 11412, 11637, 11867, 12102, 12341, 12583,
+  12831, 13085, 13342, 13606, 13874, 14148, 14427, 14711,
+  15002, 15297, 15599, 15907, 16221, 16541, 16867, 17199,
+  17539, 17884, 18237, 18597, 18964, 19338, 19719, 20108,
+  20505, 20909, 21322, 21742, 22171, 22608, 23054, 23509,
+  23973, 24445, 24928, 25419, 25921, 26432, 26953, 27485,
+  28027, 28580, 29143, 29718, 30304, 30902, 31512, 32133,
+  32767};
+
+WORD16 one_over_one_plus_x_lut[513] = {
+  32767, 32704, 32640, 32578, 32514, 32451, 32388, 32326,
+  32264, 32202, 32141, 32079, 32018, 31957, 31896, 31835,
+  31775, 31715, 31655, 31596, 31537, 31476, 31418, 31359,
+  31301, 31242, 31184, 31127, 31069, 31011, 30954, 30897,
+  30840, 30784, 30727, 30671, 30615, 30560, 30504, 30449,
+  30394, 30339, 30283, 30229, 30175, 30121, 30067, 30013,
+  29960, 29906, 29853, 29800, 29746, 29694, 29642, 29589,
+  29537, 29486, 29434, 29382, 29331, 29280, 29229, 29177,
+  29127, 29076, 29026, 28976, 28926, 28877, 28827, 28777,
+  28728, 28679, 28630, 28581, 28532, 28484, 28436, 28388,
+  28340, 28292, 28244, 28197, 28150, 28103, 28056, 28008,
+  27962, 27915, 27869, 27823, 27777, 27731, 27685, 27640,
+  27594, 27549, 27504, 27459, 27413, 27369, 27324, 27280,
+  27236, 27192, 27148, 27104, 27060, 27016, 26973, 26930,
+  26887, 26844, 26801, 26758, 26715, 26673, 26630, 26588,
+  26546, 26504, 26463, 26421, 26380, 26338, 26297, 26255,
+  26214, 26174, 26132, 26092, 26051, 26011, 25971, 25931,
+  25891, 25851, 25811, 25772, 25732, 25693, 25653, 25614,
+  25575, 25536, 25497, 25458, 25420, 25381, 25343, 25305,
+  25267, 25229, 25191, 25153, 25116, 25078, 25041, 25003,
+  24966, 24928, 24892, 24855, 24818, 24781, 24745, 24709,
+  24672, 24636, 24600, 24564, 24528, 24492, 24457, 24421,
+  24385, 24350, 24315, 24280, 24245, 24210, 24175, 24140,
+  24105, 24070, 24036, 24002, 23967, 23933, 23899, 23865,
+  23831, 23798, 23764, 23730, 23697, 23664, 23630, 23597,
+  23564, 23530, 23498, 23465, 23432, 23399, 23366, 23334,
+  23302, 23269, 23237, 23205, 23173, 23141, 23109, 23077,
+  23046, 23014, 22982, 22951, 22920, 22888, 22857, 22826,
+  22795, 22764, 22733, 22703, 22672, 22641, 22611, 22580,
+  22550, 22520, 22490, 22459, 22429, 22400, 22370, 22340,
+  22310, 22281, 22251, 22221, 22192, 22163, 22134, 22104,
+  22075, 22046, 22017, 21988, 21959, 21931, 21902, 21874,
+  21845, 21817, 21788, 21760, 21732, 21704, 21676, 21648,
+  21620, 21592, 21565, 21537, 21509, 21482, 21455, 21427,
+  21400, 21372, 21345, 21318, 21291, 21264, 21237, 21210,
+  21183, 21157, 21130, 21103, 21077, 21050, 21024, 20998,
+  20971, 20945, 20919, 20893, 20867, 20841, 20816, 20790,
+  20764, 20738, 20713, 20687, 20662, 20636, 20611, 20586,
+  20560, 20535, 20510, 20485, 20460, 20435, 20410, 20385,
+  20360, 20336, 20311, 20287, 20262, 20238, 20213, 20189,
+  20165, 20141, 20117, 20092, 20068, 20044, 20021, 19997,
+  19973, 19949, 19926, 19902, 19878, 19855, 19832, 19808,
+  19784, 19762, 19738, 19715, 19692, 19668, 19645, 19622,
+  19600, 19577, 19553, 19531, 19508, 19485, 19463, 19440,
+  19418, 19395, 19373, 19351, 19328, 19306, 19284, 19262,
+  19240, 19218, 19196, 19174, 19152, 19130, 19109, 19087,
+  19065, 19044, 19022, 19000, 18979, 18958, 18936, 18915,
+  18893, 18872, 18851, 18830, 18809, 18787, 18766, 18745,
+  18725, 18704, 18682, 18662, 18641, 18620, 18600, 18579,
+  18559, 18538, 18518, 18497, 18477, 18457, 18436, 18416,
+  18396, 18376, 18356, 18336, 18316, 18296, 18276, 18256,
+  18236, 18216, 18197, 18177, 18157, 18138, 18118, 18099,
+  18079, 18059, 18040, 18021, 18001, 17982, 17963, 17944,
+  17924, 17905, 17886, 17867, 17848, 17829, 17810, 17791,
+  17772, 17754, 17735, 17716, 17697, 17679, 17660, 17641,
+  17623, 17604, 17586, 17568, 17549, 17531, 17513, 17494,
+  17476, 17458, 17440, 17422, 17404, 17386, 17368, 17350,
+  17332, 17314, 17296, 17278, 17261, 17243, 17225, 17208,
+  17190, 17172, 17155, 17137, 17120, 17102, 17085, 17067,
+  17050, 17033, 17015, 16999, 16981, 16964, 16947, 16930,
+  16913, 16895, 16878, 16862, 16845, 16828, 16810, 16794,
+  16777, 16760, 16743, 16727, 16710, 16693, 16677, 16660,
+  16644, 16627, 16611, 16594, 16578, 16562, 16545, 16529,
+  16513, 16497, 16480, 16464, 16448, 16432, 16416, 16400,
+  16384};
+
+static inline ae_int16x4 LUTLookUpX4(ae_int16x4 value, WORD16* lut)
+{
+  ae_int16x4 shifted_value = AE_SRAI16(value, 7);
+  ae_int16x4 index = AE_ADD16S(AE_MOVDA16(256), shifted_value);
+  ae_int16x4 offset = AE_SLAI16S(AE_AND16(value, AE_MOVDA16(0x7f)),8);
+  WORD32 index0, index1, index2, index3;
+  index0 = AE_MOVAD16_3(index);
+  index1 = AE_MOVAD16_2(index);
+  index2 = AE_MOVAD16_1(index);
+  index3 = AE_MOVAD16_0(index);
+
+  ae_int16 *p_ae_lut = (ae_int16 *)lut;
+  ae_int16x4 base0123 = p_ae_lut[index0];
+  base0123 = AE_SEL16_6543(base0123, p_ae_lut[index1]);
+  base0123 = AE_SEL16_6543(base0123, p_ae_lut[index2]);
+  base0123 = AE_SEL16_6543(base0123, p_ae_lut[index3]);
+
+  ae_int16x4 slope0123 = p_ae_lut[index0 + 1];
+  slope0123 = AE_SEL16_6543(slope0123, p_ae_lut[index1 + 1]);
+  slope0123 = AE_SEL16_6543(slope0123, p_ae_lut[index2 + 1]);
+  slope0123 = AE_SEL16_6543(slope0123, p_ae_lut[index3 + 1]);
+  slope0123 = AE_SUB16S(slope0123, base0123);
+
+  ae_int16x4 delta0123;
+  delta0123 = AE_MULFP16X4RAS(slope0123, offset);
+  ae_int16x4 result0123 = AE_ADD16S(base0123, delta0123);
+
+  return result0123;
+}
+
+static inline ae_int16x4 LUTLookUp(ae_int16x4 value, WORD16* lut)
+{
+  ae_int16x4 shifted_value = AE_SRAI16(value, 7);
+  ae_int16x4 index = AE_ADD16S(AE_MOVDA16(256), shifted_value);
+  ae_int16x4 offset = AE_SLAI16S(AE_AND16(value, AE_MOVDA16(0x7f)),8);
+
+  WORD32 index0;
+  index0 = AE_MOVAD16_3(index);
+
+  ae_int16 *p_ae_lut = (ae_int16 *)lut;
+  ae_int16x4 base = p_ae_lut[index0];
+
+  ae_int16x4 slope = p_ae_lut[index0 + 1];
+  slope = AE_SUB16S(slope, base);
+
+  ae_int16x4 delta;
+  delta = AE_MULFP16X4RAS(slope, offset);
+
+  ae_int16x4 result = AE_ADD16S(base, delta);
+
+  return result;
+}
+
+// Computes exp(input - max_input)
+static inline ae_int16x4 softmaxCalculateExp(WORD32 input_beta_left_shift,
+                            WORD32 input_beta_multiplier,
+                            ae_int16x4 d_inp,
+                            ae_int16x4 max_in_row)
+{
+  ae_int32x2 input_diff1, input_diff2, scaled_diff1, scaled_diff2;
+  ae_int32x2 sym_scaled_diff1, sym_scaled_diff2;
+
+  AE_SUBW16(input_diff1, input_diff2, d_inp, max_in_row);
+
+  #if TFLITE_SINGLE_ROUNDING
+    int left_shift  = input_beta_left_shift;
+    int right_shift = input_beta_left_shift;
+    /* Single rounding macro doesn't need two shifts so this is not used */
+    (void)right_shift;
+  #else /* #if TFLITE_SINGLE_ROUNDING */
+    int left_shift  = input_beta_left_shift<0?0: input_beta_left_shift;
+    int right_shift = input_beta_left_shift>0?0:-input_beta_left_shift;
+  #endif /* #if TFLITE_SINGLE_ROUNDING */
+  MPY_BY_QUANT_MULT_SLS_X2X2_OUT32(scaled_diff1, scaled_diff2, input_diff1, input_diff2, input_beta_multiplier, left_shift, right_shift);
+  ae_int32x2 max_int16s = AE_MOVDA32(32767);
+
+  sym_scaled_diff1 = AE_ADD32S(scaled_diff1, max_int16s);
+  sym_scaled_diff2 = AE_ADD32S(scaled_diff2, max_int16s);
+  ae_int16x4 sat_sym_shifted_sum = AE_SAT16X4(sym_scaled_diff1,sym_scaled_diff2);
+
+  ae_int16x4 result = LUTLookUpX4(sat_sym_shifted_sum, exp_lut);
+
+  return result;
+}
+
+UWORD8 count_leading_zeros(ae_int32x2 integer_input)
+{
+  WORD32 value = AE_MOVDA32(integer_input);
+  if(value == 0)
+  {
+    return 32;
+  }
+  return AE_NSAZ32_L(integer_input) + 1;
+}
+
+WORD32 xa_nn_vec_softmax_sym16s_16( WORD16 * __restrict__ p_out,
+                    const   WORD16 * __restrict__ p_vec,
+                            WORD32  input_beta_left_shift,
+                            WORD32  input_beta_multiplier,
+                            WORD32  vec_length)
+{
+  /* NULL pointer checks */
+  XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_vec, -1);
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_out, sizeof(UWORD16), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_vec, sizeof(UWORD16), -1);
+  /* Basic Parameter checks */
+  XA_NNLIB_ARG_CHK_COND((vec_length <= 0), -1);
+  XA_NNLIB_ARG_CHK_COND(((input_beta_left_shift < -31) || (input_beta_left_shift > 31)), -1);
+  XA_NNLIB_ARG_CHK_COND((input_beta_multiplier < 0), -1);
+
+  // Calculating Max
+  ae_int16x4 max;
+  int i;
+  {
+    ae_int16x4 m0, m1, m2;
+    ae_int16x8 *p_inp = (ae_int16x8 *)p_vec;
+    ae_valignx2 align_input128 = AE_LA128_PP(p_inp);
+    m0 = AE_MOVDA16(0x8000);
+
+    for(i = 0; i < (vec_length >> 3); i++)
+    {
+      AE_LA16X4X2_IP(m1, m2, align_input128, p_inp);
+      m0 = AE_MAX16(m0, m1);
+      m0 = AE_MAX16(m0, m2);
+    }
+
+    ae_valign align_input64 = AE_LA64_PP(p_vec);
+    for(i = 0; i < ((vec_length & 7) >> 2); i++)
+    {
+      AE_LA16X4_IP(m1, align_input64, (ae_int16x4 *)p_inp);
+      m0 = AE_MAX16(m0, m1);
+    }
+
+    for(i = 0; i < (vec_length & 3); i++)
+    {
+      AE_L16_IP(m1, (ae_int16 *)p_inp, sizeof(ae_int16));
+      m0 = AE_MAX16(m0, m1);
+    }
+
+    if(vec_length < 4)
+    {
+      max = AE_MOVDA16((AE_MOVAD16_0(m0)));
+    }
+    else
+    {
+      ae_int32x2 temp1, temp2;
+      AE_CVTI32X4F16(temp1, temp2, m0, 0);
+      temp2 = AE_MAX32(temp1, temp2);
+
+      temp1 = AE_SEL32_LH(temp2, temp2);
+      temp1 = AE_MAX32(temp1, temp2);
+
+      max = AE_MOVDA16((AE_MOVAD32_L(temp1)));
+    }
+  }
+
+  //Compute exp and sum_of_exp
+  ae_int32x2 sum_of_exps;
+  {
+    ae_int16x4 *temp_out = (ae_int16x4 *)p_out;
+    ae_int16x4 *p_inp = (ae_int16x4 *)p_vec;
+    ae_int16x4 d_inp1, d_inp2;
+    ae_valign align_input64 = AE_LA64_PP(p_inp);
+    ae_int32x2 acc1, acc2, acc;
+    acc1 = AE_MOVDA32(0);
+    acc2 = AE_MOVDA32(0);
+    ae_valign align_output64 = AE_ZALIGN64();
+    ae_int16x4 exp1, exp2;
+    exp2 = AE_MOVDA16(0);
+    for(i = 0; i < (vec_length >> 2); i++)
+    {
+      AE_LA16X4_IP(d_inp1, align_input64, p_inp);
+      exp1 = softmaxCalculateExp(input_beta_left_shift, input_beta_multiplier, d_inp1, max);
+      AE_SA16X4_IP(exp1, align_output64, temp_out);
+      AE_ACCW16(acc1, acc2, exp1, exp2);
+    }
+    AE_SA64POS_FP(align_output64,(void *)temp_out);
+
+    int rem_length = vec_length & 3;
+    if(rem_length)
+    {
+      ae_valignx2 align_input128 = AE_LA128_PP((ae_int16x8 *)p_inp);
+      ae_valignx2 align_output128 = AE_ZALIGN128();
+      AE_LAV16X4X2_XP(d_inp1, d_inp2, align_input128, (ae_int16x8 *)p_inp, rem_length * 2);
+      exp1 = softmaxCalculateExp(input_beta_left_shift, input_beta_multiplier, d_inp1, max);
+
+      ae_int64 mask = AE_MOVF64_FROMF32X2(AE_MOVDA32(-1));
+      mask = AE_SLAA64(mask, 16 * (4 - rem_length));
+      ae_int16x4 mask16x4 = AE_MOVF16X4_FROMF64(mask);
+
+      exp1 = AE_AND16(exp1, mask16x4);
+      AE_ACCW16(acc1, acc2, exp1, d_inp2);
+      AE_SAV16X4X2_XP(exp1, d_inp2, align_output128, (ae_int16x8 *)temp_out, rem_length * 2);
+      AE_SA128POS_FP(align_output128,(void *)temp_out);
+    }
+
+    acc = AE_ADD32S(acc1, acc2);
+    acc1 = AE_MOVDA32(AE_MOVAD32_H(acc));
+    acc2 = AE_MOVDA32(AE_MOVAD32_L(acc));
+    sum_of_exps = AE_ADD32S(acc1, acc2);
+  }
+
+  // Calculate 1/sum_of_exps
+  UWORD8 headroom_plus_one = count_leading_zeros(sum_of_exps);
+  ae_int32x2 shifted_sum = AE_SRAA32RS(sum_of_exps, 14 - (headroom_plus_one - 1));
+  ae_int32x2 plus_one_sym = AE_MOVDA32(-((1<<15) + (1<<16)));
+  ae_int32x2 sym_shifted_sum = AE_ADD32S(shifted_sum, plus_one_sym);
+  ae_int16x4 sat_sym_shifted_sum = AE_SAT16X4(sym_shifted_sum, sym_shifted_sum);
+  ae_int16x4 reciprocal_scale_q015 = LUTLookUp(sat_sym_shifted_sum, one_over_one_plus_x_lut);
+
+  // Compute exp*1/sum_of_exps
+  {
+    ae_int16x8 *temp_out1 = (ae_int16x8 *)p_out;
+    WORD32 right_shift = 31 - headroom_plus_one;
+    ae_int16x4 exp1, exp2;
+    ae_valignx2 exp_aligner = AE_LA128_PP(temp_out1);
+    ae_int32x2 sfmx1, sfmx2, sfmx3, sfmx4;
+    ae_int32x2 shifted_sfmx1, shifted_sfmx2, shifted_sfmx3, shifted_sfmx4;
+    ae_int16x4 sfmx12, sfmx34;
+    ae_int16x8 *temp_out2 = (ae_int16x8 *)p_out;
+    ae_valignx2 align_output128 = AE_ZALIGN128();
+    ae_int16x4 zero = AE_MOVDA16(0);
+    for(i=0; i<(vec_length>>3); i++)
+    {
+      AE_LA16X4X2_IP(exp1, exp2, exp_aligner, temp_out1);
+      AE_MUL16X4S(sfmx1, sfmx2, exp1, reciprocal_scale_q015);
+      AE_MUL16X4S(sfmx3, sfmx4, exp2, reciprocal_scale_q015);
+      shifted_sfmx1 = AE_SRAA32RS(sfmx1, right_shift);
+      shifted_sfmx2 = AE_SRAA32RS(sfmx2, right_shift);
+      shifted_sfmx3 = AE_SRAA32RS(sfmx3, right_shift);
+      shifted_sfmx4 = AE_SRAA32RS(sfmx4, right_shift);
+      sfmx12 = AE_SAT16X4(shifted_sfmx1, shifted_sfmx2);
+      sfmx34 = AE_SAT16X4(shifted_sfmx3, shifted_sfmx4);
+      sfmx12 = AE_MAX16(sfmx12, zero);
+      sfmx34 = AE_MAX16(sfmx34, zero);
+      AE_SA16X4X2_IP(sfmx12, sfmx34, align_output128, temp_out2);
+    }
+    int rem_length = vec_length & 7;
+    if(rem_length)
+    {
+      AE_LAV16X4X2_XP(exp1, exp2, exp_aligner, temp_out1, rem_length * 2);
+      AE_MUL16X4S(sfmx1, sfmx2, exp1, reciprocal_scale_q015);
+      shifted_sfmx1 = AE_SRAA32RS(sfmx1, right_shift);
+      shifted_sfmx2 = AE_SRAA32RS(sfmx2, right_shift);
+
+      sfmx12 = AE_SAT16X4(shifted_sfmx1, shifted_sfmx2);
+      sfmx12 = AE_MAX16(sfmx12, zero);
+
+      if(rem_length > 4)
+      {
+        AE_MUL16X4S(sfmx3, sfmx4, exp2, reciprocal_scale_q015);
+        shifted_sfmx3 = AE_SRAA32RS(sfmx3, right_shift);
+        shifted_sfmx4 = AE_SRAA32RS(sfmx4, right_shift);
+        sfmx34 = AE_SAT16X4(shifted_sfmx3, shifted_sfmx4);
+        sfmx34 = AE_MAX16(sfmx34, zero);
+        AE_SAV16X4X2_XP(sfmx12, sfmx34, align_output128, temp_out2, rem_length * 2);
+      }
+      else
+      {
+        AE_SAV16X4X2_XP(sfmx12, exp2, align_output128, temp_out2, rem_length * 2);
+      }
+    }
+    AE_SA128POS_FP(align_output128,(void *)temp_out2);
+  }
+
+  return 0;
+}
diff --git a/algo/kernels/basic/hifi5/xa_nn_reduce_asym8s_asym8s.c b/algo/kernels/basic/hifi5/xa_nn_reduce_asym8s_asym8s.c
index 6fdcbe5..c079731 100644
--- a/algo/kernels/basic/hifi5/xa_nn_reduce_asym8s_asym8s.c
+++ b/algo/kernels/basic/hifi5/xa_nn_reduce_asym8s_asym8s.c
@@ -80,6 +80,7 @@ WORD32 xa_nn_reduce_getsize_nhwc(WORD32 inp_precision
     return 0;
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 /*
  * Currently only supports upto 4D input tensors.
  * 1/2/3 D input tensors will be scaled up to 4D.
@@ -1512,3 +1513,4 @@ WORD32 xa_nn_reduce_mean_4D_asym8s_asym8s(WORD8 * __restrict__ p_out
 
   return 0;
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
diff --git a/algo/kernels/cnn/hifi5/xa_nn_circ_buf.c b/algo/kernels/cnn/hifi5/xa_nn_circ_buf.c
index 7b49a65..2ce9e25 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_circ_buf.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_circ_buf.c
@@ -45,7 +45,7 @@ int xa_nn_circ_buf_nchw_getsize(
   }
 
   circ_buf_width = kernel_width + ((output_width - 1) * x_stride);
-  circ_buf_width = XT_MAX(circ_buf_width, x_padding + input_width);
+  circ_buf_width = MAX(circ_buf_width, x_padding + input_width);
 
   /* Align circ_buf_width to 8 for 8-bit and 16-bit inputs to make L8X8 and
   L16X4X2 loads possible */
@@ -349,7 +349,7 @@ int xa_nn_circ_buf_nhwc_getsize(
   int size_in_bytes;
 
   circ_buf_height = kernel_height + ((output_height - 1) * y_stride);
-  circ_buf_height = XT_MAX(circ_buf_height, (y_padding + input_height + dilation_height - 1)/dilation_height);
+  circ_buf_height = MAX(circ_buf_height, (y_padding + input_height + dilation_height - 1)/dilation_height);
 
   if(bytewidth == 4)
   {
diff --git a/algo/kernels/cnn/hifi5/xa_nn_conv2d_depthwise.c b/algo/kernels/cnn/hifi5/xa_nn_conv2d_depthwise.c
index e3ea1be..a377a41 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_conv2d_depthwise.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_conv2d_depthwise.c
@@ -77,7 +77,7 @@ static WORD32 xa_nn_dilated_conv2d_depthwise_nchw_getsize
   circ_buf_size = ALIGNED_SIZE(circ_buf_size, ALIGNMENT_16);
 
   circ_buf_width = dilated_kernel_width + ((output_width - 1) * x_stride);
-  circ_buf_width = XT_MAX(circ_buf_width, x_padding+input_width);
+  circ_buf_width = MAX(circ_buf_width, x_padding+input_width);
   if(circ_buf_bytewidth == 1 || circ_buf_bytewidth == 2)
     circ_buf_width = ALIGNED_SIZE(circ_buf_width, 8);
   else
diff --git a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_circ_buf.c b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_circ_buf.c
index 1135ba0..2c2cd33 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_circ_buf.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_circ_buf.c
@@ -111,6 +111,7 @@ WORD32 xa_nn_transpose_conv_getsize
     return total_size;
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 VOID xa_nn_transpose_conv_init_state(
     VOID *p_scratch,
     VOID *p_kernel,
@@ -165,3 +166,5 @@ VOID xa_nn_transpose_conv_init_state(
   AE_SETCEND0(p_state->cir_buf.p_end);
 
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
+
diff --git a/algo/kernels/pool/hifi5/xa_nn_avgpool.c b/algo/kernels/pool/hifi5/xa_nn_avgpool.c
index fa2bc14..b050dc9 100644
--- a/algo/kernels/pool/hifi5/xa_nn_avgpool.c
+++ b/algo/kernels/pool/hifi5/xa_nn_avgpool.c
@@ -24,6 +24,7 @@
 #include "xa_nnlib_kernels_api.h"
 #include "xa_nn_avgpool_state.h"
 #include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common_macros_hifi5.h"
 
 WORD32 xa_nn_avgpool_getsize_nchw(
     WORD32 inp_precision,
@@ -86,7 +87,7 @@ WORD32 xa_nn_avgpool_getsize_nchw(
         den_array_size = 0;
     /* Output scratch buffer size */
     full_buf_width = kernel_width + (out_width - 1)*x_stride;
-    full_buf_width = XT_MAX(full_buf_width, ALIGNED_SIZE(x_padding, 2)+input_width);
+    full_buf_width = MAX(full_buf_width, ALIGNED_SIZE(x_padding, 2)+input_width);
     full_buf_width = ALIGNED_SIZE(full_buf_width, ALIGNMENT/2);
     /* Need 2 rows of padded input width as acratch for temp output */
     full_out_width = ALIGNED_SIZE(full_buf_width + kernel_width, 4);
@@ -150,7 +151,7 @@ WORD32 xa_nn_avgpool_getsize_nhwc(
 
         if(kernel_height <= (int)MAX_HEIGHT_16_BIT_ACC) // Accumulation in 16 bit container
         {
-            zero_mem_bytes = XT_MAX(sizeof(UWORD8)*cw_plane_size, sizeof(WORD16)*input_channels);
+            zero_mem_bytes = MAX((int)(sizeof(UWORD8)*cw_plane_size), (int)(sizeof(WORD16)*input_channels));
 
             total_size = ALIGNED_SIZE(sizeof(WORD32)* out_height, ALIGNMENT) +
                          ALIGNED_SIZE(sizeof(WORD32)* out_width, ALIGNMENT) +
@@ -162,7 +163,7 @@ WORD32 xa_nn_avgpool_getsize_nhwc(
         }
         else  // Accumulation in 32 bit container
         {
-            zero_mem_bytes = XT_MAX(sizeof(UWORD8)*cw_plane_size, sizeof(WORD32)*input_channels);
+            zero_mem_bytes = MAX((int)(sizeof(UWORD8)*cw_plane_size), (int)(sizeof(WORD32)*input_channels));
 
             total_size = ALIGNED_SIZE(sizeof(WORD32)*out_height, ALIGNMENT) +
                          ALIGNED_SIZE(sizeof(WORD32)*out_width, ALIGNMENT) +
@@ -179,7 +180,7 @@ WORD32 xa_nn_avgpool_getsize_nhwc(
         int zero_mem_bytes;
 
         cw_plane_size = input_width*input_channels;
-        zero_mem_bytes = XT_MAX(sizeof(WORD16)*cw_plane_size, sizeof(WORD32)*input_channels);
+        zero_mem_bytes = MAX((int)(sizeof(WORD16)*cw_plane_size), (int)(sizeof(WORD32)*input_channels));
 
         total_size = ALIGNED_SIZE(sizeof(WORD32)*out_height, ALIGNMENT) +
             ALIGNED_SIZE(sizeof(WORD32)*out_width, ALIGNMENT) +
@@ -256,6 +257,7 @@ WORD32 xa_nn_avgpool_getsize(
     return scratch_size;
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 VOID xa_nn_avgpool_init(
     WORD32 inp_precision,
     pVOID  p_scratch,
@@ -319,3 +321,4 @@ VOID xa_nn_avgpool_init(
     /* Initialize output scratch pointer */
     p_state->p_tmp_out = (pVOID)ALIGN_PTR(p_mem, ALIGNMENT);
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
diff --git a/algo/kernels/pool/hifi5/xa_nn_maxpool.c b/algo/kernels/pool/hifi5/xa_nn_maxpool.c
index 647ed11..b5bf4dd 100644
--- a/algo/kernels/pool/hifi5/xa_nn_maxpool.c
+++ b/algo/kernels/pool/hifi5/xa_nn_maxpool.c
@@ -24,6 +24,7 @@
 #include "xa_nnlib_kernels_api.h"
 #include "xa_nn_maxpool_state.h"
 #include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common_macros_hifi5.h"
 
 WORD32 xa_nn_maxpool_getsize_nchw(
     WORD32 inp_precision,
@@ -76,7 +77,7 @@ WORD32 xa_nn_maxpool_getsize_nchw(
     state_size = ALIGNED_SIZE(sizeof(xa_nn_maxpool_state_t), ALIGNMENT);
     /* Output scratch buffer size */
     full_buf_width = kernel_width + (out_width - 1)*x_stride;
-    full_buf_width = XT_MAX(full_buf_width, x_padding + input_width);
+    full_buf_width = MAX(full_buf_width, x_padding + input_width);
     full_buf_width = ALIGNED_SIZE(full_buf_width, ALIGNMENT/2);
     /* maxpool: Need 2 rows of padded input width as acratch for temp output */
     full_out_width = ALIGNED_SIZE(full_buf_width + kernel_width, 4);
@@ -102,7 +103,7 @@ WORD32 xa_nn_maxpool_getsize_nhwc(WORD32  inp_precision,
     int full_buf_width;
     int left_pad_aligned = ALIGNED_SIZE(x_padding, ALIGNMENT);
     full_buf_width = kernel_width + (out_width - 1)*x_stride;
-    full_buf_width = XT_MAX(full_buf_width, x_padding + input_width);
+    full_buf_width = MAX(full_buf_width, x_padding + input_width);
     int right_pad = full_buf_width - (x_padding + input_width);
     full_buf_width = full_buf_width + left_pad_aligned + right_pad + kernel_width;
     
@@ -203,6 +204,7 @@ WORD32 xa_nn_maxpool_getsize(
     return scratch_size;
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 WORD32 xa_nn_maxpool_init(
     WORD32 inp_precision,
     pVOID  p_scratch,
@@ -253,3 +255,4 @@ WORD32 xa_nn_maxpool_init(
     p_state->p_scratch = (pVOID)p_mem;
     return 0;
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
diff --git a/algo/kernels/reorg/hifi5/xa_nn_concat_8.c b/algo/kernels/reorg/hifi5/xa_nn_concat_8.c
new file mode 100644
index 0000000..4cf4fd2
--- /dev/null
+++ b/algo/kernels/reorg/hifi5/xa_nn_concat_8.c
@@ -0,0 +1,157 @@
+/*******************************************************************************
+* Copyright (c) 2018-2024 Cadence Design Systems, Inc.
+*
+* Permission is hereby granted, free of charge, to any person obtaining
+* a copy of this software and associated documentation files (the
+* "Software"), to use this Software with Cadence processor cores only and
+* not with any other processors and platforms, subject to
+* the following conditions:
+*
+* The above copyright notice and this permission notice shall be included
+* in all copies or substantial portions of the Software.
+*
+* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+* IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+* SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+******************************************************************************/
+#include "xa_type_def.h"
+#include "xa_nn_common.h"
+#include "xa_nnlib_kernels_api.h"
+#include "xa_nnlib_common_macros_hifi5.h"
+#include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common.h"
+#include<string.h>
+
+WORD32 xa_nn_concat_8_8(WORD8 * __restrict__ p_out
+                        ,const WORD32 *const p_out_shape
+                        ,const WORD8 **pp_inps
+                        ,const WORD32 *const *pp_inps_shape
+                        ,WORD32 num_out_dims
+                        ,WORD32 num_inp
+                        ,WORD32 num_inp_dims
+                        ,WORD32 axis)
+{
+  XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_out_shape, -1);
+  XA_NNLIB_ARG_CHK_PTR(pp_inps, -1);
+  XA_NNLIB_ARG_CHK_PTR(pp_inps_shape, -1);
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_out_shape, sizeof(WORD32), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(pp_inps, sizeof(WORD8 *), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(pp_inps_shape, sizeof(WORD32 *), -1);
+  //Validate Arguments
+  XA_NNLIB_ARG_CHK_COND((num_out_dims <= 0 || num_out_dims > 6), -1);
+  XA_NNLIB_ARG_CHK_COND((num_inp <= 0 || num_inp > 10), -1);
+  XA_NNLIB_ARG_CHK_COND((num_inp_dims != num_out_dims), -1);
+  XA_NNLIB_ARG_CHK_COND((axis < -num_out_dims || axis >= num_out_dims), -1);
+
+  int i = 0, j = 0;
+  for(i = 0; i < num_out_dims; i++)
+  {
+    XA_NNLIB_ARG_CHK_COND((p_out_shape[i] <= 0), -1);
+  }
+
+  if(axis < 0)
+    axis = num_out_dims + axis;
+
+  WORD32 concat_size = 0;
+  for (i = 0; i < num_inp; i++)
+  {
+    XA_NNLIB_ARG_CHK_PTR(pp_inps[i], -1);
+    XA_NNLIB_ARG_CHK_PTR(pp_inps_shape[i], -1);
+    XA_NNLIB_ARG_CHK_ALIGN(pp_inps_shape[i], sizeof(WORD32), -1);
+#pragma loop_count min=1
+    for(j = 0; j < num_out_dims; j++)
+    {
+      XA_NNLIB_ARG_CHK_COND((pp_inps_shape[i][j] != p_out_shape[j] && j != axis), -1);
+    }
+    XA_NNLIB_ARG_CHK_COND((pp_inps_shape[i][axis] <= 0), -1);
+    concat_size += pp_inps_shape[i][axis];
+  }
+
+  XA_NNLIB_ARG_CHK_COND((p_out_shape[axis] != concat_size), -1);
+
+  //Calculate outer and inner size for axis
+  WORD32 outer_size = 1;
+#pragma no_simd
+  for(int i = 0; i < axis; i++)
+  {
+    outer_size *= p_out_shape[i];
+  }
+
+  WORD32 base_inner_size = 1;
+#pragma no_simd
+  for(int i = axis + 1; i < num_out_dims; i++)
+  {
+    base_inner_size *= p_out_shape[i];
+  }
+
+  if(outer_size == 1)
+  {
+    WORD8 *ptmp_out = p_out;
+    for(int i = 0; i < num_inp; i++)
+    {
+      const WORD32 copy_size = pp_inps_shape[i][axis] * base_inner_size;
+
+      {
+        WORD8 *output_ptr = ptmp_out;
+        const WORD8* input_ptr = pp_inps[i];
+
+        {
+          MEMCPY_8b(output_ptr, input_ptr, copy_size);
+        }
+        ptmp_out += copy_size;
+      }
+    }
+  }
+  else
+  {
+    WORD8 *ptmp_out = p_out;
+#pragma loop_count min=1
+    for(int i = 0; i < num_inp; i++)
+    {
+      const WORD32 copy_size = pp_inps_shape[i][axis] * base_inner_size;
+
+      if(copy_size <= 16)
+      {
+        ae_int8x16 *output_ptr;
+        ae_int8x16 *input_ptr = (ae_int8x16 *)pp_inps[i];
+        ae_valignx2 input_valign, output_valign;
+        input_valign = AE_LA128_PP(input_ptr);
+        ae_int8x8 d_inp1, d_inp2;
+#pragma loop_count min=1
+#pragma concurrent
+        for(int k = 0; k < outer_size; k++)
+        {
+          output_ptr = (ae_int8x16 *)(ptmp_out + concat_size * base_inner_size * k);
+          output_valign = AE_ZALIGN128();
+          AE_LAV8X8X2_XP(d_inp1, d_inp2, input_valign, input_ptr, copy_size);
+          AE_SAV8X8X2_XP(d_inp1, d_inp2, output_valign, output_ptr, copy_size);
+          AE_SA128POS_FP(output_valign, (void *)output_ptr);
+        }
+      }
+      else
+      {
+        WORD8 *output_ptr = ptmp_out;
+        const WORD8* input_ptr = pp_inps[i];
+
+#pragma loop_count min=1
+        for(int k = 0; k < outer_size; k++)
+        {
+          // memcpy(output_ptr, input_ptr, copy_size * sizeof(WORD8));
+          MEMCPY_8b(output_ptr, input_ptr, (int)(copy_size * sizeof(WORD8)));
+          input_ptr += copy_size;
+          output_ptr += concat_size * base_inner_size;
+        }
+      }
+      ptmp_out += copy_size;
+    }
+  }
+  return 0;
+
+}
diff --git a/algo/kernels/reorg/hifi5/xa_nn_transpose_16.c b/algo/kernels/reorg/hifi5/xa_nn_transpose_16.c
new file mode 100644
index 0000000..f9b88ed
--- /dev/null
+++ b/algo/kernels/reorg/hifi5/xa_nn_transpose_16.c
@@ -0,0 +1,224 @@
+/*******************************************************************************
+* Copyright (c) 2018-2024 Cadence Design Systems, Inc.
+*
+* Permission is hereby granted, free of charge, to any person obtaining
+* a copy of this software and associated documentation files (the
+* "Software"), to use this Software with Cadence processor cores only and
+* not with any other processors and platforms, subject to
+* the following conditions:
+*
+* The above copyright notice and this permission notice shall be included
+* in all copies or substantial portions of the Software.
+*
+* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+* IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+* SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+******************************************************************************/
+#include "xa_nnlib_common.h"
+
+/*
+ * Currently only supports upto 5D input tensors.
+ * 1/2/3/4 D input tensors will be scaled up to 5D.
+ * For example, 2x3 -> 1x1x1x2x3.
+ */
+
+WORD32 xa_nn_transpose_16_16(WORD16 * __restrict__ p_out
+                    ,const WORD32 *const p_out_shape
+                    ,const WORD16 * __restrict__ p_inp
+                    ,const WORD32 *const p_inp_shape
+                    ,const WORD32 * __restrict__ p_permute_vec
+                    ,WORD32 num_out_dims
+                    ,WORD32 num_inp_dims)
+{
+  /* NULL pointer checks */
+  XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_permute_vec, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_out_shape, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp_shape, -1);
+
+  /* Invalid input checks */
+  XA_NNLIB_ARG_CHK_COND(((num_inp_dims <= 0) || (num_inp_dims > 5)), -1);
+  XA_NNLIB_ARG_CHK_COND((num_out_dims != num_inp_dims), -1);
+
+  int itr = 0;
+  for(itr=0; itr < num_inp_dims; itr++)
+  {
+    XA_NNLIB_ARG_CHK_COND((p_inp_shape[itr] <= 0), -1);
+  }
+  for(itr=0; itr < num_out_dims; itr++)
+  {
+    XA_NNLIB_ARG_CHK_COND((p_out_shape[itr] <= 0), -1);
+  }
+
+  /* Output shape provided must be correct based on input
+   * shape and permute values */
+  for(itr=0; itr < num_out_dims; itr++)
+  {
+    int output_dim = p_out_shape[itr];
+    int expected_dim = p_inp_shape[p_permute_vec[itr]];
+    XA_NNLIB_ARG_CHK_COND((output_dim != expected_dim), -1);
+  }
+
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_out, sizeof(WORD16), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_inp, sizeof(WORD16), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_permute_vec, sizeof(WORD32), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_out_shape, sizeof(WORD32), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_inp_shape, sizeof(WORD32), -1);
+
+  /* Promoting lesser dim tensors to 5D tensors.
+   * Also updating the permute_vec and shapes as needed for optimization */
+  int p_5D_inp_shape[5] = {1, 1, 1, 1, 1};
+  int p_5D_out_shape[5] = {1, 1, 1, 1, 1};
+  int p_5D_permute_vec[5] = {0, 1, 2, 3, 4};
+
+  /* Check if any inner inp dimension is same in the output */
+  int last_dim_same = 1, last_n_same_dim = 0;
+  itr = num_inp_dims - 1;
+  while(itr >= 0)
+  {
+    last_n_same_dim = (last_dim_same && (p_permute_vec[itr] == itr)) ? (last_n_same_dim + 1) : last_n_same_dim;
+    last_dim_same = (p_permute_vec[itr] == itr) ? last_dim_same & 1 : last_dim_same & 0;
+    itr--;
+  }
+
+  int dims_added = 5 - num_inp_dims;
+  itr = num_inp_dims - 1;
+  int same_count = last_n_same_dim;
+  int count = 4;
+  while(itr >= 0)
+  {
+    p_5D_inp_shape[count] = (same_count > 0) ? p_5D_inp_shape[count]*p_inp_shape[itr] : p_inp_shape[itr];
+    p_5D_out_shape[count] = (same_count > 0) ? p_5D_out_shape[count]*p_out_shape[itr] : p_out_shape[itr];
+    same_count--;
+    itr--;
+    count = (same_count > 0) ? count : count - 1;
+  }
+
+  itr = num_inp_dims - 1;
+  same_count = (last_n_same_dim) ? num_inp_dims - (last_n_same_dim - 1) : 0;
+  count = 4;
+  while(itr >= 0)
+  {
+    p_5D_permute_vec[count] = (same_count > 0) ? p_permute_vec[itr-(last_n_same_dim - 1)] + dims_added + last_n_same_dim - 1 : p_permute_vec[itr] + dims_added;
+    same_count--;
+    itr--;
+    count--;
+  }
+
+  int out_dim0, out_dim1, out_dim2, out_dim3, out_dim4;
+  int inp_dim1, inp_dim2, inp_dim3, inp_dim4;
+  int inp_stride[5];
+
+  out_dim0 = p_5D_out_shape[0];
+  out_dim1 = p_5D_out_shape[1];
+  out_dim2 = p_5D_out_shape[2];
+  out_dim3 = p_5D_out_shape[3];
+  out_dim4 = p_5D_out_shape[4];
+
+  inp_dim1 = p_5D_inp_shape[1];
+  inp_dim2 = p_5D_inp_shape[2];
+  inp_dim3 = p_5D_inp_shape[3];
+  inp_dim4 = p_5D_inp_shape[4];
+
+  inp_stride[0] = inp_dim1*inp_dim2*inp_dim3*inp_dim4;
+  inp_stride[1] = inp_dim2*inp_dim3*inp_dim4;
+  inp_stride[2] = inp_dim3*inp_dim4;
+  inp_stride[3] = inp_dim4;
+  inp_stride[4] = 1;
+
+  if(last_n_same_dim)
+  {
+    int itr0, itr1, itr2, itr3, itr4;
+    WORD16 *p_inp0 = (WORD16*)p_inp;
+    for(itr0 = 0; itr0 < out_dim0; itr0++)
+    {
+      WORD16 *p_inp1 = p_inp0+(itr0*inp_stride[p_5D_permute_vec[0]]);
+#pragma loop_count min=1
+      for(itr1 = 0; itr1 < out_dim1; itr1++)
+      {
+        WORD16 *p_inp2 = p_inp1+(itr1*inp_stride[p_5D_permute_vec[1]]);
+#pragma loop_count min=1
+        for(itr2 = 0; itr2 < out_dim2; itr2++)
+        {
+          WORD16 *p_inp3 = p_inp2+(itr2*inp_stride[p_5D_permute_vec[2]]);
+#pragma loop_count min=1
+          for(itr3 = 0; itr3 < out_dim3; itr3++, p_out+=out_dim4)
+          {
+            WORD16 *p_inp4 = p_inp3+(itr3*inp_stride[p_5D_permute_vec[3]]);
+            ae_int16x8 *__restrict__ pae_i = (ae_int16x8 *)(p_inp4);
+            ae_int16x8 *__restrict__ pae_o = (ae_int16x8 *)(p_out);
+            ae_valignx2 a_inp = AE_LA128_PP(pae_i);
+            ae_valignx2 a_out = AE_ZALIGN128();
+            ae_int16x4 d0,d1;
+            for(itr4 = 0; itr4 < (out_dim4 >> 3); itr4++)
+            {
+              AE_LA16X4X2_IP(d0, d1, a_inp, (ae_int16x8*)pae_i);
+              AE_SA16X4X2_IP(d0, d1, a_out, (ae_int16x8*)pae_o);
+            }
+            AE_LAV16X4X2_XP(d0, d1, a_inp, (ae_int16x8*)pae_i, (out_dim4 & 7)<<1);
+            AE_SAV16X4X2_XP(d0, d1, a_out, (ae_int16x8*)pae_o, (out_dim4 & 7)<<1);
+            AE_SA128POS_FP(a_out, pae_o);
+          }
+        }
+      }
+    }
+  }
+  else
+  {
+    int itr0, itr1, itr2, itr3, itr4;
+    WORD16 *p_inp0 = (WORD16*)p_inp;
+    for(itr0 = 0; itr0 < out_dim0; itr0++)
+    {
+      WORD16 *p_inp1 = p_inp0+(itr0*inp_stride[p_5D_permute_vec[0]]);
+      for(itr1 = 0; itr1 < out_dim1; itr1++)
+      {
+        WORD16 *p_inp2 = p_inp1+(itr1*inp_stride[p_5D_permute_vec[1]]);
+        for(itr2 = 0; itr2 < out_dim2; itr2++)
+        {
+          WORD16 *p_inp3 = p_inp2+(itr2*inp_stride[p_5D_permute_vec[2]]);
+          for(itr3 = 0; itr3 < out_dim3; itr3++)
+          {
+            WORD16 *p_inp4 = p_inp3+(itr3*inp_stride[p_5D_permute_vec[3]]);
+
+            ae_valign a_out = AE_ZALIGN64();
+            for(itr4 = 0; itr4 < (out_dim4 >> 2); itr4++)
+            {
+              ae_int16x4 d0, d1, d2, d3;
+              ae_int16x4 tmp0;
+
+              d1 = AE_L16_X ((ae_int16*)p_inp4, inp_stride[p_5D_permute_vec[4]]<<1);
+              d2 = AE_L16_X ((ae_int16*)p_inp4, 2*inp_stride[p_5D_permute_vec[4]]<<1);
+              d3 = AE_L16_X ((ae_int16*)p_inp4, 3*inp_stride[p_5D_permute_vec[4]]<<1);
+              AE_L16_XP(d0, (ae_int16*)p_inp4, 4*inp_stride[p_5D_permute_vec[4]]<<1);
+
+              tmp0 = AE_SEL16_6543(d0, d1);
+              tmp0 = AE_SEL16_6543(tmp0, d2);
+              tmp0 = AE_SEL16_6543(tmp0, d3);
+
+              AE_SA16X4_IP(tmp0, a_out, (ae_int16x4 *)p_out);
+            }
+            AE_SA64POS_FP(a_out, p_out);
+#pragma loop_count max=3
+            for(itr4 = 0; itr4 < (out_dim4 & 3); itr4++)
+            {
+              ae_int16x4 d0;
+              AE_L16_XP(d0, (ae_int16*)p_inp4, inp_stride[p_5D_permute_vec[4]]<<1);
+              AE_S16_0_IP(d0, (ae_int16 *)p_out, 2);
+            }
+          }
+        }
+      }
+    }
+  }
+
+  return 0;
+}
+
+
diff --git a/include/nnlib/xa_nnlib_kernels_api.h b/include/nnlib/xa_nnlib_kernels_api.h
index c8a0bbb..20f33ec 100644
--- a/include/nnlib/xa_nnlib_kernels_api.h
+++ b/include/nnlib/xa_nnlib_kernels_api.h
@@ -1676,6 +1676,12 @@
 			WORD32  vec_length,
 			pVOID   p_scratch);
 
+  WORD32 xa_nn_vec_softmax_sym16s_16( WORD16 * __restrict__ p_out,
+      const   WORD16 * __restrict__ p_vec,
+      WORD32  input_beta_left_shift,
+      WORD32  input_beta_multiplier,
+      WORD32  vec_length);
+
 	WORD32 xa_nn_vec_sigmoid_asym8u_asym8u(UWORD8 *p_out,
 			const UWORD8 *p_vec,
 			WORD32 zero_point,
@@ -3039,6 +3045,14 @@
                     ,WORD32 num_out_dims
                     ,WORD32 num_inp_dims);
 
+  WORD32 xa_nn_transpose_16_16(WORD16 * __restrict__ p_out
+                    ,const WORD32 *const p_out_shape
+                    ,const WORD16 * __restrict__ p_inp
+                    ,const WORD32 *const p_inp_shape
+                    ,const WORD32 * __restrict__ p_permute_vec
+                    ,WORD32 num_out_dims
+                    ,WORD32 num_inp_dims);
+
   WORD32 xa_nn_batch_norm_3D_8_8(WORD8 * __restrict__ p_out
                     ,const WORD8 * __restrict__ p_inp
                     ,const WORD16 * __restrict__ p_alpha
@@ -3083,6 +3097,44 @@ WORD32 xa_nn_resize_nearest_neighbour_8_8(pWORD8 __restrict__ p_out
                     ,FLOAT32 width_offset
                     ,WORD32  align_corners);
 
+WORD32 xa_nn_concat_8_8(WORD8 * __restrict__ p_out
+        ,const WORD32 *const p_out_shape
+        ,const WORD8 **p_inps
+        ,const WORD32 *const *pp_inps_shape
+        ,WORD32 num_out_dims
+        ,WORD32 num_inp
+        ,WORD32 num_inp_dims
+        ,WORD32 axis);
+
+#ifdef ENABLE_SCRATCH_SIZE_API_ONLY
+
+#if defined(hifi5)
+#define get_softmax_scratch_size                get_softmax_scratch_size_hifi5
+#define xa_nn_avgpool_getsize                   xa_nn_avgpool_getsize_hifi5
+#define xa_nn_conv2d_depthwise_getsize          xa_nn_conv2d_depthwise_getsize_hifi5
+#define xa_nn_conv2d_std_getsize                xa_nn_conv2d_std_getsize_hifi5
+#define xa_nn_conv2d_std_getsize_sym4s          xa_nn_conv2d_std_getsize_sym4s_hifi5
+#define xa_nn_dilated_conv2d_depthwise_getsize  xa_nn_dilated_conv2d_depthwise_getsize_hifi5
+#define xa_nn_dilated_conv2d_std_getsize        xa_nn_dilated_conv2d_std_getsize_hifi5
+#define xa_nn_maxpool_getsize                   xa_nn_maxpool_getsize_hifi5
+#define xa_nn_reduce_getsize_nhwc               xa_nn_reduce_getsize_nhwc_hifi5
+#define xa_nn_transpose_conv_getsize            xa_nn_transpose_conv_getsize_hifi5
+
+#elif defined(hifi4)
+#define get_softmax_scratch_size                get_softmax_scratch_size_hifi4
+#define xa_nn_avgpool_getsize                   xa_nn_avgpool_getsize_hifi4
+#define xa_nn_conv2d_depthwise_getsize          xa_nn_conv2d_depthwise_getsize_hifi4
+#define xa_nn_conv2d_std_getsize                xa_nn_conv2d_std_getsize_hifi4
+#define xa_nn_conv2d_std_getsize_sym4s          xa_nn_conv2d_std_getsize_sym4s_hifi4
+#define xa_nn_dilated_conv2d_depthwise_getsize  xa_nn_dilated_conv2d_depthwise_getsize_hifi4
+#define xa_nn_dilated_conv2d_std_getsize        xa_nn_dilated_conv2d_std_getsize_hifi4
+#define xa_nn_maxpool_getsize                   xa_nn_maxpool_getsize_hifi4
+#define xa_nn_reduce_getsize_nhwc               xa_nn_reduce_getsize_nhwc_hifi4
+#define xa_nn_transpose_conv_getsize            xa_nn_transpose_conv_getsize_hifi4
+
+#endif
+
+#endif /* #ifdef ENABLE_SCRATCH_SIZE_API_ONLY */
 
 	/* Mapping the functions names from previous naming convension for backward compatibility */
 #define xa_nn_matXvec_asym8xasym8_asym8 xa_nn_matXvec_asym8uxasym8u_asym8u
diff --git a/include/nnlib/xa_nnlib_standards.h b/include/nnlib/xa_nnlib_standards.h
index 88c619a..fb91967 100644
--- a/include/nnlib/xa_nnlib_standards.h
+++ b/include/nnlib/xa_nnlib_standards.h
@@ -22,7 +22,9 @@
 #ifndef __STANDARDS_H__
 #define __STANDARDS_H__
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include <xtensa/config/core-isa.h>
+#endif
 
 #if defined(__cplusplus)
 extern "C"
@@ -151,7 +153,7 @@ typedef struct _xa_nnlib_shape_t{
 
 enum xa_error_severity {
   xa_severity_nonfatal = 0,
-  xa_severity_fatal    = (int)0xffffffff
+  xa_severity_fatal    = (unsigned int)0xffffffff
 };
 
 enum xa_error_class {
