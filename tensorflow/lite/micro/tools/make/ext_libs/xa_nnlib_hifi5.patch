diff --git a/algo/common/include/xa_nn_common.h b/algo/common/include/xa_nn_common.h
index 5a45d27..4fa7215 100644
--- a/algo/common/include/xa_nn_common.h
+++ b/algo/common/include/xa_nn_common.h
@@ -64,8 +64,10 @@
 #define STRINGIZE(A) STRINGIZE_NX(A)    /*  Turn A into a string literal after macro-expanding it. */
 //#include STRINGIZE(PPCAT(cstub,XTENSA_CORE).h)
 //#include STRINGIZE(PPCAT(PPCAT(cstub,XTENSA_CORE),c.h))
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include "xtensa/tie/xt_hifi3.h"
 #include "xtensa/config/core-isa.h"
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 #endif
 
 //-----------------------------------------------------
@@ -89,6 +91,7 @@
 #define INV_TBL_BITS 7
 extern const int32_t tab_invQ30[128];
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #if XCHAL_HAVE_NSA
   #define NSA(n) XT_NSA(n)
 #else
@@ -100,6 +103,7 @@ extern const int32_t tab_invQ30[128];
     return AE_NSAQ56S(t)-8;
   }
 #endif
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 
 #ifdef COMPILER_XTENSA
   #define ATTRIBUTE_ALWAYS_INLINE __attribute__((always_inline))
@@ -123,11 +127,13 @@ extern const int32_t tab_invQ30[128];
 #define return_int64(x) {  union {ae_int64  ai;int64_t   i; } r; r.ai = x;  return r.i; }
 #endif
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #if  defined (__cplusplus) || defined(COMPILER_XTENSA)
 
 #else
 #error sorry, C compiler is not supported excluding the XCC
 #endif
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 
 
 #ifdef COMPILER_MSVC
diff --git a/algo/common/include/xa_nnlib_common.h b/algo/common/include/xa_nnlib_common.h
index fcf379a..d66f2a0 100644
--- a/algo/common/include/xa_nnlib_common.h
+++ b/algo/common/include/xa_nnlib_common.h
@@ -21,12 +21,16 @@
 ******************************************************************************/
 #ifndef __XA_NNLIB_LEGACY_COMPAT_H__
 #define __XA_NNLIB_LEGACY_COMPAT_H__
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include <xtensa/config/core-isa.h>
 #include "xtensa/tie/xt_hifi2.h"
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 #include "xa_nnlib_api.h"
 #include "xa_nnlib_standards.h"
 #include "xa_nnlib_err_chk.h"
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include "xa_nnlib_hifi_isa_compat.h"
+#endif
 #include "xa_nn_common.h"
 #include "xa_nnlib_common_internal.h"
 #endif /* __XA_NNLIB_LEGACY_COMPAT_H__ */
@@ -36,4 +40,4 @@
 #define XA_HAVE_HIFI3_CORE 1
 #else
 #define XA_HAVE_HIFI3_CORE 0
-#endif
\ No newline at end of file
+#endif
diff --git a/algo/common/include/xa_nnlib_common_macros_hifi5.h b/algo/common/include/xa_nnlib_common_macros_hifi5.h
index b2ac8e3..9ad396d 100644
--- a/algo/common/include/xa_nnlib_common_macros_hifi5.h
+++ b/algo/common/include/xa_nnlib_common_macros_hifi5.h
@@ -28,6 +28,8 @@
 #define NULL (void *)0
 #endif /* NULL */
 
+#define MAX(a, b)   (((a) > (b)) ? (a) : (b))
+
 /* Macros for memcpy */
 #define MEMCPY_8b(out, inp, N) \
 { \
diff --git a/algo/kernels/activations/hifi5/xa_nn_softmax_sym16s_16.c b/algo/kernels/activations/hifi5/xa_nn_softmax_sym16s_16.c
new file mode 100644
index 0000000..8cc14c5
--- /dev/null
+++ b/algo/kernels/activations/hifi5/xa_nn_softmax_sym16s_16.c
@@ -0,0 +1,429 @@
+/*******************************************************************************
+* Copyright (c) 2018-2024 Cadence Design Systems, Inc.
+*
+* Permission is hereby granted, free of charge, to any person obtaining
+* a copy of this software and associated documentation files (the
+* "Software"), to use this Software with Cadence processor cores only and
+* not with any other processors and platforms, subject to
+* the following conditions:
+*
+* The above copyright notice and this permission notice shall be included
+* in all copies or substantial portions of the Software.
+*
+* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+* IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+* SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+******************************************************************************/
+#include "xa_type_def.h"
+#include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common.h"
+#include "xa_nnlib_common_macros_hifi5.h"
+WORD16 exp_lut[513] = {
+      2,     2,     2,     2,     2,     2,     2,     2,
+      2,     2,     2,     2,     2,     2,     2,     2,
+      2,     2,     2,     2,     2,     2,     2,     2,
+      2,     2,     2,     3,     3,     3,     3,     3,
+      3,     3,     3,     3,     3,     3,     3,     3,
+      3,     3,     3,     3,     4,     4,     4,     4,
+      4,     4,     4,     4,     4,     4,     4,     4,
+      4,     5,     5,     5,     5,     5,     5,     5,
+      5,     5,     5,     6,     6,     6,     6,     6,
+      6,     6,     6,     7,     7,     7,     7,     7,
+      7,     7,     7,     8,     8,     8,     8,     8,
+      8,     9,     9,     9,     9,     9,     9,    10,
+     10,    10,    10,    10,    11,    11,    11,    11,
+     11,    12,    12,    12,    12,    13,    13,    13,
+     13,    14,    14,    14,    14,    15,    15,    15,
+     16,    16,    16,    17,    17,    17,    18,    18,
+     18,    19,    19,    19,    20,    20,    21,    21,
+     21,    22,    22,    23,    23,    24,    24,    25,
+     25,    26,    26,    27,    27,    28,    28,    29,
+     29,    30,    30,    31,    32,    32,    33,    34,
+     34,    35,    36,    36,    37,    37,    38,    39,
+     40,    40,    42,    42,    43,    44,    45,    45,
+     46,    47,    48,    49,    50,    51,    52,    53,
+     54,    55,    56,    57,    59,    60,    60,    62,
+     63,    65,    65,    67,    68,    69,    71,    73,
+     74,    75,    77,    78,    80,    81,    83,    85,
+     86,    88,    90,    92,    93,    95,    97,    99,
+    101,   103,   105,   107,   109,   112,   114,   116,
+    118,   121,   123,   126,   128,   131,   133,   135,
+    139,   141,   144,   147,   149,   152,   155,   158,
+    162,   165,   168,   171,   174,   178,   181,   185,
+    189,   192,   196,   200,   204,   208,   212,   217,
+    221,   225,   230,   234,   239,   243,   248,   253,
+    258,   263,   268,   273,   279,   284,   290,   296,
+    302,   308,   314,   320,   327,   333,   340,   346,
+    353,   360,   366,   374,   381,   389,   397,   404,
+    413,   421,   429,   437,   446,   455,   464,   473,
+    482,   492,   501,   511,   522,   532,   543,   553,
+    564,   575,   586,   598,   610,   622,   634,   646,
+    659,   672,   685,   699,   713,   727,   741,   756,
+    771,   786,   801,   817,   833,   850,   866,   884,
+    901,   919,   937,   955,   974,   993,  1013,  1033,
+   1053,  1074,  1095,  1117,  1139,  1161,  1184,  1207,
+   1232,  1256,  1281,  1306,  1332,  1358,  1385,  1412,
+   1440,  1468,  1497,  1527,  1557,  1587,  1619,  1651,
+   1683,  1716,  1750,  1785,  1820,  1856,  1892,  1930,
+   1968,  2006,  2046,  2087,  2128,  2170,  2212,  2256,
+   2300,  2346,  2392,  2439,  2488,  2537,  2587,  2638,
+   2690,  2743,  2796,  2852,  2908,  2966,  3024,  3084,
+   3145,  3207,  3270,  3334,  3400,  3467,  3535,  3605,
+   3677,  3749,  3822,  3898,  3975,  4053,  4133,  4214,
+   4297,  4383,  4469,  4557,  4647,  4739,  4833,  4927,
+   5024,  5124,  5225,  5328,  5433,  5541,  5649,  5761,
+   5875,  5991,  6109,  6230,  6352,  6477,  6605,  6736,
+   6868,  7004,  7141,  7282,  7427,  7572,  7722,  7874,
+   8030,  8188,  8350,  8514,  8683,  8854,  9028,  9206,
+   9387,  9572,  9762,  9954, 10151, 10351, 10555, 10763,
+  10976, 11191, 11412, 11637, 11867, 12102, 12341, 12583,
+  12831, 13085, 13342, 13606, 13874, 14148, 14427, 14711,
+  15002, 15297, 15599, 15907, 16221, 16541, 16867, 17199,
+  17539, 17884, 18237, 18597, 18964, 19338, 19719, 20108,
+  20505, 20909, 21322, 21742, 22171, 22608, 23054, 23509,
+  23973, 24445, 24928, 25419, 25921, 26432, 26953, 27485,
+  28027, 28580, 29143, 29718, 30304, 30902, 31512, 32133,
+  32767};
+
+WORD16 one_over_one_plus_x_lut[513] = {
+  32767, 32704, 32640, 32578, 32514, 32451, 32388, 32326,
+  32264, 32202, 32141, 32079, 32018, 31957, 31896, 31835,
+  31775, 31715, 31655, 31596, 31537, 31476, 31418, 31359,
+  31301, 31242, 31184, 31127, 31069, 31011, 30954, 30897,
+  30840, 30784, 30727, 30671, 30615, 30560, 30504, 30449,
+  30394, 30339, 30283, 30229, 30175, 30121, 30067, 30013,
+  29960, 29906, 29853, 29800, 29746, 29694, 29642, 29589,
+  29537, 29486, 29434, 29382, 29331, 29280, 29229, 29177,
+  29127, 29076, 29026, 28976, 28926, 28877, 28827, 28777,
+  28728, 28679, 28630, 28581, 28532, 28484, 28436, 28388,
+  28340, 28292, 28244, 28197, 28150, 28103, 28056, 28008,
+  27962, 27915, 27869, 27823, 27777, 27731, 27685, 27640,
+  27594, 27549, 27504, 27459, 27413, 27369, 27324, 27280,
+  27236, 27192, 27148, 27104, 27060, 27016, 26973, 26930,
+  26887, 26844, 26801, 26758, 26715, 26673, 26630, 26588,
+  26546, 26504, 26463, 26421, 26380, 26338, 26297, 26255,
+  26214, 26174, 26132, 26092, 26051, 26011, 25971, 25931,
+  25891, 25851, 25811, 25772, 25732, 25693, 25653, 25614,
+  25575, 25536, 25497, 25458, 25420, 25381, 25343, 25305,
+  25267, 25229, 25191, 25153, 25116, 25078, 25041, 25003,
+  24966, 24928, 24892, 24855, 24818, 24781, 24745, 24709,
+  24672, 24636, 24600, 24564, 24528, 24492, 24457, 24421,
+  24385, 24350, 24315, 24280, 24245, 24210, 24175, 24140,
+  24105, 24070, 24036, 24002, 23967, 23933, 23899, 23865,
+  23831, 23798, 23764, 23730, 23697, 23664, 23630, 23597,
+  23564, 23530, 23498, 23465, 23432, 23399, 23366, 23334,
+  23302, 23269, 23237, 23205, 23173, 23141, 23109, 23077,
+  23046, 23014, 22982, 22951, 22920, 22888, 22857, 22826,
+  22795, 22764, 22733, 22703, 22672, 22641, 22611, 22580,
+  22550, 22520, 22490, 22459, 22429, 22400, 22370, 22340,
+  22310, 22281, 22251, 22221, 22192, 22163, 22134, 22104,
+  22075, 22046, 22017, 21988, 21959, 21931, 21902, 21874,
+  21845, 21817, 21788, 21760, 21732, 21704, 21676, 21648,
+  21620, 21592, 21565, 21537, 21509, 21482, 21455, 21427,
+  21400, 21372, 21345, 21318, 21291, 21264, 21237, 21210,
+  21183, 21157, 21130, 21103, 21077, 21050, 21024, 20998,
+  20971, 20945, 20919, 20893, 20867, 20841, 20816, 20790,
+  20764, 20738, 20713, 20687, 20662, 20636, 20611, 20586,
+  20560, 20535, 20510, 20485, 20460, 20435, 20410, 20385,
+  20360, 20336, 20311, 20287, 20262, 20238, 20213, 20189,
+  20165, 20141, 20117, 20092, 20068, 20044, 20021, 19997,
+  19973, 19949, 19926, 19902, 19878, 19855, 19832, 19808,
+  19784, 19762, 19738, 19715, 19692, 19668, 19645, 19622,
+  19600, 19577, 19553, 19531, 19508, 19485, 19463, 19440,
+  19418, 19395, 19373, 19351, 19328, 19306, 19284, 19262,
+  19240, 19218, 19196, 19174, 19152, 19130, 19109, 19087,
+  19065, 19044, 19022, 19000, 18979, 18958, 18936, 18915,
+  18893, 18872, 18851, 18830, 18809, 18787, 18766, 18745,
+  18725, 18704, 18682, 18662, 18641, 18620, 18600, 18579,
+  18559, 18538, 18518, 18497, 18477, 18457, 18436, 18416,
+  18396, 18376, 18356, 18336, 18316, 18296, 18276, 18256,
+  18236, 18216, 18197, 18177, 18157, 18138, 18118, 18099,
+  18079, 18059, 18040, 18021, 18001, 17982, 17963, 17944,
+  17924, 17905, 17886, 17867, 17848, 17829, 17810, 17791,
+  17772, 17754, 17735, 17716, 17697, 17679, 17660, 17641,
+  17623, 17604, 17586, 17568, 17549, 17531, 17513, 17494,
+  17476, 17458, 17440, 17422, 17404, 17386, 17368, 17350,
+  17332, 17314, 17296, 17278, 17261, 17243, 17225, 17208,
+  17190, 17172, 17155, 17137, 17120, 17102, 17085, 17067,
+  17050, 17033, 17015, 16999, 16981, 16964, 16947, 16930,
+  16913, 16895, 16878, 16862, 16845, 16828, 16810, 16794,
+  16777, 16760, 16743, 16727, 16710, 16693, 16677, 16660,
+  16644, 16627, 16611, 16594, 16578, 16562, 16545, 16529,
+  16513, 16497, 16480, 16464, 16448, 16432, 16416, 16400,
+  16384};
+
+static inline ae_int16x4 LUTLookUpX4(ae_int16x4 value, WORD16* lut)
+{
+  ae_int16x4 shifted_value = AE_SRAI16(value, 7);
+  ae_int16x4 index = AE_ADD16S(AE_MOVDA16(256), shifted_value);
+  ae_int16x4 offset = AE_SLAI16S(AE_AND16(value, AE_MOVDA16(0x7f)),8);
+  WORD32 index0, index1, index2, index3;
+  index0 = AE_MOVAD16_3(index);
+  index1 = AE_MOVAD16_2(index);
+  index2 = AE_MOVAD16_1(index);
+  index3 = AE_MOVAD16_0(index);
+
+  ae_int16 *p_ae_lut = (ae_int16 *)lut;
+  ae_int16x4 base0123 = p_ae_lut[index0];
+  base0123 = AE_SEL16_6543(base0123, p_ae_lut[index1]);
+  base0123 = AE_SEL16_6543(base0123, p_ae_lut[index2]);
+  base0123 = AE_SEL16_6543(base0123, p_ae_lut[index3]);
+
+  ae_int16x4 slope0123 = p_ae_lut[index0 + 1];
+  slope0123 = AE_SEL16_6543(slope0123, p_ae_lut[index1 + 1]);
+  slope0123 = AE_SEL16_6543(slope0123, p_ae_lut[index2 + 1]);
+  slope0123 = AE_SEL16_6543(slope0123, p_ae_lut[index3 + 1]);
+  slope0123 = AE_SUB16S(slope0123, base0123);
+
+  ae_int16x4 delta0123;
+  delta0123 = AE_MULFP16X4RAS(slope0123, offset);
+  ae_int16x4 result0123 = AE_ADD16S(base0123, delta0123);
+
+  return result0123;
+}
+
+static inline ae_int16x4 LUTLookUp(ae_int16x4 value, WORD16* lut)
+{
+  ae_int16x4 shifted_value = AE_SRAI16(value, 7);
+  ae_int16x4 index = AE_ADD16S(AE_MOVDA16(256), shifted_value);
+  ae_int16x4 offset = AE_SLAI16S(AE_AND16(value, AE_MOVDA16(0x7f)),8);
+
+  WORD32 index0;
+  index0 = AE_MOVAD16_3(index);
+
+  ae_int16 *p_ae_lut = (ae_int16 *)lut;
+  ae_int16x4 base = p_ae_lut[index0];
+
+  ae_int16x4 slope = p_ae_lut[index0 + 1];
+  slope = AE_SUB16S(slope, base);
+
+  ae_int16x4 delta;
+  delta = AE_MULFP16X4RAS(slope, offset);
+
+  ae_int16x4 result = AE_ADD16S(base, delta);
+
+  return result;
+}
+
+// Computes exp(input - max_input)
+static inline ae_int16x4 softmaxCalculateExp(WORD32 input_beta_left_shift,
+                            WORD32 input_beta_multiplier,
+                            ae_int16x4 d_inp,
+                            ae_int16x4 max_in_row)
+{
+  ae_int32x2 input_diff1, input_diff2, scaled_diff1, scaled_diff2;
+  ae_int32x2 sym_scaled_diff1, sym_scaled_diff2;
+
+  AE_SUBW16(input_diff1, input_diff2, d_inp, max_in_row);
+
+  #if TFLITE_SINGLE_ROUNDING
+    int left_shift  = input_beta_left_shift;
+    int right_shift = input_beta_left_shift;
+    /* Single rounding macro doesn't need two shifts so this is not used */
+    (void)right_shift;
+  #else /* #if TFLITE_SINGLE_ROUNDING */
+    int left_shift  = input_beta_left_shift<0?0: input_beta_left_shift;
+    int right_shift = input_beta_left_shift>0?0:-input_beta_left_shift;
+  #endif /* #if TFLITE_SINGLE_ROUNDING */
+  MPY_BY_QUANT_MULT_SLS_X2X2_OUT32(scaled_diff1, scaled_diff2, input_diff1, input_diff2, input_beta_multiplier, left_shift, right_shift);
+  ae_int32x2 max_int16s = AE_MOVDA32(32767);
+
+  sym_scaled_diff1 = AE_ADD32S(scaled_diff1, max_int16s);
+  sym_scaled_diff2 = AE_ADD32S(scaled_diff2, max_int16s);
+  ae_int16x4 sat_sym_shifted_sum = AE_SAT16X4(sym_scaled_diff1,sym_scaled_diff2);
+
+  ae_int16x4 result = LUTLookUpX4(sat_sym_shifted_sum, exp_lut);
+
+  return result;
+}
+
+UWORD8 count_leading_zeros(ae_int32x2 integer_input)
+{
+  WORD32 value = AE_MOVDA32(integer_input);
+  if(value == 0)
+  {
+    return 32;
+  }
+  return AE_NSAZ32_L(integer_input) + 1;
+}
+
+WORD32 xa_nn_vec_softmax_sym16s_16( WORD16 * __restrict__ p_out,
+                    const   WORD16 * __restrict__ p_vec,
+                            WORD32  input_beta_left_shift,
+                            WORD32  input_beta_multiplier,
+                            WORD32  vec_length)
+{
+  /* NULL pointer checks */
+  XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_vec, -1);
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_out, sizeof(UWORD16), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_vec, sizeof(UWORD16), -1);
+  /* Basic Parameter checks */
+  XA_NNLIB_ARG_CHK_COND((vec_length <= 0), -1);
+  XA_NNLIB_ARG_CHK_COND(((input_beta_left_shift < -31) || (input_beta_left_shift > 31)), -1);
+  XA_NNLIB_ARG_CHK_COND((input_beta_multiplier < 0), -1);
+
+  // Calculating Max
+  ae_int16x4 max;
+  int i;
+  {
+    ae_int16x4 m0, m1, m2;
+    ae_int16x8 *p_inp = (ae_int16x8 *)p_vec;
+    ae_valignx2 align_input128 = AE_LA128_PP(p_inp);
+    m0 = AE_MOVDA16(0x8000);
+
+    for(i = 0; i < (vec_length >> 3); i++)
+    {
+      AE_LA16X4X2_IP(m1, m2, align_input128, p_inp);
+      m0 = AE_MAX16(m0, m1);
+      m0 = AE_MAX16(m0, m2);
+    }
+
+    ae_valign align_input64 = AE_LA64_PP(p_vec);
+    for(i = 0; i < ((vec_length & 7) >> 2); i++)
+    {
+      AE_LA16X4_IP(m1, align_input64, (ae_int16x4 *)p_inp);
+      m0 = AE_MAX16(m0, m1);
+    }
+
+    for(i = 0; i < (vec_length & 3); i++)
+    {
+      AE_L16_IP(m1, (ae_int16 *)p_inp, sizeof(ae_int16));
+      m0 = AE_MAX16(m0, m1);
+    }
+
+    if(vec_length < 4)
+    {
+      max = AE_MOVDA16((AE_MOVAD16_0(m0)));
+    }
+    else
+    {
+      ae_int32x2 temp1, temp2;
+      AE_CVTI32X4F16(temp1, temp2, m0, 0);
+      temp2 = AE_MAX32(temp1, temp2);
+
+      temp1 = AE_SEL32_LH(temp2, temp2);
+      temp1 = AE_MAX32(temp1, temp2);
+
+      max = AE_MOVDA16((AE_MOVAD32_L(temp1)));
+    }
+  }
+
+  //Compute exp and sum_of_exp
+  ae_int32x2 sum_of_exps;
+  {
+    ae_int16x4 *temp_out = (ae_int16x4 *)p_out;
+    ae_int16x4 *p_inp = (ae_int16x4 *)p_vec;
+    ae_int16x4 d_inp1, d_inp2;
+    ae_valign align_input64 = AE_LA64_PP(p_inp);
+    ae_int32x2 acc1, acc2, acc;
+    acc1 = AE_MOVDA32(0);
+    acc2 = AE_MOVDA32(0);
+    ae_valign align_output64 = AE_ZALIGN64();
+    ae_int16x4 exp1, exp2;
+    exp2 = AE_MOVDA16(0);
+    for(i = 0; i < (vec_length >> 2); i++)
+    {
+      AE_LA16X4_IP(d_inp1, align_input64, p_inp);
+      exp1 = softmaxCalculateExp(input_beta_left_shift, input_beta_multiplier, d_inp1, max);
+      AE_SA16X4_IP(exp1, align_output64, temp_out);
+      AE_ACCW16(acc1, acc2, exp1, exp2);
+    }
+    AE_SA64POS_FP(align_output64,(void *)temp_out);
+
+    int rem_length = vec_length & 3;
+    if(rem_length)
+    {
+      ae_valignx2 align_input128 = AE_LA128_PP((ae_int16x8 *)p_inp);
+      ae_valignx2 align_output128 = AE_ZALIGN128();
+      AE_LAV16X4X2_XP(d_inp1, d_inp2, align_input128, (ae_int16x8 *)p_inp, rem_length * 2);
+      exp1 = softmaxCalculateExp(input_beta_left_shift, input_beta_multiplier, d_inp1, max);
+
+      ae_int64 mask = AE_MOVF64_FROMF32X2(AE_MOVDA32(-1));
+      mask = AE_SLAA64(mask, 16 * (4 - rem_length));
+      ae_int16x4 mask16x4 = AE_MOVF16X4_FROMF64(mask);
+
+      exp1 = AE_AND16(exp1, mask16x4);
+      AE_ACCW16(acc1, acc2, exp1, d_inp2);
+      AE_SAV16X4X2_XP(exp1, d_inp2, align_output128, (ae_int16x8 *)temp_out, rem_length * 2);
+      AE_SA128POS_FP(align_output128,(void *)temp_out);
+    }
+
+    acc = AE_ADD32S(acc1, acc2);
+    acc1 = AE_MOVDA32(AE_MOVAD32_H(acc));
+    acc2 = AE_MOVDA32(AE_MOVAD32_L(acc));
+    sum_of_exps = AE_ADD32S(acc1, acc2);
+  }
+
+  // Calculate 1/sum_of_exps
+  UWORD8 headroom_plus_one = count_leading_zeros(sum_of_exps);
+  ae_int32x2 shifted_sum = AE_SRAA32RS(sum_of_exps, 14 - (headroom_plus_one - 1));
+  ae_int32x2 plus_one_sym = AE_MOVDA32(-((1<<15) + (1<<16)));
+  ae_int32x2 sym_shifted_sum = AE_ADD32S(shifted_sum, plus_one_sym);
+  ae_int16x4 sat_sym_shifted_sum = AE_SAT16X4(sym_shifted_sum, sym_shifted_sum);
+  ae_int16x4 reciprocal_scale_q015 = LUTLookUp(sat_sym_shifted_sum, one_over_one_plus_x_lut);
+
+  // Compute exp*1/sum_of_exps
+  {
+    ae_int16x8 *temp_out1 = (ae_int16x8 *)p_out;
+    WORD32 right_shift = 31 - headroom_plus_one;
+    ae_int16x4 exp1, exp2;
+    ae_valignx2 exp_aligner = AE_LA128_PP(temp_out1);
+    ae_int32x2 sfmx1, sfmx2, sfmx3, sfmx4;
+    ae_int32x2 shifted_sfmx1, shifted_sfmx2, shifted_sfmx3, shifted_sfmx4;
+    ae_int16x4 sfmx12, sfmx34;
+    ae_int16x8 *temp_out2 = (ae_int16x8 *)p_out;
+    ae_valignx2 align_output128 = AE_ZALIGN128();
+    ae_int16x4 zero = AE_MOVDA16(0);
+#pragma concurrent
+    for(i=0; i<(vec_length>>3); i++)
+    {
+      AE_LA16X4X2_IP(exp1, exp2, exp_aligner, temp_out1);
+      AE_MUL16X4S(sfmx1, sfmx2, exp1, reciprocal_scale_q015);
+      AE_MUL16X4S(sfmx3, sfmx4, exp2, reciprocal_scale_q015);
+      shifted_sfmx1 = AE_SRAA32RS(sfmx1, right_shift);
+      shifted_sfmx2 = AE_SRAA32RS(sfmx2, right_shift);
+      shifted_sfmx3 = AE_SRAA32RS(sfmx3, right_shift);
+      shifted_sfmx4 = AE_SRAA32RS(sfmx4, right_shift);
+      sfmx12 = AE_SAT16X4(shifted_sfmx1, shifted_sfmx2);
+      sfmx34 = AE_SAT16X4(shifted_sfmx3, shifted_sfmx4);
+      sfmx12 = AE_MAX16(sfmx12, zero);
+      sfmx34 = AE_MAX16(sfmx34, zero);
+      AE_SA16X4X2_IP(sfmx12, sfmx34, align_output128, temp_out2);
+    }
+    int rem_length = vec_length & 7;
+    if(rem_length)
+    {
+      AE_LAV16X4X2_XP(exp1, exp2, exp_aligner, temp_out1, rem_length * 2);
+      AE_MUL16X4S(sfmx1, sfmx2, exp1, reciprocal_scale_q015);
+      shifted_sfmx1 = AE_SRAA32RS(sfmx1, right_shift);
+      shifted_sfmx2 = AE_SRAA32RS(sfmx2, right_shift);
+
+      sfmx12 = AE_SAT16X4(shifted_sfmx1, shifted_sfmx2);
+      sfmx12 = AE_MAX16(sfmx12, zero);
+
+      if(rem_length > 4)
+      {
+        AE_MUL16X4S(sfmx3, sfmx4, exp2, reciprocal_scale_q015);
+        shifted_sfmx3 = AE_SRAA32RS(sfmx3, right_shift);
+        shifted_sfmx4 = AE_SRAA32RS(sfmx4, right_shift);
+        sfmx34 = AE_SAT16X4(shifted_sfmx3, shifted_sfmx4);
+        sfmx34 = AE_MAX16(sfmx34, zero);
+        AE_SAV16X4X2_XP(sfmx12, sfmx34, align_output128, temp_out2, rem_length * 2);
+      }
+      else
+      {
+        AE_SAV16X4X2_XP(sfmx12, exp2, align_output128, temp_out2, rem_length * 2);
+      }
+    }
+    AE_SA128POS_FP(align_output128,(void *)temp_out2);
+  }
+
+  return 0;
+}
diff --git a/algo/kernels/basic/hifi5/xa_nn_elm_quantize.c b/algo/kernels/basic/hifi5/xa_nn_elm_quantize.c
index eab6c50..bc2521b 100644
--- a/algo/kernels/basic/hifi5/xa_nn_elm_quantize.c
+++ b/algo/kernels/basic/hifi5/xa_nn_elm_quantize.c
@@ -23,10 +23,87 @@
 #include "xa_nnlib_common_fpu.h"
 #include "xa_nnlib_common_macros_hifi5.h"
 #include <math.h>
-
 #define PACK_32X2(dst1, src1, src2) \
 dst1 = AE_SEL8X8(AE_MOVINT8X8_FROMINT16X4(src1), AE_MOVINT8X8_FROMINT16X4(src2), AE_MOVINT8X8_FROMINT32X2(AE_MOVDA32X2(0x0e0c0a08, 0x06040200)));
 
+WORD32 xa_nn_elm_requantize_asym8u_asym8s(WORD8 * __restrict__ p_out,
+                                    const UWORD8 * __restrict__ p_inp,
+                                    WORD32 inp_zero_bias,
+                                    WORD32 out_zero_bias,
+                                    WORD32 out_shift,
+                                    WORD32 out_multiplier,
+                                    WORD32 num_elm)
+{
+  /* NULL pointer checks */
+  XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_out, sizeof(WORD8), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_inp, sizeof(WORD8), -1);
+  /* Basic Parameter checks */
+  XA_NNLIB_ARG_CHK_COND((num_elm <= 0), -1);
+  XA_NNLIB_ARG_CHK_COND(((out_zero_bias < -128) || (out_zero_bias > 127)), -1);
+  XA_NNLIB_ARG_CHK_COND(((inp_zero_bias < 0) || (inp_zero_bias > 255)), -1);
+  XA_NNLIB_ARG_CHK_COND(((out_shift < -31) || (out_shift > 31)), -1);
+  XA_NNLIB_ARG_CHK_COND((out_multiplier < 0), -1);
+
+  int i;
+  int left_shift, right_shift;
+#if TFLITE_SINGLE_ROUNDING
+  left_shift = out_shift;
+  /* Single rounding doesn't need two shifts */
+  (void)right_shift;
+#else /* #if TFLITE_SINGLE_ROUNDING */
+  left_shift  = (out_shift < 0)?0:out_shift;
+  right_shift = (out_shift > 0)?0:-out_shift;
+#endif /* #if TFLITE_SINGLE_ROUNDING */
+
+  ae_int8x8 *p_i = (ae_int8x8 *)p_inp;
+  WORD8 *p_o = p_out;
+
+  ae_valign align_inp = AE_LA64_PP(p_inp);
+  ae_valign align_dst = AE_ZALIGN64();
+  ae_int8x8 d_inp_zero_bias   = AE_MOVDA8(inp_zero_bias);
+  ae_int32x2 d_out_multiplier = AE_MOVDA32(out_multiplier);
+  ae_int8x8 d_inp0, d_out0;
+  ae_int16x4 ONE = AE_MOVDA16(1);
+  ae_int16x4 d_inp16_0,d_inp16_1;
+  ae_int32x2 d_inp32_0, d_inp32_1, d_inp32_2, d_inp32_3;
+  ae_int16x4 d_out0_16, d_out1_16;
+
+  for(i = 0; i < num_elm >> 3; i++)
+  {
+    AE_LA8X8_IP(d_inp0, align_inp, p_i);
+    AE_SUBW8U(d_inp16_0, d_inp16_1, d_inp0, d_inp_zero_bias);
+    AE_MUL16X4(d_inp32_0 , d_inp32_1 , d_inp16_0 , ONE);
+    AE_MUL16X4(d_inp32_2 , d_inp32_3 , d_inp16_1 , ONE);
+    MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(d_out0_16 , d_inp32_0, d_inp32_1, d_out_multiplier, left_shift, right_shift, out_zero_bias);
+    MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(d_out1_16 , d_inp32_2, d_inp32_3, d_out_multiplier, left_shift, right_shift, out_zero_bias);
+    d_out0 = AE_SAT8X8X16(d_out0_16 , d_out1_16);
+    AE_SA8X8_IP(d_out0 ,align_dst, (ae_int8x8 *)p_o);
+  }
+  AE_SA64POS_FP(align_dst, p_o);
+
+  ae_valignx2 align_inpx2 = AE_LA128_PP(p_i);
+  ae_valignx2 align_dstx2 = AE_ZALIGN128();
+
+  if(num_elm & 7){
+    ae_int8x8 d_inp0_8x8, d_inp1_8x8, d_out0_8x8, d_out1_8x8;
+    AE_LAV8X8X2_XP(d_inp0_8x8, d_inp1_8x8, align_inpx2, (ae_int8x16 *)p_i, num_elm & 7);
+    AE_SUBW8U(d_inp16_0, d_inp16_1, d_inp0_8x8, d_inp_zero_bias);
+    AE_MUL16X4(d_inp32_0 , d_inp32_1 , d_inp16_0 , ONE);
+    AE_MUL16X4(d_inp32_2 , d_inp32_3 , d_inp16_1 , ONE);
+    MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(d_out0_16 , d_inp32_0, d_inp32_1, d_out_multiplier, left_shift, right_shift, out_zero_bias);
+    MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(d_out1_16 , d_inp32_2, d_inp32_3, d_out_multiplier, left_shift, right_shift, out_zero_bias);
+    d_out0_8x8 = AE_SAT8X8X16(d_out0_16 , d_out1_16);
+    d_out1_8x8 = d_inp1_8x8;
+    AE_SAV8X8X2_XP(d_out0_8x8, d_out1_8x8, align_dstx2, (ae_int8x16 *)p_o, num_elm & 7);
+  }
+  AE_SA128POS_FP(align_dstx2, p_o);
+
+  return 0;
+}
+
 WORD32 xa_nn_elm_requantize_asym16s_asym8s(WORD8 * __restrict__ p_out,
                                     const WORD16 * __restrict__ p_inp,
                                     WORD32  inp_zero_bias,
@@ -584,7 +661,8 @@ WORD32 xa_nn_elm_quantize_f32_asym8s(WORD8 * __restrict__ p_out,
   ae_int16x4 d_out_zero_bias = AE_MOVDA16(out_zero_bias);
   xtfloat *out_scale_ptr = &out_scale;
   xtfloatx2 d_out_scale = (xtfloatx2)*out_scale_ptr;
-
+  xtfloatx2 d_one = FLOAT_SX2(AE_MOVDA32(1),0);
+  xtfloatx2 d_one_over_out_scale = XT_DIV_SX2(d_one, d_out_scale);
   for(i = 0; i < (num_elm >> 3); i++)
   {
     xtfloatx2 d_inp0, d_inp1, d_inp2, d_inp3;
@@ -593,14 +671,11 @@ WORD32 xa_nn_elm_quantize_f32_asym8s(WORD8 * __restrict__ p_out,
     ae_int16x4 d_out16_0, d_out16_1;
     ae_int32x2 d_out32_0, d_out32_1, d_out32_2, d_out32_3;
 
-
     AE_LASX2X2_IP(d_inp0, d_inp1, align_inp, p_i);
     AE_LASX2X2_IP(d_inp2, d_inp3, align_inp, p_i);
 
-    d_inp0_t = XT_DIV_SX2(d_inp0, d_out_scale);
-    d_inp1_t = XT_DIV_SX2(d_inp1, d_out_scale);
-    d_inp2_t = XT_DIV_SX2(d_inp2, d_out_scale);
-    d_inp3_t = XT_DIV_SX2(d_inp3, d_out_scale);
+    MUL_SX2X2(d_inp0_t, d_inp1_t, d_inp0, d_inp1, d_one_over_out_scale, d_one_over_out_scale);
+    MUL_SX2X2(d_inp2_t, d_inp3_t, d_inp2, d_inp3, d_one_over_out_scale, d_one_over_out_scale);
 
     d_inp0_t = XT_FIROUND_SX2(d_inp0_t);
     d_inp1_t = XT_FIROUND_SX2(d_inp1_t);
@@ -624,31 +699,60 @@ WORD32 xa_nn_elm_quantize_f32_asym8s(WORD8 * __restrict__ p_out,
   }
   AE_SA64POS_FP(align_dst, p_o);
 
-  for(i = 0; i < (num_elm & 7); i++)
+  WORD32 rem_elm = (num_elm & 7);
+  if(rem_elm > 0)
   {
-    FLOAT32 d_inp0;
-    FLOAT32 d_inp0_t;
+    xtfloatx2 d_inp0, d_inp1, d_inp2, d_inp3;
+    xtfloatx2 d_inp0_t, d_inp1_t, d_inp2_t, d_inp3_t;
     ae_int8x8 d_out0;
-    ae_int16x4 d_out16_0;
-    ae_int32x2 d_out32_0;
+    ae_int16x4 d_out16_0, d_out16_1;
+    ae_int32x2 d_out32_0, d_out32_1, d_out32_2, d_out32_3;
+    ae_int16x4 d_tmp0, d_tmp1, d_tmp2, d_tmp3;
+    ae_valignx2 align_dstx2;
+
+    WORD32 rem_elm0, rem_elm1;
+    rem_elm0 = rem_elm >= 4 ? 16 : rem_elm << 2;
+    rem_elm1 = rem_elm <= 4 ? 0 : (rem_elm - 4) << 2;
+    ae_int16x8 *p_i16 = (ae_int16x8 *)p_i;
+    align_inp = AE_LA128_PP(p_i16);
+    align_dstx2 = AE_ZALIGN128();
+    AE_LAV16X4X2_XP(d_tmp0, d_tmp1, align_inp, p_i16, rem_elm0);
+    AE_LAV16X4X2_XP(d_tmp2, d_tmp3, align_inp, p_i16, rem_elm1);
 
-    AE_LSIP(d_inp0, (xtfloat *)p_i, 4);
+    d_tmp0 = AE_SEL16_2301(d_tmp0, d_tmp0);
+    d_tmp1 = AE_SEL16_2301(d_tmp1, d_tmp1);
+    d_tmp2 = AE_SEL16_2301(d_tmp2, d_tmp2);
+    d_tmp3 = AE_SEL16_2301(d_tmp3, d_tmp3);
 
-    d_inp0_t = XT_DIV_S(d_inp0, (FLOAT32)d_out_scale);
+    d_inp0 = AE_MOVXTFLOATX2_FROMINT32X2(AE_MOVINT32X2_FROMINT16X4(d_tmp0));
+    d_inp1 = AE_MOVXTFLOATX2_FROMINT32X2(AE_MOVINT32X2_FROMINT16X4(d_tmp1));
+    d_inp2 = AE_MOVXTFLOATX2_FROMINT32X2(AE_MOVINT32X2_FROMINT16X4(d_tmp2));
+    d_inp3 = AE_MOVXTFLOATX2_FROMINT32X2(AE_MOVINT32X2_FROMINT16X4(d_tmp3));
 
-    d_inp0_t = XT_FIROUND_S(d_inp0_t);
+    MUL_SX2X2(d_inp0_t, d_inp1_t, d_inp0, d_inp1, d_one_over_out_scale, d_one_over_out_scale);
+    MUL_SX2X2(d_inp2_t, d_inp3_t, d_inp2, d_inp3, d_one_over_out_scale, d_one_over_out_scale);
 
-    d_out32_0 = XT_TRUNC_S(d_inp0_t, 0);
+    d_inp0_t = XT_FIROUND_SX2(d_inp0_t);
+    d_inp1_t = XT_FIROUND_SX2(d_inp1_t);
+    d_inp2_t = XT_FIROUND_SX2(d_inp2_t);
+    d_inp3_t = XT_FIROUND_SX2(d_inp3_t);
 
-    d_out16_0 = AE_SAT16X4(d_out32_0, d_out32_0);
+    d_out32_0 = XT_TRUNC_SX2(d_inp0_t, 0);
+    d_out32_1 = XT_TRUNC_SX2(d_inp1_t, 0);
+    d_out32_2 = XT_TRUNC_SX2(d_inp2_t, 0);
+    d_out32_3 = XT_TRUNC_SX2(d_inp3_t, 0);
+
+    d_out16_0 = AE_SAT16X4(d_out32_0, d_out32_1);
+    d_out16_1 = AE_SAT16X4(d_out32_2, d_out32_3);
 
     d_out16_0 = AE_ADD16S(d_out16_0, d_out_zero_bias);
+    d_out16_1 = AE_ADD16S(d_out16_1, d_out_zero_bias);
 
-    d_out0 = AE_SAT8X8X16(d_out16_0, d_out16_0);
+    d_out0 = AE_SAT8X8X16(d_out16_0, d_out16_1);
 
-    *((ae_int8 *)p_o + i) = AE_MOVINT8_FROMINT8X8(d_out0);
+    AE_SAV8X8X2_XP(d_out0, d_out0, align_dstx2, (ae_int8x16 *)p_o, rem_elm);
+    AE_SA128POS_FP(align_dstx2, (ae_int8x16 *)p_o);
   }
-
   return 0;
 }
 #endif /* #if !HAVE_VFPU */
@@ -791,6 +895,8 @@ WORD32 xa_nn_elm_quantize_f32_asym16s(WORD16 * __restrict__ p_out,
   ae_int32x2 d_out_zero_bias = AE_MOVDA32(out_zero_bias);
   xtfloat *out_scale_ptr = &out_scale;
   xtfloatx2 d_out_scale = (xtfloatx2)*out_scale_ptr;
+  xtfloatx2 d_one = FLOAT_SX2(AE_MOVDA32(1),0);
+  xtfloatx2 d_one_over_out_scale = XT_DIV_SX2(d_one, d_out_scale);
 
   for(i = 0; i < (num_elm >> 3); i++)
   {
@@ -803,10 +909,8 @@ WORD32 xa_nn_elm_quantize_f32_asym16s(WORD16 * __restrict__ p_out,
     AE_LASX2X2_IP(d_inp0, d_inp1, align_inp, p_i);
     AE_LASX2X2_IP(d_inp2, d_inp3, align_inp, p_i);
 
-    d_inp0_t = XT_DIV_SX2(d_inp0, d_out_scale);
-    d_inp1_t = XT_DIV_SX2(d_inp1, d_out_scale);
-    d_inp2_t = XT_DIV_SX2(d_inp2, d_out_scale);
-    d_inp3_t = XT_DIV_SX2(d_inp3, d_out_scale);
+    MUL_SX2X2(d_inp0_t, d_inp1_t, d_inp0, d_inp1, d_one_over_out_scale, d_one_over_out_scale);
+    MUL_SX2X2(d_inp2_t, d_inp3_t, d_inp2, d_inp3, d_one_over_out_scale, d_one_over_out_scale);
 
     d_inp0_t = XT_FIROUND_SX2(d_inp0_t);
     d_inp1_t = XT_FIROUND_SX2(d_inp1_t);
@@ -856,10 +960,8 @@ WORD32 xa_nn_elm_quantize_f32_asym16s(WORD16 * __restrict__ p_out,
     d_inp2 = AE_MOVXTFLOATX2_FROMINT32X2(AE_MOVINT32X2_FROMINT16X4(d_tmp2));
     d_inp3 = AE_MOVXTFLOATX2_FROMINT32X2(AE_MOVINT32X2_FROMINT16X4(d_tmp3));
 
-    d_inp0_t = XT_DIV_SX2(d_inp0, d_out_scale);
-    d_inp1_t = XT_DIV_SX2(d_inp1, d_out_scale);
-    d_inp2_t = XT_DIV_SX2(d_inp2, d_out_scale);
-    d_inp3_t = XT_DIV_SX2(d_inp3, d_out_scale);
+    MUL_SX2X2(d_inp0_t, d_inp1_t, d_inp0, d_inp1, d_one_over_out_scale, d_one_over_out_scale);
+    MUL_SX2X2(d_inp2_t, d_inp3_t, d_inp2, d_inp3, d_one_over_out_scale, d_one_over_out_scale);
 
     d_inp0_t = XT_FIROUND_SX2(d_inp0_t);
     d_inp1_t = XT_FIROUND_SX2(d_inp1_t);
diff --git a/algo/kernels/basic/hifi5/xa_nn_reduce_asym8s_asym8s.c b/algo/kernels/basic/hifi5/xa_nn_reduce_asym8s_asym8s.c
index 6fdcbe5..c079731 100644
--- a/algo/kernels/basic/hifi5/xa_nn_reduce_asym8s_asym8s.c
+++ b/algo/kernels/basic/hifi5/xa_nn_reduce_asym8s_asym8s.c
@@ -80,6 +80,7 @@ WORD32 xa_nn_reduce_getsize_nhwc(WORD32 inp_precision
     return 0;
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 /*
  * Currently only supports upto 4D input tensors.
  * 1/2/3 D input tensors will be scaled up to 4D.
@@ -1512,3 +1513,4 @@ WORD32 xa_nn_reduce_mean_4D_asym8s_asym8s(WORD8 * __restrict__ p_out
 
   return 0;
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
diff --git a/algo/kernels/cnn/hifi5/xa_nn_circ_buf.c b/algo/kernels/cnn/hifi5/xa_nn_circ_buf.c
index 7b49a65..2ce9e25 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_circ_buf.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_circ_buf.c
@@ -45,7 +45,7 @@ int xa_nn_circ_buf_nchw_getsize(
   }
 
   circ_buf_width = kernel_width + ((output_width - 1) * x_stride);
-  circ_buf_width = XT_MAX(circ_buf_width, x_padding + input_width);
+  circ_buf_width = MAX(circ_buf_width, x_padding + input_width);
 
   /* Align circ_buf_width to 8 for 8-bit and 16-bit inputs to make L8X8 and
   L16X4X2 loads possible */
@@ -349,7 +349,7 @@ int xa_nn_circ_buf_nhwc_getsize(
   int size_in_bytes;
 
   circ_buf_height = kernel_height + ((output_height - 1) * y_stride);
-  circ_buf_height = XT_MAX(circ_buf_height, (y_padding + input_height + dilation_height - 1)/dilation_height);
+  circ_buf_height = MAX(circ_buf_height, (y_padding + input_height + dilation_height - 1)/dilation_height);
 
   if(bytewidth == 4)
   {
diff --git a/algo/kernels/cnn/hifi5/xa_nn_circ_buf.h b/algo/kernels/cnn/hifi5/xa_nn_circ_buf.h
index 920da3e..eb3c44b 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_circ_buf.h
+++ b/algo/kernels/cnn/hifi5/xa_nn_circ_buf.h
@@ -23,6 +23,11 @@
 #ifndef __XA_NN_CIRC_BUF_H__
 #define __XA_NN_CIRC_BUF_H__
 
+#ifdef ENABLE_SCRATCH_SIZE_API_ONLY
+#define xa_nn_circ_buf_nchw_getsize     xa_nn_circ_buf_nchw_getsize_hifi5
+#define xa_nn_circ_buf_nhwc_getsize     xa_nn_circ_buf_nhwc_getsize_hifi5
+#endif
+
 #define OUT_HEIGHT_PER_ITER 2
 
 #define ALIGNMENT_16   16   /* 16 bytes alignment */
diff --git a/algo/kernels/cnn/hifi5/xa_nn_conv2d_depthwise.c b/algo/kernels/cnn/hifi5/xa_nn_conv2d_depthwise.c
index e3ea1be..d22c403 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_conv2d_depthwise.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_conv2d_depthwise.c
@@ -77,7 +77,7 @@ static WORD32 xa_nn_dilated_conv2d_depthwise_nchw_getsize
   circ_buf_size = ALIGNED_SIZE(circ_buf_size, ALIGNMENT_16);
 
   circ_buf_width = dilated_kernel_width + ((output_width - 1) * x_stride);
-  circ_buf_width = XT_MAX(circ_buf_width, x_padding+input_width);
+  circ_buf_width = MAX(circ_buf_width, x_padding+input_width);
   if(circ_buf_bytewidth == 1 || circ_buf_bytewidth == 2)
     circ_buf_width = ALIGNED_SIZE(circ_buf_width, 8);
   else
@@ -385,6 +385,7 @@ static WORD32 xa_nn_dilated_conv2d_depthwise_getsize_generic
     case -2: /* For f16*/
       scratch_bytewidth = 2; /* 16b scratch */
       circ_buf_bytewidth = 2; /* bytewidth as per precision */
+      break;
     case -8: /* For sym16s */
       scratch_bytewidth = 8; /* 64b scratch */
       circ_buf_bytewidth = 2; /* bytewidth for sym16s */
diff --git a/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_f32.c b/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_f32.c
index c5b3952..6ef0e8f 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_f32.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_f32.c
@@ -66,13 +66,27 @@ static WORD32 conv_x_left_pad(
   out_width_over_x_pad = out_width_over_x_pad > out_width ? out_width : out_width_over_x_pad;
 
   /* When kernel convolves over x-left pad region only, output is just bias */
-  for(i=0;i<out_height;i++)
+  if(p_bias != NULL)
   {
-    for(j=0;j<out_width_over_x_pad;j++)
+    for(i=0;i<out_height;i++)
     {
-      for(k=0;k<out_channels;k++)
+      for(j=0;j<out_width_over_x_pad;j++)
       {
-        p_out[i*out_height_offset+j*out_width_offset+k*out_channels_offset] = p_bias[k];
+        for(k=0;k<out_channels;k++)
+        {
+          p_out[i*out_height_offset+j*out_width_offset+k*out_channels_offset] = p_bias[k];
+        }
+      }
+    }
+  } else {
+    for(i=0;i<out_height;i++)
+    {
+      for(j=0;j<out_width_over_x_pad;j++)
+      {
+        for(k=0;k<out_channels;k++)
+        {
+          p_out[i*out_height_offset+j*out_width_offset+k*out_channels_offset] = 0.0f;
+        }
       }
     }
   }
@@ -97,13 +111,26 @@ static WORD32 conv_x_right_pad(
   WORD32 out_width_over_x_r_pad = out_width - idx_out_width_over_x_r_pad;
 
   /* When kernel convolves over x-right pad region only, output is just bias */
-  for(i=0;i<out_height;i++)
-  {
-    for(j=idx_out_width_over_x_r_pad;j<out_width;j++)
+  if(p_bias != NULL){
+    for(i=0;i<out_height;i++)
+    {
+      for(j=idx_out_width_over_x_r_pad;j<out_width;j++)
+      {
+        for(k=0;k<out_channels;k++)
+        {
+          p_out[i*out_height_offset+j*out_width_offset+k*out_channels_offset] = p_bias[k];
+        }
+      }
+    }
+  } else {
+    for(i=0;i<out_height;i++)
     {
-      for(k=0;k<out_channels;k++)
+      for(j=idx_out_width_over_x_r_pad;j<out_width;j++)
       {
-        p_out[i*out_height_offset+j*out_width_offset+k*out_channels_offset] = p_bias[k];
+        for(k=0;k<out_channels;k++)
+        {
+          p_out[i*out_height_offset+j*out_width_offset+k*out_channels_offset] = 0.0f;
+        }
       }
     }
   }
@@ -134,7 +161,6 @@ WORD32 xa_nn_conv2d_std_f32(
   XA_NNLIB_ARG_CHK_PTR(p_out, -1);
   XA_NNLIB_ARG_CHK_PTR(p_kernel, -1);
   XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
-  XA_NNLIB_ARG_CHK_PTR(p_bias, -1);
   XA_NNLIB_ARG_CHK_PTR(p_scratch, -1);
   /* Pointer alignment checks */
   XA_NNLIB_ARG_CHK_ALIGN(p_out, sizeof(FLOAT32), -1);
diff --git a/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_sym8sxasym8s.c b/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_sym8sxasym8s.c
index 777958f..640bc18 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_sym8sxasym8s.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_sym8sxasym8s.c
@@ -65,7 +65,10 @@ static WORD32 conv_x_left_pad(
         left_shift  = p_out_shift[k] < 0 ? 0 : p_out_shift[k];
         right_shift = p_out_shift[k] > 0 ? 0 : -p_out_shift[k];
 #endif /* #if TFLITE_SINGLE_ROUNDING */
-        ae_int32x2 acc = AE_MOVDA32(p_bias[k]);
+        ae_int32x2 acc = 0;
+        if(p_bias != NULL){
+          acc = AE_MOVDA32(p_bias[k]);
+        }
         MPY_BY_QUANT_MULT_X2_OUT32(acc, acc, p_out_multiplier[k], left_shift, right_shift);
         acc = AE_ADD32S(acc, AE_MOVDA32(out_zero_bias));
         AE_MINMAX32(acc, min_int8, max_int8);
@@ -118,7 +121,10 @@ static WORD32 conv_x_right_pad(
         left_shift  = p_out_shift[k] < 0 ? 0 : p_out_shift[k];
         right_shift = p_out_shift[k] > 0 ? 0 : -p_out_shift[k];
 #endif /* #if TFLITE_SINGLE_ROUNDING */
-        ae_int32x2 acc = AE_MOVDA32(p_bias[k]);
+        ae_int32x2 acc = 0;
+        if(p_bias != NULL){
+          acc = AE_MOVDA32(p_bias[k]);
+        }
         MPY_BY_QUANT_MULT_X2_OUT32(acc, acc, p_out_multiplier[k], left_shift, right_shift);
         acc = AE_ADD32S(acc, AE_MOVDA32(out_zero_bias));
         AE_MINMAX32(acc, min_int8, max_int8);
@@ -168,7 +174,10 @@ static void conv_y_pad_nhwc_out(
         left_shift  = p_out_shift[k] < 0 ? 0 : p_out_shift[k];
         right_shift = p_out_shift[k] > 0 ? 0 : -p_out_shift[k];
 #endif /* #if TFLITE_SINGLE_ROUNDING */
-        ae_int32x2 acc = AE_MOVDA32(p_bias[k]);
+        ae_int32x2 acc = 0;
+        if(p_bias != NULL){
+          acc = AE_MOVDA32(p_bias[k]);
+        }
         MPY_BY_QUANT_MULT_X2_OUT32(acc, acc, p_out_multiplier[k], left_shift, right_shift);
         acc = AE_ADD32S(acc, AE_MOVDA32(out_zero_bias));
         AE_MINMAX32(acc, min_int8, max_int8);
@@ -681,7 +690,6 @@ WORD32 xa_nn_conv2d_std_per_chan_sym8sxasym8s(
   XA_NNLIB_ARG_CHK_PTR(p_out, -1);
   XA_NNLIB_ARG_CHK_PTR(p_kernel, -1);
   XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
-  XA_NNLIB_ARG_CHK_PTR(p_bias, -1);
   XA_NNLIB_ARG_CHK_PTR(p_scratch, -1);
   /* Pointer alignment checks */
   XA_NNLIB_ARG_CHK_ALIGN(p_bias, sizeof(WORD32), -1);
diff --git a/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_sym8sxsym16s.c b/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_sym8sxsym16s.c
index 0f69a94..1631cf3 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_sym8sxsym16s.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_conv2d_std_sym8sxsym16s.c
@@ -55,7 +55,10 @@ static WORD32 conv_x_left_pad(
       ae_int64 q1;
       for(k = 0; k < out_channels; k++)
       {
-        AE_L64_IP(q1, pbias, 8);
+        q1 = 0;
+        if(pbias != NULL){
+          AE_L64_IP(q1, pbias, 8);
+        }
         ae_int32x2 acc;
         MPY_BY_QUANT_MULT_ACC64_OUT32(acc, q1, p_out_multiplier[k], p_out_shift[k]);
         d1 = AE_SAT16X4(acc, acc);
@@ -98,7 +101,10 @@ static WORD32 conv_x_right_pad(
       ae_int64 q1;
       for(k = 0; k < out_channels; k++)
       {
-        AE_L64_IP(q1, pbias, 8);
+        q1 = 0;
+        if(pbias != NULL){
+          AE_L64_IP(q1, pbias, 8);
+        }
         ae_int32x2 acc;
         MPY_BY_QUANT_MULT_ACC64_OUT32(acc, q1, p_out_multiplier[k], p_out_shift[k]);
         d1 = AE_SAT16X4(acc, acc);
@@ -137,7 +143,6 @@ WORD32 xa_nn_conv2d_std_per_chan_sym8sxsym16s(
   XA_NNLIB_ARG_CHK_PTR(p_out, -1);
   XA_NNLIB_ARG_CHK_PTR(p_kernel, -1);
   XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
-  XA_NNLIB_ARG_CHK_PTR(p_bias, -1);
   XA_NNLIB_ARG_CHK_PTR(p_scratch, -1);
   /* Pointer alignment checks */
   XA_NNLIB_ARG_CHK_ALIGN(p_bias, sizeof(WORD64), -1);
diff --git a/algo/kernels/cnn/hifi5/xa_nn_matXvec_f32_circ.c b/algo/kernels/cnn/hifi5/xa_nn_matXvec_f32_circ.c
index da9ccb3..06187b9 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_matXvec_f32_circ.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_matXvec_f32_circ.c
@@ -73,7 +73,8 @@ DISCARD_FUN_FOR_NONVOID_RETURN(WORD32, xa_nn_matXvec_f32_circ,(
     SETUP_BIAS_BATCH_VEC_UNROLL(idx_row);
 
 #define SETUP_BIAS_BATCH_FOR_f32(idx_row,idx_vec) \
-    xtfloat _xtfloat_bias_ ##idx_row ##_ ##idx_vec = p_bias[(vec_itr + idx_vec)]; \
+    xtfloat _xtfloat_bias_ ##idx_row ##_ ##idx_vec = 0.0f; \
+    if(p_bias != NULL) {_xtfloat_bias_ ##idx_row ##_ ##idx_vec = p_bias[(vec_itr + idx_vec)];} \
 
 #define LOAD_VEC_BATCH_f32(idx_vec) \
     AE_LSX2X2_IP(_xtfloatx2_vec_batch_ ##idx_vec,_xtfloatx2_vec_batch_1_ ##idx_vec,_xtfloatx4_p_vec_batch_ ##idx_vec,16);
@@ -328,8 +329,12 @@ WORD32 xa_nn_matXvec_f32_circ(
                 {
                     ae_valignx2 align_mat0, align_mat1;
                     ae_valignx2 align_vec0, align_vec1;
-                    xtfloatx2 _xtfloat_bias_0 = p_bias[vec_itr];
-                    xtfloatx2 _xtfloat_bias_1 = p_bias[vec_itr+1];
+                    xtfloatx2 _xtfloat_bias_0 = 0.0f;
+                    xtfloatx2 _xtfloat_bias_1 = 0.0f;
+                    if(p_bias != NULL){
+                      _xtfloat_bias_0 = p_bias[vec_itr];
+                      _xtfloat_bias_1 = p_bias[vec_itr+1];
+                    }
 
                     xtfloatx2 _xtfloatx2_acc_0_0 = (xtfloatx2)0.0f;
                     xtfloatx2 _xtfloatx2_acc_0_1 = (xtfloatx2)0.0f;
@@ -433,8 +438,13 @@ WORD32 xa_nn_matXvec_f32_circ(
                 }
                 for (; m_itr < rows ; m_itr++)
                 {
-                    xtfloat _xtfloat_bias_0 = p_bias[vec_itr];
-                    xtfloat _xtfloat_bias_1 = p_bias[vec_itr+1];
+
+                    xtfloat _xtfloat_bias_0 = 0.0f;
+                    xtfloat _xtfloat_bias_1 = 0.0f;
+                    if(p_bias != NULL){
+                      _xtfloat_bias_0 = p_bias[vec_itr];
+                      _xtfloat_bias_1 = p_bias[vec_itr+1];
+                    }
                     xtfloat _xtfloatx2_acc_0_0 = (xtfloat)0.0f;
                     xtfloat _xtfloatx2_acc_0_1 = (xtfloat)0.0f;
 
@@ -470,7 +480,10 @@ WORD32 xa_nn_matXvec_f32_circ(
                 vec_itr = vec_count -1;
                 for(m_itr = 0; m_itr < (rows); m_itr ++)
                 {
-                    xtfloat _xtfloat_bias = p_bias[vec_itr];
+                    xtfloat _xtfloat_bias = 0.0f;
+                    if(p_bias != NULL){
+                      _xtfloat_bias = p_bias[vec_itr];
+                    }
                     xtfloat _xtfloatx2_acc_0_0 = (xtfloat)0.0f;
                     xtfloat _xtfloatx2_vec_batch_0  = (xtfloat)0.0f ;
                     xtfloat *_xtfloatx2_p_vec_batch_0  = (xtfloat *)(&p_vec[(vec_itr)*vec_offset]);
diff --git a/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym8sxasym8s_asym8s_circ.c b/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym8sxasym8s_asym8s_circ.c
index 20f54fe..7c9b542 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym8sxasym8s_asym8s_circ.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym8sxasym8s_asym8s_circ.c
@@ -1133,10 +1133,6 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
       return -1;
     }
   }
-  if (!p_bias)
-  {
-    return -1;
-  }
 
 #ifndef AE_MULAZB8Q8X8
   int c_itr = 0;
@@ -1167,15 +1163,20 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
     int out_stride = out_row_offset;
 
     ae_int32x4 *pt_bias = (ae_int32x4 *)p_bias;
-    ae_valignx2 align_p_bias = AE_LA128_PP(pt_bias);
-    
+    ae_valignx2 align_p_bias;
+    if(p_bias != NULL) {
+      align_p_bias = AE_LA128_PP(pt_bias);
+    }
     ae_int32x4 *pt_out_mult = (ae_int32x4 *)p_out_multiplier;
     ae_valignx2 align_p_out_mult = AE_LA128_PP(pt_out_mult);
     // Process loop for 4 rows and 4 vectors 
     for(vec_itr = 0; vec_itr < (vec_count & ~(4-1)); vec_itr += 4)
     {
 #ifdef AE_MULAZB8Q8X8
-      AE_LA32X2X2_IP(d_bias_0, d_bias_1, align_p_bias, (ae_int32x4 *)pt_bias);
+      d_bias_0 = d_bias_1 = 0;
+      if(p_bias != NULL){
+        AE_LA32X2X2_IP(d_bias_0, d_bias_1, align_p_bias, (ae_int32x4 *)pt_bias);
+      }
       AE_S32X2X2_I(d_bias_0, d_bias_1, (ae_int32x4 *)bias_buffer, 0);
 #endif    
       WORD8* p_dst_0 = (WORD8*)p_out + (vec_itr + 0);
@@ -1210,8 +1211,10 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
       AE_MULA8Q8X8(acc_row0, acc_row1, vec0_batch_0, vec1_batch_0, vec2_batch_0, vec3_batch_0, mat_z_b);
       AE_MULA8Q8X8(acc_row0, acc_row1, vec0_batch_1, vec1_batch_1, vec2_batch_1, vec3_batch_1, mat_z_b);
  
-      ae_int32x2 d_bias_0, d_bias_1;
-      AE_LA32X2X2_IP(d_bias_0, d_bias_1, align_p_bias, (ae_int32x4 *)pt_bias);
+      ae_int32x2 d_bias_0 = 0, d_bias_1 = 0;
+      if(p_bias != NULL){
+        AE_LA32X2X2_IP(d_bias_0, d_bias_1, align_p_bias, (ae_int32x4 *)pt_bias);
+      }
       acc_row0 = AE_SUB32S(d_bias_0, acc_row0);
       acc_row1 = AE_SUB32S(d_bias_1, acc_row1);
       AE_S32X2X2_I(acc_row0, acc_row1, (ae_int32x4 *)bias_buffer, 0);
@@ -1349,16 +1352,20 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
       AE_MULA8Q8X8(acc_row1, acc_row0, vec3_batch_2, vec2_batch_2, vec1_batch_2, vec0_batch_2, mat_z_b);
       AE_MULA8Q8X8(acc_row1, acc_row0, vec3_batch_3, vec2_batch_3, vec1_batch_3, vec0_batch_3, mat_z_b);
  
-      ae_int32x2 d_bias_0, d_bias_1;
-      d_bias_1 = AE_MOVDA32X2(p_bias[vec_itr + 3], p_bias[vec_itr + 2]);
-      d_bias_0 = AE_MOVDA32X2(p_bias[vec_itr + 1], p_bias[vec_itr + 0]);
-      
+      ae_int32x2 d_bias_0 = 0, d_bias_1 = 0;
+      if(p_bias != NULL){
+        d_bias_1 = AE_MOVDA32X2(p_bias[vec_itr + 3], p_bias[vec_itr + 2]);
+        d_bias_0 = AE_MOVDA32X2(p_bias[vec_itr + 1], p_bias[vec_itr + 0]);
+      }
       acc_row0 = AE_SUB32S(d_bias_0, acc_row0);
       acc_row1 = AE_SUB32S(d_bias_1, acc_row1);
       AE_S32X2X2_I(acc_row0, acc_row1, (ae_int32x4 *)bias_buffer, 0);
 #else
-      ae_int32x2 acc_row0 = AE_MOVDA32X2(p_bias[vec_itr + 1], p_bias[vec_itr + 0]);
-      ae_int32x2 acc_row1 = AE_MOVDA32X2(p_bias[vec_itr + 3], p_bias[vec_itr + 2]);
+      ae_int32x2 acc_row0 = 0, acc_row1 = 0;
+      if(p_bias != NULL){
+        acc_row0 = AE_MOVDA32X2(p_bias[vec_itr + 1], p_bias[vec_itr + 0]);
+        acc_row1 = AE_MOVDA32X2(p_bias[vec_itr + 3], p_bias[vec_itr + 2]);
+      }
       AE_S32X2X2_I(acc_row0, acc_row1, (ae_int32x4 *)bias_buffer, 0);
 #endif
       
@@ -1522,11 +1529,13 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
       AE_S32X2X2_I(acc_row0, acc_row1, (ae_int32x4 *)bias_buffer, 0);
 #else
       ae_int32x2 d_bias_0, d_bias_1;
-      ae_int32x2 bias_0, bias_1, bias_2, bias_3;
-      bias_3 = AE_L32_I((ae_int32 *)p_bias, 12);
-      bias_2 = AE_L32_I((ae_int32 *)p_bias, 8);
-      bias_1 = AE_L32_I((ae_int32 *)p_bias, 4);
-      AE_L32_IP(bias_0, (ae_int32 *)p_bias, 16);
+      ae_int32x2 bias_0 = 0, bias_1 = 0, bias_2 = 0, bias_3 = 0;
+      if(p_bias != NULL){
+        bias_3 = AE_L32_I((ae_int32 *)p_bias, 12);
+        bias_2 = AE_L32_I((ae_int32 *)p_bias, 8);
+        bias_1 = AE_L32_I((ae_int32 *)p_bias, 4);
+        AE_L32_IP(bias_0, (ae_int32 *)p_bias, 16);
+      }
 
       d_bias_0 = AE_SEL32_HH(bias_1, bias_0);
       d_bias_1 = AE_SEL32_HH(bias_3, bias_2);
@@ -1730,8 +1739,13 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
         AE_MULA8Q8X8(acc_row0 , acc_row1 , vec0_0 , vec1_0 , vec2_0 , vec3_0 , mat_z_b);
       }
 
-      acc_row0 = AE_SUB32S(AE_MOVDA32X2(p_bias[vec_itr + 0], p_bias[vec_itr + 1]), acc_row0);
-      acc_row1 = AE_SUB32S(AE_MOVDA32X2(p_bias[vec_itr + 2], p_bias[vec_itr + 3]), acc_row1);
+      ae_int32x2 bias_01 = 0, bias_23 = 0;
+      if(p_bias != NULL){
+        bias_01 = AE_MOVDA32X2(p_bias[vec_itr + 0], p_bias[vec_itr + 1]);
+        bias_23 = AE_MOVDA32X2(p_bias[vec_itr + 2], p_bias[vec_itr + 3]);
+      }
+      acc_row0 = AE_SUB32S(bias_01, acc_row0);
+      acc_row1 = AE_SUB32S(bias_23, acc_row1);
       AE_S32X2X2_I(AE_MOVDA32(AE_MOVAD32_H(acc_row0)), AE_MOVDA32(AE_MOVAD32_L(acc_row0)), (ae_int32x4*)acc_buffer, 0);
       AE_S32X2X2_I(AE_MOVDA32(AE_MOVAD32_H(acc_row1)), AE_MOVDA32(AE_MOVAD32_L(acc_row1)), (ae_int32x4*)acc_buffer, 16);
 #endif
@@ -1820,14 +1834,24 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
         AE_L32X2X2_I(acc_row0_vec2, acc_row0_vec3, (ae_int32x4*)acc_buffer, 16);
         AE_L32X2X2_I(acc_row1_vec2, acc_row1_vec3, (ae_int32x4*)acc_buffer, 16);
 #else
-        ae_int32x2 acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
-        ae_int32x2 acc_row1_vec0 = acc_row0_vec0;
-        ae_int32x2 acc_row0_vec1 = AE_MOVDA32(p_bias[vec_itr + 1]);
-        ae_int32x2 acc_row1_vec1 = acc_row0_vec1;
-        ae_int32x2 acc_row0_vec2 = AE_MOVDA32(p_bias[vec_itr + 2]);
-        ae_int32x2 acc_row1_vec2 = acc_row0_vec2;
-        ae_int32x2 acc_row0_vec3 = AE_MOVDA32(p_bias[vec_itr + 3]);
-        ae_int32x2 acc_row1_vec3 = acc_row0_vec3;
+        ae_int32x2 acc_row0_vec0 = 0;
+        ae_int32x2 acc_row1_vec0 = 0;
+        ae_int32x2 acc_row0_vec1 = 0;
+        ae_int32x2 acc_row1_vec1 = 0;
+        ae_int32x2 acc_row0_vec2 = 0;
+        ae_int32x2 acc_row1_vec2 = 0;
+        ae_int32x2 acc_row0_vec3 = 0;
+        ae_int32x2 acc_row1_vec3 = 0;
+        if(p_bias != NULL){
+          acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+          acc_row1_vec0 = acc_row0_vec0;
+          acc_row0_vec1 = AE_MOVDA32(p_bias[vec_itr + 1]);
+          acc_row1_vec1 = acc_row0_vec1;
+          acc_row0_vec2 = AE_MOVDA32(p_bias[vec_itr + 2]);
+          acc_row1_vec2 = acc_row0_vec2;
+          acc_row0_vec3 = AE_MOVDA32(p_bias[vec_itr + 3]);
+          acc_row1_vec3 = acc_row0_vec3;
+        }
 #endif
         
         ae_int8* p_vec_0  = (ae_int8 *)(p_vec1 + vec_itr * vec_stride);
@@ -1893,8 +1917,11 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
         ae_int32x2 acc_row0_vec0 = acc_row0;
         ae_int32x2 acc_row1_vec0 = acc_row1;
 #else
-        ae_int32x2 acc_row0_vec0 = AE_MOVDA32X2(p_bias[vec_itr + 0], p_bias[vec_itr + 1]);
-        ae_int32x2 acc_row1_vec0 = AE_MOVDA32X2(p_bias[vec_itr + 2], p_bias[vec_itr + 3]);
+        ae_int32x2 acc_row0_vec0 = 0, acc_row1_vec0 = 0;
+        if(p_bias != NULL){
+          acc_row0_vec0 = AE_MOVDA32X2(p_bias[vec_itr + 0], p_bias[vec_itr + 1]);
+          acc_row1_vec0 = AE_MOVDA32X2(p_bias[vec_itr + 2], p_bias[vec_itr + 3]);
+        }
 #endif
 
         ae_int8* p_vec_0  = (ae_int8*)(p_vec1 + vec_itr * vec_stride);
@@ -1971,10 +1998,17 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
       for (m_itr = 0; m_itr < (rows & ~(4 - 1)); m_itr += 4)
       {
 #ifndef AE_MULAZB8Q8X8
-        ae_int32x2 acc_row0_vec0 = AE_SUB32S(AE_MOVDA32(p_bias[vec_itr]), AE_MOVDA32(AE_MOVAD32_H(acc_row0)));
-        ae_int32x2 acc_row1_vec0 = AE_SUB32S(AE_MOVDA32(p_bias[vec_itr]), AE_MOVDA32(AE_MOVAD32_H(acc_row1)));
+        ae_int32x2 bias_0 = 0;
+        if(p_bias != NULL){
+          bias_0 = p_bias[vec_itr];
+        }
+        ae_int32x2 acc_row0_vec0 = AE_SUB32S(bias_0, AE_MOVDA32(AE_MOVAD32_H(acc_row0)));
+        ae_int32x2 acc_row1_vec0 = AE_SUB32S(bias_0, AE_MOVDA32(AE_MOVAD32_H(acc_row1)));
 #else
-        ae_int32x2 acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+        ae_int32x2 acc_row0_vec0 = 0;
+        if(p_bias != NULL){
+          acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+        }
         ae_int32x2 acc_row1_vec0 = acc_row0_vec0;
 #endif
 
@@ -2009,10 +2043,17 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
       for (; m_itr < rows; m_itr++)
       {
 #ifndef AE_MULAZB8Q8X8
-        ae_int32x2 acc_row0_vec0 = AE_SUB32S(AE_MOVDA32(p_bias[vec_itr]), acc_row0);
-        ae_int32x2 acc_row1_vec0 = AE_SUB32S(AE_MOVDA32(p_bias[vec_itr]), acc_row1);
+        ae_int32x2 bias_0 = 0;
+        if(p_bias != NULL){
+          bias_0 = p_bias[vec_itr];
+        }
+        ae_int32x2 acc_row0_vec0 = AE_SUB32S(bias_0, acc_row0);
+        ae_int32x2 acc_row1_vec0 = AE_SUB32S(bias_0, acc_row1);
 #else
-        ae_int32x2 acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+        ae_int32x2 acc_row0_vec0 = 0;
+        if(p_bias != NULL){
+          acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+        }
         ae_int32x2 acc_row1_vec0 = acc_row0_vec0;
 #endif
 
@@ -2101,8 +2142,13 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
           }
         }
 
-        acc_row0 = AE_SUB32S(AE_MOVDA32X2(p_bias[vec_itr + ii +  0], p_bias[vec_itr + ii +  8]), acc_row0);
-        acc_row1 = AE_SUB32S(AE_MOVDA32X2(p_bias[vec_itr + ii + 16], p_bias[vec_itr + ii + 24]), acc_row1);
+        ae_int32x2 bias_01 = 0, bias_23 = 0;
+        if(p_bias != NULL){
+          bias_01 = AE_MOVDA32X2(p_bias[vec_itr + ii +  0], p_bias[vec_itr + ii +  8]);
+          bias_23 = AE_MOVDA32X2(p_bias[vec_itr + ii + 16], p_bias[vec_itr + ii + 24]);
+        }
+        acc_row0 = AE_SUB32S(bias_01, acc_row0);
+        acc_row1 = AE_SUB32S(bias_23, acc_row1);
         AE_S32X2X2_I(AE_MOVDA32(AE_MOVAD32_H(acc_row0)), AE_MOVDA32(AE_MOVAD32_L(acc_row0)), (ae_int32x4*)acc_buffer, 0);
         AE_S32X2X2_I(AE_MOVDA32(AE_MOVAD32_H(acc_row1)), AE_MOVDA32(AE_MOVAD32_L(acc_row1)), (ae_int32x4*)acc_buffer, 16);
 #endif
@@ -2180,14 +2226,24 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
           AE_L32X2X2_I(acc_row0_vec2, acc_row0_vec3, (ae_int32x4*)acc_buffer, 16);
           AE_L32X2X2_I(acc_row1_vec2, acc_row1_vec3, (ae_int32x4*)acc_buffer, 16);
 #else
-          ae_int32x2 acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + ii + 0]);
-          ae_int32x2 acc_row1_vec0 = acc_row0_vec0;
-          ae_int32x2 acc_row0_vec1 = AE_MOVDA32(p_bias[vec_itr + ii + 8]);
-          ae_int32x2 acc_row1_vec1 = acc_row0_vec1;
-          ae_int32x2 acc_row0_vec2 = AE_MOVDA32(p_bias[vec_itr + ii + 16]);
-          ae_int32x2 acc_row1_vec2 = acc_row0_vec2;
-          ae_int32x2 acc_row0_vec3 = AE_MOVDA32(p_bias[vec_itr + ii + 24]);
-          ae_int32x2 acc_row1_vec3 = acc_row0_vec3;
+          ae_int32x2 acc_row0_vec0 = 0;
+          ae_int32x2 acc_row1_vec0 = 0;
+          ae_int32x2 acc_row0_vec1 = 0;
+          ae_int32x2 acc_row1_vec1 = 0;
+          ae_int32x2 acc_row0_vec2 = 0;
+          ae_int32x2 acc_row1_vec2 = 0;
+          ae_int32x2 acc_row0_vec3 = 0;
+          ae_int32x2 acc_row1_vec3 = 0;
+          if(p_bias != NULL){
+            acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + ii + 0]);
+            acc_row1_vec0 = acc_row0_vec0;
+            acc_row0_vec1 = AE_MOVDA32(p_bias[vec_itr + ii + 8]);
+            acc_row1_vec1 = acc_row0_vec1;
+            acc_row0_vec2 = AE_MOVDA32(p_bias[vec_itr + ii + 16]);
+            acc_row1_vec2 = acc_row0_vec2;
+            acc_row0_vec3 = AE_MOVDA32(p_bias[vec_itr + ii + 24]);
+            acc_row1_vec3 = acc_row0_vec3;
+          }
 #endif
           
           ae_int8* p_vec_0  = (ae_int8 *)(p_vec1 + (vec_itr + ii) * vec_stride);
@@ -2257,8 +2313,11 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
           ae_int32x2 acc_row0_vec0 = acc_row0;
           ae_int32x2 acc_row1_vec0 = acc_row1;
 #else
-          ae_int32x2 acc_row0_vec0 = AE_MOVDA32X2(p_bias[vec_itr + ii + 0], p_bias[vec_itr + ii + 8]);
-          ae_int32x2 acc_row1_vec0 = AE_MOVDA32X2(p_bias[vec_itr + ii + 16], p_bias[vec_itr + ii + 24]);
+          ae_int32x2 acc_row0_vec0 = 0, acc_row1_vec0 = 0;
+          if(p_bias != NULL){
+            acc_row0_vec0 = AE_MOVDA32X2(p_bias[vec_itr + ii + 0], p_bias[vec_itr + ii + 8]);
+            acc_row1_vec0 = AE_MOVDA32X2(p_bias[vec_itr + ii + 16], p_bias[vec_itr + ii + 24]);
+          }
 #endif
 
           ae_int8* p_vec_0  = (ae_int8 *)(p_vec1 + (vec_itr + ii) * vec_stride);
@@ -2337,9 +2396,13 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
           AE_MULA8Q8X8(acc_row0 , acc_row1 , vec0_1 , vec1_1 , vec2_1 , vec3_1 , mat_z_b);
         }
       }
-      
-      acc_row0 = AE_SUB32S(AE_MOVDA32X2(p_bias[vec_itr + 0], p_bias[vec_itr + 1]), acc_row0);
-      acc_row1 = AE_SUB32S(AE_MOVDA32X2(p_bias[vec_itr + 2], p_bias[vec_itr + 3]), acc_row1);
+      ae_int32x2 bias_01 = 0, bias_23 = 0;
+      if(p_bias != NULL){
+        bias_01 = AE_MOVDA32X2(p_bias[vec_itr + 0], p_bias[vec_itr + 1]);
+        bias_23 = AE_MOVDA32X2(p_bias[vec_itr + 2], p_bias[vec_itr + 3]);
+      }
+      acc_row0 = AE_SUB32S(bias_01, acc_row0);
+      acc_row1 = AE_SUB32S(bias_23, acc_row1);
       AE_S32X2X2_I(AE_MOVDA32(AE_MOVAD32_H(acc_row0)), AE_MOVDA32(AE_MOVAD32_L(acc_row0)), (ae_int32x4*)acc_buffer, 0);
       AE_S32X2X2_I(AE_MOVDA32(AE_MOVAD32_H(acc_row1)), AE_MOVDA32(AE_MOVAD32_L(acc_row1)), (ae_int32x4*)acc_buffer, 16);
 #endif
@@ -2418,14 +2481,25 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
         AE_L32X2X2_I(acc_row0_vec2, acc_row0_vec3, (ae_int32x4*)acc_buffer, 16);
         AE_L32X2X2_I(acc_row1_vec2, acc_row1_vec3, (ae_int32x4*)acc_buffer, 16);
 #else
-        ae_int32x2 acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
-        ae_int32x2 acc_row1_vec0 = acc_row0_vec0;
-        ae_int32x2 acc_row0_vec1 = AE_MOVDA32(p_bias[vec_itr + 1]);
-        ae_int32x2 acc_row1_vec1 = acc_row0_vec1;
-        ae_int32x2 acc_row0_vec2 = AE_MOVDA32(p_bias[vec_itr + 2]);
-        ae_int32x2 acc_row1_vec2 = acc_row0_vec2;
-        ae_int32x2 acc_row0_vec3 = AE_MOVDA32(p_bias[vec_itr + 3]);
-        ae_int32x2 acc_row1_vec3 = acc_row0_vec3;
+        ae_int32x2 acc_row0_vec0 = 0;
+        ae_int32x2 acc_row1_vec0 = 0;
+        ae_int32x2 acc_row0_vec1 = 0;
+        ae_int32x2 acc_row1_vec1 = 0;
+        ae_int32x2 acc_row0_vec2 = 0;
+        ae_int32x2 acc_row1_vec2 = 0;
+        ae_int32x2 acc_row0_vec3 = 0;
+        ae_int32x2 acc_row1_vec3 = 0;
+
+        if(p_bias != NULL){
+          acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+          acc_row1_vec0 = acc_row0_vec0;
+          acc_row0_vec1 = AE_MOVDA32(p_bias[vec_itr + 1]);
+          acc_row1_vec1 = acc_row0_vec1;
+          acc_row0_vec2 = AE_MOVDA32(p_bias[vec_itr + 2]);
+          acc_row1_vec2 = acc_row0_vec2;
+          acc_row0_vec3 = AE_MOVDA32(p_bias[vec_itr + 3]);
+          acc_row1_vec3 = acc_row0_vec3;
+        }
 #endif
         
         ae_int8* p_vec_0  = (ae_int8 *)(p_vec1 + vec_itr * vec_stride);
@@ -2494,8 +2568,11 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
         ae_int32x2 acc_row0_vec0 = acc_row0;
         ae_int32x2 acc_row1_vec0 = acc_row1;
 #else
-        ae_int32x2 acc_row0_vec0 = AE_MOVDA32X2(p_bias[vec_itr + 0], p_bias[vec_itr + 1]);
-        ae_int32x2 acc_row1_vec0 = AE_MOVDA32X2(p_bias[vec_itr + 2], p_bias[vec_itr + 3]);
+        ae_int32x2 acc_row0_vec0 = 0, acc_row1_vec0 = 0;
+        if(p_bias != NULL){
+          acc_row0_vec0 = AE_MOVDA32X2(p_bias[vec_itr + 0], p_bias[vec_itr + 1]);
+          acc_row1_vec0 = AE_MOVDA32X2(p_bias[vec_itr + 2], p_bias[vec_itr + 3]);
+        }
 #endif
 
         ae_int8* p_vec_0  = (ae_int8*)(p_vec1 + vec_itr * vec_stride);
@@ -2558,7 +2635,11 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
           AE_MULA8Q8X8(acc_row0 , acc_row1 , vec0_1 , vec0_1 , vec0_1 , vec0_1 , mat_z_b);
         }
       }
-      acc_row0 = AE_SUB32S(AE_MOVDA32(p_bias[vec_itr]), acc_row0);
+      ae_int32x2 bias_0 = 0;
+      if(p_bias != NULL){
+        bias_0 = p_bias[vec_itr];
+      }
+      acc_row0 = AE_SUB32S(bias_0, acc_row0);
 #endif
 
       WORD8* p_dst = (WORD8*)p_out + (vec_itr + 0) * out_offset;
@@ -2582,7 +2663,10 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
         ae_int32x2 acc_row0_vec0 = acc_row0;
         ae_int32x2 acc_row1_vec0 = acc_row0;
 #else
-        ae_int32x2 acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+        ae_int32x2 acc_row0_vec0 = 0;
+        if(p_bias != NULL){
+          acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+        }
         ae_int32x2 acc_row1_vec0 = acc_row0_vec0;
 #endif
 
@@ -2623,7 +2707,10 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
         ae_int32x2 acc_row0_vec0 = acc_row0;
         ae_int32x2 acc_row1_vec0 = acc_row0;
 #else
-        ae_int32x2 acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+        ae_int32x2 acc_row0_vec0 = 0;
+        if(p_bias != NULL){
+          acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+        }
         ae_int32x2 acc_row1_vec0 = acc_row0_vec0;
 #endif
 
diff --git a/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym8sxsym16s_sym16s_circ.c b/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym8sxsym16s_sym16s_circ.c
index a12700a..0c53340 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym8sxsym16s_sym16s_circ.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym8sxsym16s_sym16s_circ.c
@@ -1181,10 +1181,6 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
       return -1;
     }
   }
-  if (!p_bias)
-  {
-    return -1;
-  }
 
   /* Special conv2d case when k_h*k_w*i_c < 9, o_c is multiple of 4 and out_data_format = NHWC */
   if(cols1 < 9 && (vec_count & 0x3) == 0 && (out_col_offset == 1))
@@ -1200,10 +1196,14 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
     // Process loop for 4 vectors 
     for(vec_itr = 0; vec_itr < (vec_count & ~(4-1)); vec_itr += 4)
     {
-      ae_int64 bias_0 = p_bias[vec_itr + 0];
-      ae_int64 bias_1 = p_bias[vec_itr + 1];
-      ae_int64 bias_2 = p_bias[vec_itr + 2];
-      ae_int64 bias_3 = p_bias[vec_itr + 3];
+      ae_int64 bias_0, bias_1, bias_2, bias_3;
+      bias_0 = bias_1 = bias_2 = bias_3 = 0;
+      if(p_bias != NULL){
+        bias_0 = p_bias[vec_itr + 0];
+        bias_1 = p_bias[vec_itr + 1];
+        bias_2 = p_bias[vec_itr + 2];
+        bias_3 = p_bias[vec_itr + 3];
+      }
       
       WORD16* p_dst_0 = (WORD16*)p_out + (vec_itr + 0) * out_offset;
       WORD16* p_dst_1 = (WORD16*)p_out + (vec_itr + 1) * out_offset;
@@ -1294,10 +1294,14 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
     // Process loop for unroll of 4 vectors
     for(; vec_itr < (vec_count & ~(4-1)); vec_itr += 4)
     {
-      ae_int64 bias_0 = p_bias[vec_itr + 0];
-      ae_int64 bias_1 = p_bias[vec_itr + 1];
-      ae_int64 bias_2 = p_bias[vec_itr + 2];
-      ae_int64 bias_3 = p_bias[vec_itr + 3];
+      ae_int64 bias_0, bias_1, bias_2, bias_3;
+      bias_0 = bias_1 = bias_2 = bias_3 = 0;
+      if(p_bias != NULL){
+        bias_0 = p_bias[vec_itr + 0];
+        bias_1 = p_bias[vec_itr + 1];
+        bias_2 = p_bias[vec_itr + 2];
+        bias_3 = p_bias[vec_itr + 3];
+      }
 
       WORD16* p_dst_0 = (WORD16*)p_out + (vec_itr + 0) * out_offset;
       WORD16* p_dst_1 = (WORD16*)p_out + (vec_itr + 1) * out_offset;
@@ -1500,8 +1504,12 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
     // Process loop for unroll of 2 vectors
     for(; vec_itr < (vec_count & ~(2-1)); vec_itr += 2)
     {
-      ae_int64 bias_0 = p_bias[vec_itr + 0];
-      ae_int64 bias_1 = p_bias[vec_itr + 1];
+      ae_int64 bias_0, bias_1;
+      bias_0 = bias_1 = 0;
+      if(p_bias != NULL){
+        bias_0 = p_bias[vec_itr + 0];
+        bias_1 = p_bias[vec_itr + 1];
+      }
 
       WORD16* p_dst_0 = (WORD16*)p_out + (vec_itr + 0) * out_offset;
       WORD16* p_dst_1 = (WORD16*)p_out + (vec_itr + 1) * out_offset;
@@ -1586,7 +1594,10 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
     // remaining vectors
     for(; vec_itr < vec_count; vec_itr++)
     {
-      ae_int64 bias_0 = p_bias[vec_itr + 0];
+      ae_int64 bias_0 = 0;
+      if(p_bias != NULL){
+        bias_0 = p_bias[vec_itr + 0];
+      }
       WORD16* p_dst = (WORD16*)p_out + (vec_itr + 0) * out_offset;
       m_itr = 0;
 
@@ -1664,11 +1675,15 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
 
     // Process loop for unroll of 4 vectors
     for(; vec_itr < (vec_count & ~(4-1)); vec_itr += 4)
-    {
-      ae_int64 bias_0 = p_bias[vec_itr + 0];
-      ae_int64 bias_1 = p_bias[vec_itr + 1];
-      ae_int64 bias_2 = p_bias[vec_itr + 2];
-      ae_int64 bias_3 = p_bias[vec_itr + 3];
+    {
+      ae_int64 bias_0, bias_1, bias_2, bias_3;
+      bias_0 = bias_1 = bias_2 = bias_3 = 0;
+      if(p_bias != NULL){
+        bias_0 = p_bias[vec_itr + 0];
+        bias_1 = p_bias[vec_itr + 1];
+        bias_2 = p_bias[vec_itr + 2];
+        bias_3 = p_bias[vec_itr + 3];
+      }
 
       WORD16* p_dst_0 = (WORD16*)p_out + (vec_itr + 0) * out_offset;
       WORD16* p_dst_1 = (WORD16*)p_out + (vec_itr + 1) * out_offset;
@@ -1870,8 +1885,12 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
     // Process loop for unroll of 2 vectors
     for(; vec_itr < (vec_count & ~(2-1)); vec_itr += 2)
     {
-      ae_int64 bias_0 = p_bias[vec_itr + 0];
-      ae_int64 bias_1 = p_bias[vec_itr + 1];
+      ae_int64 bias_0, bias_1;
+      bias_0 = bias_1 = 0;
+      if(p_bias != NULL){
+        bias_0 = p_bias[vec_itr + 0];
+        bias_1 = p_bias[vec_itr + 1];
+      }
 
       WORD16* p_dst_0 = (WORD16*)p_out + (vec_itr + 0) * out_offset;
       WORD16* p_dst_1 = (WORD16*)p_out + (vec_itr + 1) * out_offset;
@@ -1956,7 +1975,10 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
     // remaining vectors
     for(; vec_itr < vec_count; vec_itr++)
     {
-      ae_int64 bias_0 = p_bias[vec_itr + 0];
+      ae_int64 bias_0 = 0;
+      if(p_bias != NULL){
+        bias_0 = p_bias[vec_itr + 0];
+      }
       WORD16* p_dst = (WORD16*)p_out + (vec_itr + 0) * out_offset;
       m_itr = 0;
 
diff --git a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_circ_buf.c b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_circ_buf.c
index 1135ba0..2c2cd33 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_circ_buf.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_circ_buf.c
@@ -111,6 +111,7 @@ WORD32 xa_nn_transpose_conv_getsize
     return total_size;
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 VOID xa_nn_transpose_conv_init_state(
     VOID *p_scratch,
     VOID *p_kernel,
@@ -165,3 +166,5 @@ VOID xa_nn_transpose_conv_init_state(
   AE_SETCEND0(p_state->cir_buf.p_end);
 
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
+
diff --git a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_f32.c b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_f32.c
index 6e6510d..f9c8bbb 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_f32.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_f32.c
@@ -37,7 +37,7 @@ DISCARD_FUN_FOR_NONVOID_RETURN(WORD32, xa_nn_transpose_conv_f32, (FLOAT32* outpu
             int filter_height, int filter_width,
             int output_height, int output_width,
             int num_elements,
-            FLOAT32* scratch_buffer))
+            void* scratch_buffer))
 #else
 static inline void tconv2d_f32(FLOAT32* output_data,
     const FLOAT32* input_data,
@@ -407,7 +407,10 @@ static inline void tconv_pad_f32(
       xtfloat q1;
       for(k = 0; k < out_channels; k++)
       {
-        AE_LSIP(q1, pbias, 4);
+        q1 = 0.0f;
+        if(p_bias!= NULL){
+          AE_LSIP(q1, pbias, 4);
+        }
         AE_SSXP(q1, ptrout, out_channels_offset*sizeof(FLOAT32));
       }
     }
@@ -626,7 +629,7 @@ int xa_nn_transpose_conv_f32(FLOAT32* output_data,
     int filter_height, int filter_width,
     int output_height, int output_width,
     int num_elements,
-    FLOAT32* scratch_buffer)
+    void* scratch_buffer)
 {
   /* NULL pointer checks */
   XA_NNLIB_ARG_CHK_PTR(output_data, -1);
diff --git a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c
index 7c0e8be..c59830e 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c
@@ -280,8 +280,11 @@ static inline void tconv2d_sym8sxasym8s(WORD8* output_data,
   int loop_cnt = ((output_depth & 1) || ((unsigned int)output_data & 3)) ? 0 : output_depth;
   for (out_channel = 0; out_channel < loop_cnt; out_channel+=2)
   {
-    dbias0 = AE_MOVDA32X2(pbias[out_channel], pbias[out_channel+1]);
-    
+    ae_int32x2 dbias0_l, dbias0_h;
+    AE_L32_XP(dbias0_h, pbias, bias_offset);
+    AE_L32_XP(dbias0_l, pbias, bias_offset);
+    dbias0 = AE_SEL32_LL(dbias0_h, dbias0_l);
+
     ae_int32 *pscratch0 = (ae_int32*)&scratch_buffer[out_channel];
     ae_int32 *pscratch1 = pscratch0 + output_depth; 
     ae_int8 *pout0 = (ae_int8*)&output_data[out_channel];
@@ -484,7 +487,10 @@ static inline void tconv_pad(
       ae_int32x2 q1;
       for(k = 0; k < out_channels; k++)
       {
-        AE_L32_IP(q1, pbias, 4);
+        q1 = 0;
+        if(p_bias != NULL){
+          AE_L32_IP(q1, pbias, 4);
+        }
         ae_int32x2 acc;
         int left_shift, right_shift;
 #if TFLITE_SINGLE_ROUNDING
@@ -727,7 +733,7 @@ int xa_nn_transpose_conv_sym8sxasym8s(WORD8* output_data,
 		int num_elements,
 		int input_offset, int output_offset,
 		int *output_shift, int *output_multiplier,
-		int32_t* scratch_buffer)
+		void* scratch_buffer)
 {
 	/* NULL pointer checks */
 	XA_NNLIB_ARG_CHK_PTR(output_data, -1);
diff --git a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxsym16s.c b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxsym16s.c
index fa846cc..afbb261 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxsym16s.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxsym16s.c
@@ -449,7 +449,10 @@ static inline void tconv_pad(
       ae_int64 q1;
       for(k = 0; k < out_channels; k++)
       {
-        AE_L64_IP(q1, pbias, 8);
+        q1 = 0;
+        if(p_bias != NULL){
+          AE_L64_IP(q1, pbias, 8);
+        }
         ae_int32x2 acc;
         MPY_BY_QUANT_MULT_ACC64_OUT32(acc, q1, p_out_multiplier[k], p_out_shift[k]);
         d1 = AE_SAT16X4(acc, acc);
@@ -681,7 +684,7 @@ int xa_nn_transpose_conv_sym8sxsym16s(WORD16* output_data,
 		int output_height, int output_width,
 		int num_elements,
 		int *output_shift, int *output_multiplier,
-		int64_t* scratch_buffer)
+		void* scratch_buffer)
 {
 	/* NULL pointer checks */
 	XA_NNLIB_ARG_CHK_PTR(output_data, -1);
diff --git a/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c b/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c
index 05d4fe8..ce20e25 100644
--- a/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c
+++ b/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c
@@ -1368,6 +1368,173 @@ WORD32 xa_nn_matmul_asym8sxasym8s_asym8s(
       }
     }
   }
+  else if(cols1 <= 16 &&
+    cols1 == row_stride1 &&
+    cols1 == vec_offset &&
+    out_stride == 1)
+  {
+    ae_int32x2 acc_row01_vec0, acc_row23_vec0;
+    ae_int32x2 acc_row01_vec1, acc_row23_vec1;
+
+    ae_int8x16 *p_mat1_0;
+    ae_int8x16 *p_vec_0;
+
+    int batch_row = rows >> 2;
+    int batch_vec = vec_count >> 1;
+
+    p_mat1_0 = (ae_int8x16 *)p_mat1;
+
+    ae_valignx2 align_p_mat1_0;
+    align_p_mat1_0 = AE_LA128_PP(p_mat1_0);
+
+    AE_MOVZBVCDR(biasvc1);
+    int rem_cols = (cols1 & 15);
+    int rem_g8 = (rem_cols > 8)?1:0;
+
+    ae_int8x8 sel1 = AE_MOVINT8X8_FROMINT32X2(AE_MOVDA32X2(post_loop_sel_pattern[2 * (rem_cols & 7) * !rem_g8], post_loop_sel_pattern[2 * (rem_cols & 7) * !rem_g8 + 1]));
+    ae_int8x8 sel2;
+    if(rem_g8)
+      sel2 = AE_MOVINT8X8_FROMINT32X2(AE_MOVDA32X2(post_loop_sel_pattern[2 * (rem_cols & 7) * rem_g8], post_loop_sel_pattern[2 * (rem_cols & 7) * rem_g8 + 1]));
+    else
+      sel2 = AE_MOVINT8X8_FROMINT32X2(AE_MOVDA32(0));
+
+    ae_int8x8 mat_bias = AE_MOVDA8((WORD8)(-mat1_zero_bias));
+
+    for (m_itr = 0; m_itr < batch_row; m_itr ++)
+    {
+      p_vec_0 = (ae_int8x16 *)p_vec1;
+
+      ae_int8x8 mat1_row0_0, mat1_row1_0, mat1_row2_0, mat1_row3_0;
+      ae_int8x8 mat1_row0_1, mat1_row1_1, mat1_row2_1, mat1_row3_1;
+
+      ae_int8x8 vec0_0, vec0_1, vec1_0, vec1_1;
+
+      ae_valignx2 align_p_vec_0;
+      align_p_vec_0 = AE_LA128_PP(p_vec_0);
+
+      AE_LAV8X8X2_XP(mat1_row0_0, mat1_row0_1, align_p_mat1_0, p_mat1_0, row_stride1);
+      AE_LAV8X8X2_XP(mat1_row1_0, mat1_row1_1, align_p_mat1_0, p_mat1_0, row_stride1);
+      AE_LAV8X8X2_XP(mat1_row2_0, mat1_row2_1, align_p_mat1_0, p_mat1_0, row_stride1);
+      AE_LAV8X8X2_XP(mat1_row3_0, mat1_row3_1, align_p_mat1_0, p_mat1_0, row_stride1);
+
+      mat1_row0_0 = AE_SEL8X8(mat1_row0_0, mat_bias, sel1);
+      mat1_row1_0 = AE_SEL8X8(mat1_row1_0, mat_bias, sel1);
+      mat1_row2_0 = AE_SEL8X8(mat1_row2_0, mat_bias, sel1);
+      mat1_row3_0 = AE_SEL8X8(mat1_row3_0, mat_bias, sel1);
+      mat1_row0_1 = AE_SEL8X8(mat1_row0_1, mat_bias, sel2);
+      mat1_row1_1 = AE_SEL8X8(mat1_row1_1, mat_bias, sel2);
+      mat1_row2_1 = AE_SEL8X8(mat1_row2_1, mat_bias, sel2);
+      mat1_row3_1 = AE_SEL8X8(mat1_row3_1, mat_bias, sel2);
+
+      ae_int32x2 bias_01 = AE_ZERO32(), bias_23 = AE_ZERO32();
+      if(p_bias )
+      {
+        bias_01 = AE_MOVDA32X2(p_bias[4 * m_itr], p_bias[(4 * m_itr + 1)]);
+        bias_23 = AE_MOVDA32X2(p_bias[(4 * m_itr + 2)], p_bias[(4 * m_itr + 3)]);
+      }
+      ae_int8* p_out0_0 = (ae_int8 *)&(p_out[4* m_itr]);
+      ae_int8* p_out0_1 = (ae_int8 *)(p_out0_0 + rows);
+      for(vec_itr = 0; vec_itr < batch_vec; vec_itr++)
+      {
+        acc_row01_vec0 = bias_01;
+        acc_row23_vec0 = bias_23;
+        acc_row01_vec1 = bias_01;
+        acc_row23_vec1 = bias_23;
+
+        AE_LAV8X8X2_XP(vec0_0, vec0_1, align_p_vec_0, p_vec_0, cols1);
+        AE_LAV8X8X2_XP(vec1_0, vec1_1, align_p_vec_0, p_vec_0, cols1);
+
+        MAT_VEC_MAC(acc_row01_vec0 , acc_row23_vec0 , mat1_row0_0 , mat1_row1_0 , mat1_row2_0 , mat1_row3_0 ,vec0_0, -mat1_zero_bias, -vec1_zero_bias);
+        MAT_VEC_MAC(acc_row01_vec0 , acc_row23_vec0 , mat1_row0_1 , mat1_row1_1 , mat1_row2_1 , mat1_row3_1 ,vec0_1, -mat1_zero_bias, -vec1_zero_bias);
+        MAT_VEC_MAC(acc_row01_vec1 , acc_row23_vec1 , mat1_row0_0 , mat1_row1_0 , mat1_row2_0 , mat1_row3_0 ,vec1_0, -mat1_zero_bias, -vec1_zero_bias);
+        MAT_VEC_MAC(acc_row01_vec1 , acc_row23_vec1 , mat1_row0_1 , mat1_row1_1 , mat1_row2_1 , mat1_row3_1 ,vec1_1, -mat1_zero_bias, -vec1_zero_bias);
+
+        ae_int16x4 out_0, out_1;
+
+        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_row01_vec0, acc_row23_vec0, out_multiplier, left_shift, right_shift, out_zero_bias);
+        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_1, acc_row01_vec1, acc_row23_vec1, out_multiplier, left_shift, right_shift, out_zero_bias);
+
+        AE_MINMAX16(out_0, AE_MOVDA16(-128), AE_MOVDA16(127));
+        AE_MINMAX16(out_1, AE_MOVDA16(-128), AE_MOVDA16(127));
+
+        ae_int8x8 temp_vec0 = AE_SAT8X8X16(out_0, out_1);
+        AE_SW_S8_7_XP(temp_vec0, p_out0_0, 1);
+        AE_SW_S8_6_XP(temp_vec0, p_out0_0, 1);
+        AE_SW_S8_5_XP(temp_vec0, p_out0_0, 1);
+        AE_SW_S8_4_XP(temp_vec0, p_out0_0, 2*rows-3);
+        AE_SW_S8_3_XP(temp_vec0, p_out0_1, 1);
+        AE_SW_S8_2_XP(temp_vec0, p_out0_1, 1);
+        AE_SW_S8_1_XP(temp_vec0, p_out0_1, 1);
+        AE_S8_0_XP(temp_vec0, p_out0_1, 2*rows-3);
+      }
+      // reminder vectors
+      if(vec_count & 1)
+      {
+        acc_row01_vec1 = bias_01;
+        acc_row23_vec1 = bias_23;
+
+        AE_LAV8X8X2_XP(vec1_0, vec1_1, align_p_vec_0, p_vec_0, cols1);
+
+        MAT_VEC_MAC(acc_row01_vec1 , acc_row23_vec1 , mat1_row0_0 , mat1_row1_0 , mat1_row2_0 , mat1_row3_0 ,vec1_0, -mat1_zero_bias, -vec1_zero_bias);
+        MAT_VEC_MAC(acc_row01_vec1 , acc_row23_vec1 , mat1_row0_1 , mat1_row1_1 , mat1_row2_1 , mat1_row3_1 ,vec1_1, -mat1_zero_bias, -vec1_zero_bias);
+
+        ae_int16x4 out_1;
+
+        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_1, acc_row01_vec1, acc_row23_vec1, out_multiplier, left_shift, right_shift, out_zero_bias);
+
+        AE_MINMAX16(out_1, AE_MOVDA16(-128), AE_MOVDA16(127));
+
+        ae_int8x8 temp_vec0 = AE_SAT8X8X16(out_1, out_1);
+        AE_SW_S8_7_XP(temp_vec0, p_out0_0, 1);
+        AE_SW_S8_6_XP(temp_vec0, p_out0_0, 1);
+        AE_SW_S8_5_XP(temp_vec0, p_out0_0, 1);
+        AE_SW_S8_4_XP(temp_vec0, p_out0_0, 1);
+      }
+    }
+    //reminder matrix
+    for(m_itr = (rows & (~3)); m_itr < rows; m_itr++)
+    {
+      p_vec_0 = (ae_int8x16 *)p_vec1;
+
+      ae_int8x8 mat1_row3_0;
+      ae_int8x8 mat1_row3_1;
+
+      ae_int8x8 vec0_0, vec0_1;
+
+      ae_valignx2 align_p_vec_0;
+      align_p_vec_0 = AE_LA128_PP(p_vec_0);
+
+      AE_LAV8X8X2_XP(mat1_row3_0, mat1_row3_1, align_p_mat1_0, p_mat1_0, row_stride1);
+
+      mat1_row3_0 = AE_SEL8X8(mat1_row3_0, mat_bias, sel1);
+      mat1_row3_1 = AE_SEL8X8(mat1_row3_1, mat_bias, sel2);
+
+      ae_int32x2 bias = AE_ZERO32();
+      if(p_bias)
+      {
+        bias = AE_MOVDA32(p_bias[m_itr]);
+      }
+      ae_int8* p_out0_0 = (ae_int8 *)&(p_out[m_itr]);
+
+      for(vec_itr = 0; vec_itr < vec_count; vec_itr++)
+      {
+        ae_int32x2 acc_vec0 = bias;
+
+        AE_LAV8X8X2_XP(vec0_0, vec0_1, align_p_vec_0, p_vec_0, cols1);
+
+        MAT_VEC_MAC(acc_vec0 , acc_vec0 , mat1_row3_0 , mat1_row3_0 , mat1_row3_0 , mat1_row3_0 ,vec0_0, -mat1_zero_bias, -vec1_zero_bias);
+        MAT_VEC_MAC(acc_vec0 , acc_vec0 , mat1_row3_1 , mat1_row3_1 , mat1_row3_1 , mat1_row3_1 ,vec0_1, -mat1_zero_bias, -vec1_zero_bias);
+
+        ae_int16x4 out_0;
+
+        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_vec0, acc_vec0, out_multiplier, left_shift, right_shift, out_zero_bias);
+
+        AE_MINMAX16(out_0, AE_MOVDA16(-128), AE_MOVDA16(127));
+        ae_int8x8 temp_vec0 = AE_SAT8X8X16(out_0, out_0);
+        AE_S8_0_XP(temp_vec0, p_out0_0, rows);
+      }
+    }
+  }
   else if (p_mat1 && p_vec1)
   {
     ae_int32x2 acc_row0_vec0, acc_row0_vec1, acc_row0_vec2, acc_row0_vec3;
diff --git a/algo/kernels/pool/hifi5/xa_nn_avgpool.c b/algo/kernels/pool/hifi5/xa_nn_avgpool.c
index fa2bc14..8c2bf54 100644
--- a/algo/kernels/pool/hifi5/xa_nn_avgpool.c
+++ b/algo/kernels/pool/hifi5/xa_nn_avgpool.c
@@ -24,8 +24,9 @@
 #include "xa_nnlib_kernels_api.h"
 #include "xa_nn_avgpool_state.h"
 #include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common_macros_hifi5.h"
 
-WORD32 xa_nn_avgpool_getsize_nchw(
+static WORD32 xa_nn_avgpool_getsize_nchw(
     WORD32 inp_precision,
     WORD32 input_width,
     WORD32 kernel_height,
@@ -86,7 +87,7 @@ WORD32 xa_nn_avgpool_getsize_nchw(
         den_array_size = 0;
     /* Output scratch buffer size */
     full_buf_width = kernel_width + (out_width - 1)*x_stride;
-    full_buf_width = XT_MAX(full_buf_width, ALIGNED_SIZE(x_padding, 2)+input_width);
+    full_buf_width = MAX(full_buf_width, ALIGNED_SIZE(x_padding, 2)+input_width);
     full_buf_width = ALIGNED_SIZE(full_buf_width, ALIGNMENT/2);
     /* Need 2 rows of padded input width as acratch for temp output */
     full_out_width = ALIGNED_SIZE(full_buf_width + kernel_width, 4);
@@ -97,7 +98,7 @@ WORD32 xa_nn_avgpool_getsize_nchw(
     return total_size;
 }
 
-WORD32 xa_nn_avgpool_getsize_nhwc(
+static WORD32 xa_nn_avgpool_getsize_nhwc(
     WORD32 inp_precision,
     WORD32 input_channels,
     WORD32 input_width,
@@ -150,7 +151,7 @@ WORD32 xa_nn_avgpool_getsize_nhwc(
 
         if(kernel_height <= (int)MAX_HEIGHT_16_BIT_ACC) // Accumulation in 16 bit container
         {
-            zero_mem_bytes = XT_MAX(sizeof(UWORD8)*cw_plane_size, sizeof(WORD16)*input_channels);
+            zero_mem_bytes = MAX((int)(sizeof(UWORD8)*cw_plane_size), (int)(sizeof(WORD16)*input_channels));
 
             total_size = ALIGNED_SIZE(sizeof(WORD32)* out_height, ALIGNMENT) +
                          ALIGNED_SIZE(sizeof(WORD32)* out_width, ALIGNMENT) +
@@ -162,7 +163,7 @@ WORD32 xa_nn_avgpool_getsize_nhwc(
         }
         else  // Accumulation in 32 bit container
         {
-            zero_mem_bytes = XT_MAX(sizeof(UWORD8)*cw_plane_size, sizeof(WORD32)*input_channels);
+            zero_mem_bytes = MAX((int)(sizeof(UWORD8)*cw_plane_size), (int)(sizeof(WORD32)*input_channels));
 
             total_size = ALIGNED_SIZE(sizeof(WORD32)*out_height, ALIGNMENT) +
                          ALIGNED_SIZE(sizeof(WORD32)*out_width, ALIGNMENT) +
@@ -179,7 +180,7 @@ WORD32 xa_nn_avgpool_getsize_nhwc(
         int zero_mem_bytes;
 
         cw_plane_size = input_width*input_channels;
-        zero_mem_bytes = XT_MAX(sizeof(WORD16)*cw_plane_size, sizeof(WORD32)*input_channels);
+        zero_mem_bytes = MAX((int)(sizeof(WORD16)*cw_plane_size), (int)(sizeof(WORD32)*input_channels));
 
         total_size = ALIGNED_SIZE(sizeof(WORD32)*out_height, ALIGNMENT) +
             ALIGNED_SIZE(sizeof(WORD32)*out_width, ALIGNMENT) +
@@ -256,6 +257,7 @@ WORD32 xa_nn_avgpool_getsize(
     return scratch_size;
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 VOID xa_nn_avgpool_init(
     WORD32 inp_precision,
     pVOID  p_scratch,
@@ -319,3 +321,4 @@ VOID xa_nn_avgpool_init(
     /* Initialize output scratch pointer */
     p_state->p_tmp_out = (pVOID)ALIGN_PTR(p_mem, ALIGNMENT);
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
diff --git a/algo/kernels/pool/hifi5/xa_nn_maxpool.c b/algo/kernels/pool/hifi5/xa_nn_maxpool.c
index 647ed11..9dac4c8 100644
--- a/algo/kernels/pool/hifi5/xa_nn_maxpool.c
+++ b/algo/kernels/pool/hifi5/xa_nn_maxpool.c
@@ -24,8 +24,9 @@
 #include "xa_nnlib_kernels_api.h"
 #include "xa_nn_maxpool_state.h"
 #include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common_macros_hifi5.h"
 
-WORD32 xa_nn_maxpool_getsize_nchw(
+static WORD32 xa_nn_maxpool_getsize_nchw(
     WORD32 inp_precision,
     WORD32 input_width,
     WORD32 kernel_height,
@@ -76,7 +77,7 @@ WORD32 xa_nn_maxpool_getsize_nchw(
     state_size = ALIGNED_SIZE(sizeof(xa_nn_maxpool_state_t), ALIGNMENT);
     /* Output scratch buffer size */
     full_buf_width = kernel_width + (out_width - 1)*x_stride;
-    full_buf_width = XT_MAX(full_buf_width, x_padding + input_width);
+    full_buf_width = MAX(full_buf_width, x_padding + input_width);
     full_buf_width = ALIGNED_SIZE(full_buf_width, ALIGNMENT/2);
     /* maxpool: Need 2 rows of padded input width as acratch for temp output */
     full_out_width = ALIGNED_SIZE(full_buf_width + kernel_width, 4);
@@ -87,22 +88,22 @@ WORD32 xa_nn_maxpool_getsize_nchw(
     return total_size;
 }
 
-WORD32 xa_nn_maxpool_getsize_nhwc(WORD32  inp_precision,
-                                  WORD32  input_width,
-                                  WORD32  input_channels,
-                                  WORD32 kernel_height,
-                                  WORD32 kernel_width,
-                                  WORD32 x_stride,
-                                  WORD32 y_stride,
-                                  WORD32 x_padding,
-                                  WORD32 out_width)
+static WORD32 xa_nn_maxpool_getsize_nhwc(WORD32  inp_precision,
+                                         WORD32  input_width,
+                                         WORD32  input_channels,
+                                         WORD32 kernel_height,
+                                         WORD32 kernel_width,
+                                         WORD32 x_stride,
+                                         WORD32 y_stride,
+                                         WORD32 x_padding,
+                                         WORD32 out_width)
 {
     int scratch_bytewidth, scratch_size;
 
     int full_buf_width;
     int left_pad_aligned = ALIGNED_SIZE(x_padding, ALIGNMENT);
     full_buf_width = kernel_width + (out_width - 1)*x_stride;
-    full_buf_width = XT_MAX(full_buf_width, x_padding + input_width);
+    full_buf_width = MAX(full_buf_width, x_padding + input_width);
     int right_pad = full_buf_width - (x_padding + input_width);
     full_buf_width = full_buf_width + left_pad_aligned + right_pad + kernel_width;
     
@@ -203,6 +204,7 @@ WORD32 xa_nn_maxpool_getsize(
     return scratch_size;
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 WORD32 xa_nn_maxpool_init(
     WORD32 inp_precision,
     pVOID  p_scratch,
@@ -253,3 +255,4 @@ WORD32 xa_nn_maxpool_init(
     p_state->p_scratch = (pVOID)p_mem;
     return 0;
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
diff --git a/algo/kernels/reorg/hifi5/xa_nn_concat_8.c b/algo/kernels/reorg/hifi5/xa_nn_concat_8.c
new file mode 100644
index 0000000..cae88a1
--- /dev/null
+++ b/algo/kernels/reorg/hifi5/xa_nn_concat_8.c
@@ -0,0 +1,156 @@
+/*******************************************************************************
+* Copyright (c) 2018-2024 Cadence Design Systems, Inc.
+*
+* Permission is hereby granted, free of charge, to any person obtaining
+* a copy of this software and associated documentation files (the
+* "Software"), to use this Software with Cadence processor cores only and
+* not with any other processors and platforms, subject to
+* the following conditions:
+*
+* The above copyright notice and this permission notice shall be included
+* in all copies or substantial portions of the Software.
+*
+* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+* IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+* SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+******************************************************************************/
+#include "xa_type_def.h"
+#include "xa_nn_common.h"
+#include "xa_nnlib_kernels_api.h"
+#include "xa_nnlib_common_macros_hifi5.h"
+#include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common.h"
+
+WORD32 xa_nn_concat_8_8(WORD8 * __restrict__ p_out
+                        ,const WORD32 *const p_out_shape
+                        ,const WORD8 **pp_inps
+                        ,const WORD32 *const *pp_inps_shape
+                        ,WORD32 num_out_dims
+                        ,WORD32 num_inp
+                        ,WORD32 num_inp_dims
+                        ,WORD32 axis)
+{
+  XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_out_shape, -1);
+  XA_NNLIB_ARG_CHK_PTR(pp_inps, -1);
+  XA_NNLIB_ARG_CHK_PTR(pp_inps_shape, -1);
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_out_shape, sizeof(WORD32), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(pp_inps, sizeof(WORD8 *), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(pp_inps_shape, sizeof(WORD32 *), -1);
+  //Validate Arguments
+  XA_NNLIB_ARG_CHK_COND((num_out_dims <= 0 || num_out_dims > 6), -1);
+  XA_NNLIB_ARG_CHK_COND((num_inp <= 0 || num_inp > 10), -1);
+  XA_NNLIB_ARG_CHK_COND((num_inp_dims != num_out_dims), -1);
+  XA_NNLIB_ARG_CHK_COND((axis < -num_out_dims || axis >= num_out_dims), -1);
+
+  int i = 0, j = 0;
+  for(i = 0; i < num_out_dims; i++)
+  {
+    XA_NNLIB_ARG_CHK_COND((p_out_shape[i] <= 0), -1);
+  }
+
+  if(axis < 0)
+    axis = num_out_dims + axis;
+
+  WORD32 concat_size = 0;
+  for (i = 0; i < num_inp; i++)
+  {
+    XA_NNLIB_ARG_CHK_PTR(pp_inps[i], -1);
+    XA_NNLIB_ARG_CHK_PTR(pp_inps_shape[i], -1);
+    XA_NNLIB_ARG_CHK_ALIGN(pp_inps_shape[i], sizeof(WORD32), -1);
+#pragma loop_count min=1
+    for(j = 0; j < num_out_dims; j++)
+    {
+      XA_NNLIB_ARG_CHK_COND((pp_inps_shape[i][j] != p_out_shape[j] && j != axis), -1);
+    }
+    XA_NNLIB_ARG_CHK_COND((pp_inps_shape[i][axis] <= 0), -1);
+    concat_size += pp_inps_shape[i][axis];
+  }
+
+  XA_NNLIB_ARG_CHK_COND((p_out_shape[axis] != concat_size), -1);
+
+  //Calculate outer and inner size for axis
+  WORD32 outer_size = 1;
+#pragma no_simd
+  for(int i = 0; i < axis; i++)
+  {
+    outer_size *= p_out_shape[i];
+  }
+
+  WORD32 base_inner_size = 1;
+#pragma no_simd
+  for(int i = axis + 1; i < num_out_dims; i++)
+  {
+    base_inner_size *= p_out_shape[i];
+  }
+
+  if(outer_size == 1)
+  {
+    WORD8 *ptmp_out = p_out;
+    for(int i = 0; i < num_inp; i++)
+    {
+      const WORD32 copy_size = pp_inps_shape[i][axis] * base_inner_size;
+
+      {
+        WORD8 *output_ptr = ptmp_out;
+        const WORD8* input_ptr = pp_inps[i];
+
+        {
+          MEMCPY_8b(output_ptr, input_ptr, copy_size);
+        }
+        ptmp_out += copy_size;
+      }
+    }
+  }
+  else
+  {
+    WORD8 *ptmp_out = p_out;
+#pragma loop_count min=1
+    for(int i = 0; i < num_inp; i++)
+    {
+      const WORD32 copy_size = pp_inps_shape[i][axis] * base_inner_size;
+
+      if(copy_size <= 16)
+      {
+        ae_int8x16 *output_ptr;
+        ae_int8x16 *input_ptr = (ae_int8x16 *)pp_inps[i];
+        ae_valignx2 input_valign, output_valign;
+        input_valign = AE_LA128_PP(input_ptr);
+        ae_int8x8 d_inp1, d_inp2;
+#pragma loop_count min=1
+#pragma concurrent
+        for(int k = 0; k < outer_size; k++)
+        {
+          output_ptr = (ae_int8x16 *)(ptmp_out + concat_size * base_inner_size * k);
+          output_valign = AE_ZALIGN128();
+          AE_LAV8X8X2_XP(d_inp1, d_inp2, input_valign, input_ptr, copy_size);
+          AE_SAV8X8X2_XP(d_inp1, d_inp2, output_valign, output_ptr, copy_size);
+          AE_SA128POS_FP(output_valign, (void *)output_ptr);
+        }
+      }
+      else
+      {
+        WORD8 *output_ptr = ptmp_out;
+        const WORD8* input_ptr = pp_inps[i];
+
+#pragma loop_count min=1
+        for(int k = 0; k < outer_size; k++)
+        {
+          // memcpy(output_ptr, input_ptr, copy_size * sizeof(WORD8));
+          MEMCPY_8b(output_ptr, input_ptr, (int)(copy_size * sizeof(WORD8)));
+          input_ptr += copy_size;
+          output_ptr += concat_size * base_inner_size;
+        }
+      }
+      ptmp_out += copy_size;
+    }
+  }
+  return 0;
+
+}
diff --git a/algo/kernels/reorg/hifi5/xa_nn_split_v_8.c b/algo/kernels/reorg/hifi5/xa_nn_split_v_8.c
new file mode 100644
index 0000000..28a381e
--- /dev/null
+++ b/algo/kernels/reorg/hifi5/xa_nn_split_v_8.c
@@ -0,0 +1,147 @@
+/*******************************************************************************
+* Copyright (c) 2018-2024 Cadence Design Systems, Inc.
+*
+* Permission is hereby granted, free of charge, to any person obtaining
+* a copy of this software and associated documentation files (the
+* "Software"), to use this Software with Cadence processor cores only and
+* not with any other processors and platforms, subject to
+* the following conditions:
+*
+* The above copyright notice and this permission notice shall be included
+* in all copies or substantial portions of the Software.
+*
+* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+* IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+* SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+******************************************************************************/
+#include "xa_type_def.h"
+#include "xa_nnlib_kernels_api.h"
+#include "xa_nnlib_common_macros_hifi5.h"
+#include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common.h"
+
+WORD32 xa_nn_split_v_8_8(WORD8 ** __restrict__ pp_outs
+                        ,const WORD32 *const *pp_outs_shape
+                        ,const WORD8 *p_inp
+                        ,const WORD32 *const p_inp_shape
+                        ,WORD32 num_out
+                        ,WORD32 num_out_dims
+                        ,WORD32 num_inp_dims
+                        ,WORD32 axis)
+{
+  XA_NNLIB_ARG_CHK_PTR(pp_outs, -1);
+  XA_NNLIB_ARG_CHK_PTR(pp_outs_shape, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp_shape, -1);
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_inp_shape, sizeof(WORD32), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(pp_outs, sizeof(WORD8 *), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(pp_outs_shape, sizeof(WORD32 *), -1);
+  //Validate Arguments
+  XA_NNLIB_ARG_CHK_COND((num_out <= 0 || num_out > 10), -1);
+  XA_NNLIB_ARG_CHK_COND((num_inp_dims <= 0 || num_inp_dims > 6), -1);
+  XA_NNLIB_ARG_CHK_COND((num_inp_dims != num_out_dims), -1);
+  XA_NNLIB_ARG_CHK_COND((axis < -num_out_dims || axis >= num_out_dims), -1);
+
+  int i = 0, j = 0;
+  for(i = 0; i < num_inp_dims; i++)
+  {
+    XA_NNLIB_ARG_CHK_COND((p_inp_shape[i] <= 0), -1);
+  }
+
+  if(axis < 0)
+    axis = num_inp_dims + axis;
+
+  WORD32 concat_size = 0;
+  for (i = 0; i < num_out; i++)
+  {
+    XA_NNLIB_ARG_CHK_PTR(pp_outs[i], -1);
+    XA_NNLIB_ARG_CHK_PTR(pp_outs_shape[i], -1);
+    XA_NNLIB_ARG_CHK_ALIGN(pp_outs_shape[i], sizeof(WORD32), -1);
+#pragma loop_count min=1
+    for(j = 0; j < num_inp_dims; j++)
+    {
+      XA_NNLIB_ARG_CHK_COND((p_inp_shape[j] != pp_outs_shape[i][j] && j != axis), -1);
+    }
+    XA_NNLIB_ARG_CHK_COND((pp_outs_shape[i][axis] <= 0), -1);
+    concat_size += pp_outs_shape[i][axis];
+  }
+
+  XA_NNLIB_ARG_CHK_COND((p_inp_shape[axis] != concat_size), -1);
+
+  /*Calculate outer and inner size for axis*/
+  WORD32 outer_size = 1;
+#pragma no_simd
+  for(i = 0; i < axis; i++)
+  {
+    outer_size *= p_inp_shape[i];
+  }
+
+  WORD32 base_inner_size = 1;
+#pragma no_simd
+  for(i = axis + 1; i < num_inp_dims; i++)
+  {
+    base_inner_size *= p_inp_shape[i];
+  }
+  if(outer_size == 1)
+  {
+    const WORD8 *ptmp_inp = p_inp;
+    for(int i = 0; i < num_out; i++)
+    {
+      const WORD32 copy_size = pp_outs_shape[i][axis] * base_inner_size;
+
+      {
+        const WORD8 *input_ptr = ptmp_inp;
+        WORD8* output_ptr = pp_outs[i];
+
+        {
+          MEMCPY_8b(output_ptr, input_ptr, copy_size);
+        }
+        ptmp_inp += copy_size;
+      }
+    }
+  }
+  else
+  {
+    WORD32 inp_axis = p_inp_shape[axis];
+    WORD32 next_inp_idx = 0;
+    for(j = 0; j < num_out; j++)
+    {
+      WORD8 *output_ptr = pp_outs[j];
+      WORD32 copy_size = pp_outs_shape[j][axis] * base_inner_size;
+      if(copy_size <= 16)
+      {
+        ae_int8x8 d_inp1, d_inp2;
+        ae_valignx2 input_valign, output_valign;
+        output_valign = AE_ZALIGN128();
+#pragma loop_count min=1
+#pragma concurrent
+        for(i = 0; i < outer_size; i++)
+        {
+          ae_int8x16 *input_ptr = (ae_int8x16*) (p_inp + next_inp_idx + i* inp_axis * base_inner_size);
+          input_valign = AE_LA128_PP((ae_int8x16 *)input_ptr);
+          AE_LAV8X8X2_XP(d_inp1, d_inp2, input_valign, input_ptr, copy_size);
+          AE_SAV8X8X2_XP(d_inp1, d_inp2, output_valign, (ae_int8x16*)output_ptr, copy_size);
+        }
+        AE_SA128POS_FP(output_valign, (void *)output_ptr);
+      }
+      else
+      {
+#pragma loop_count min=1
+        for(i = 0; i < outer_size; i++)
+        {
+          const WORD8* input_ptr = p_inp + next_inp_idx + i * inp_axis * base_inner_size;
+          MEMCPY_8b(output_ptr, input_ptr, (int)(copy_size * sizeof(WORD8)));
+          output_ptr += copy_size;
+        }
+      }
+      next_inp_idx += copy_size;
+    }
+  }
+  return 0;
+}
diff --git a/algo/kernels/reorg/hifi5/xa_nn_transpose_16.c b/algo/kernels/reorg/hifi5/xa_nn_transpose_16.c
new file mode 100644
index 0000000..8e1e13a
--- /dev/null
+++ b/algo/kernels/reorg/hifi5/xa_nn_transpose_16.c
@@ -0,0 +1,265 @@
+/*******************************************************************************
+* Copyright (c) 2018-2024 Cadence Design Systems, Inc.
+*
+* Permission is hereby granted, free of charge, to any person obtaining
+* a copy of this software and associated documentation files (the
+* "Software"), to use this Software with Cadence processor cores only and
+* not with any other processors and platforms, subject to
+* the following conditions:
+*
+* The above copyright notice and this permission notice shall be included
+* in all copies or substantial portions of the Software.
+*
+* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+* IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+* SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+******************************************************************************/
+#include "xa_nnlib_common.h"
+
+/*
+ * Currently only supports upto 5D input tensors.
+ * 1/2/3/4 D input tensors will be scaled up to 5D.
+ * For example, 2x3 -> 1x1x1x2x3.
+ */
+
+
+WORD32 xa_nn_transpose_16_16(WORD16 * __restrict__ p_out
+                    ,const WORD32 *const p_out_shape
+                    ,const WORD16 * __restrict__ p_inp
+                    ,const WORD32 *const p_inp_shape
+                    ,const WORD32 * __restrict__ p_permute_vec
+                    ,WORD32 num_out_dims
+                    ,WORD32 num_inp_dims)
+{
+  /* NULL pointer checks */
+  XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_permute_vec, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_out_shape, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp_shape, -1);
+
+  /* Invalid input checks */
+  XA_NNLIB_ARG_CHK_COND(((num_inp_dims <= 0) || (num_inp_dims > 5)), -1);
+  XA_NNLIB_ARG_CHK_COND((num_out_dims != num_inp_dims), -1);
+
+  int itr = 0;
+  for(itr=0; itr < num_inp_dims; itr++)
+  {
+    XA_NNLIB_ARG_CHK_COND((p_inp_shape[itr] <= 0), -1);
+  }
+  for(itr=0; itr < num_out_dims; itr++)
+  {
+    XA_NNLIB_ARG_CHK_COND((p_out_shape[itr] <= 0), -1);
+  }
+
+  /* Output shape provided must be correct based on input
+   * shape and permute values */
+  for(itr=0; itr < num_out_dims; itr++)
+  {
+    int output_dim = p_out_shape[itr];
+    int expected_dim = p_inp_shape[p_permute_vec[itr]];
+    XA_NNLIB_ARG_CHK_COND((output_dim != expected_dim), -1);
+  }
+
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_out, sizeof(WORD16), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_inp, sizeof(WORD16), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_permute_vec, sizeof(WORD32), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_out_shape, sizeof(WORD32), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_inp_shape, sizeof(WORD32), -1);
+
+
+  /* Shift all dim with 1 in the outer part */
+  int eff_output_shape[5];
+  int eff_permute_vec[5];
+
+  for(int i = 0; i < num_out_dims; i++)
+  {
+    eff_output_shape[i] = p_out_shape[i];
+    eff_permute_vec[i] = p_permute_vec[i];
+  }
+
+  int one_i=num_out_dims-1, non_one_i=num_out_dims-1;
+  while(one_i > 0 && non_one_i >=0){
+    while(one_i > 0 && eff_output_shape[one_i]!=1){
+      one_i--;
+    }
+    non_one_i = one_i;
+    while(non_one_i >= 0 && eff_output_shape[non_one_i]==1)
+    {
+      non_one_i--;
+    }
+    if(one_i > 0 && non_one_i >=0){
+      int temp;
+      /*swap output_shape*/
+      {
+        temp = eff_output_shape[one_i];
+        eff_output_shape[one_i] = eff_output_shape[non_one_i];
+        eff_output_shape[non_one_i] = temp;
+      }
+      /*swap permute_vec*/
+      {
+        temp = eff_permute_vec[one_i];
+        eff_permute_vec[one_i] = eff_permute_vec[non_one_i];
+        eff_permute_vec[non_one_i] = temp;
+      }
+
+    }
+  }
+
+  /* Promoting lesser dim tensors to 5D tensors.
+   * Also updating the permute_vec and shapes as needed for optimization */
+  int p_5D_inp_shape[5] = {1, 1, 1, 1, 1};
+  int p_5D_out_shape[5] = {1, 1, 1, 1, 1};
+  int p_5D_permute_vec[5] = {0, 1, 2, 3, 4};
+
+  /* Check if any inner inp dimension is same in the output */
+  int last_dim_same = 1, last_n_same_dim = 0;
+  itr = num_inp_dims - 1;
+
+  while(itr >= 0)
+  {
+    last_n_same_dim = (last_dim_same && (eff_permute_vec[itr] == itr)) ? (last_n_same_dim + 1) : last_n_same_dim;
+    last_dim_same = (eff_permute_vec[itr] == itr) ? last_dim_same & 1 : last_dim_same & 0;
+    itr--;
+  }
+
+  int dims_added = 5 - num_inp_dims;
+  itr = num_inp_dims - 1;
+  int same_count = last_n_same_dim;
+  int count = 4;
+  while(itr >= 0)
+  {
+    p_5D_inp_shape[count] = (same_count > 0) ? p_5D_inp_shape[count]*p_inp_shape[itr] : p_inp_shape[itr];
+    p_5D_out_shape[count] = (same_count > 0) ? p_5D_out_shape[count]*eff_output_shape[itr] : eff_output_shape[itr];
+    same_count--;
+    itr--;
+    count = (same_count > 0) ? count : count - 1;
+  }
+
+  itr = num_inp_dims - 1;
+  same_count = (last_n_same_dim) ? num_inp_dims - (last_n_same_dim - 1) : 0;
+  count = 4;
+  while(itr >= 0)
+  {
+    p_5D_permute_vec[count] = (same_count > 0) ? eff_permute_vec[itr-(last_n_same_dim - 1)] + dims_added + last_n_same_dim - 1 : eff_permute_vec[itr] + dims_added;
+    same_count--;
+    itr--;
+    count--;
+  }
+
+  int out_dim0, out_dim1, out_dim2, out_dim3, out_dim4;
+  int inp_dim1, inp_dim2, inp_dim3, inp_dim4;
+  int inp_stride[5];
+
+  out_dim0 = p_5D_out_shape[0];
+  out_dim1 = p_5D_out_shape[1];
+  out_dim2 = p_5D_out_shape[2];
+  out_dim3 = p_5D_out_shape[3];
+  out_dim4 = p_5D_out_shape[4];
+
+  inp_dim1 = p_5D_inp_shape[1];
+  inp_dim2 = p_5D_inp_shape[2];
+  inp_dim3 = p_5D_inp_shape[3];
+  inp_dim4 = p_5D_inp_shape[4];
+
+  inp_stride[0] = inp_dim1*inp_dim2*inp_dim3*inp_dim4;
+  inp_stride[1] = inp_dim2*inp_dim3*inp_dim4;
+  inp_stride[2] = inp_dim3*inp_dim4;
+  inp_stride[3] = inp_dim4;
+  inp_stride[4] = 1;
+
+  if(last_n_same_dim)
+  {
+    int itr0, itr1, itr2, itr3, itr4;
+    WORD16 *p_inp0 = (WORD16*)p_inp;
+    for(itr0 = 0; itr0 < out_dim0; itr0++)
+    {
+      WORD16 *p_inp1 = p_inp0+(itr0*inp_stride[p_5D_permute_vec[0]]);
+#pragma loop_count min=1
+      for(itr1 = 0; itr1 < out_dim1; itr1++)
+      {
+        WORD16 *p_inp2 = p_inp1+(itr1*inp_stride[p_5D_permute_vec[1]]);
+#pragma loop_count min=1
+        for(itr2 = 0; itr2 < out_dim2; itr2++)
+        {
+          WORD16 *p_inp3 = p_inp2+(itr2*inp_stride[p_5D_permute_vec[2]]);
+#pragma loop_count min=1
+          for(itr3 = 0; itr3 < out_dim3; itr3++, p_out+=out_dim4)
+          {
+            WORD16 *p_inp4 = p_inp3+(itr3*inp_stride[p_5D_permute_vec[3]]);
+            ae_int16x8 *__restrict__ pae_i = (ae_int16x8 *)(p_inp4);
+            ae_int16x8 *__restrict__ pae_o = (ae_int16x8 *)(p_out);
+            ae_valignx2 a_inp = AE_LA128_PP(pae_i);
+            ae_valignx2 a_out = AE_ZALIGN128();
+            ae_int16x4 d0,d1;
+            for(itr4 = 0; itr4 < (out_dim4 >> 3); itr4++)
+            {
+              AE_LA16X4X2_IP(d0, d1, a_inp, (ae_int16x8*)pae_i);
+              AE_SA16X4X2_IP(d0, d1, a_out, (ae_int16x8*)pae_o);
+            }
+            AE_LAV16X4X2_XP(d0, d1, a_inp, (ae_int16x8*)pae_i, (out_dim4 & 7)<<1);
+            AE_SAV16X4X2_XP(d0, d1, a_out, (ae_int16x8*)pae_o, (out_dim4 & 7)<<1);
+            AE_SA128POS_FP(a_out, pae_o);
+          }
+        }
+      }
+    }
+  }
+  else
+  {
+    int itr0, itr1, itr2, itr3, itr4;
+    WORD16 *p_inp0 = (WORD16*)p_inp;
+    for(itr0 = 0; itr0 < out_dim0; itr0++)
+    {
+      WORD16 *p_inp1 = p_inp0+(itr0*inp_stride[p_5D_permute_vec[0]]);
+      for(itr1 = 0; itr1 < out_dim1; itr1++)
+      {
+        WORD16 *p_inp2 = p_inp1+(itr1*inp_stride[p_5D_permute_vec[1]]);
+        for(itr2 = 0; itr2 < out_dim2; itr2++)
+        {
+          WORD16 *p_inp3 = p_inp2+(itr2*inp_stride[p_5D_permute_vec[2]]);
+          for(itr3 = 0; itr3 < out_dim3; itr3++)
+          {
+            WORD16 *p_inp4 = p_inp3+(itr3*inp_stride[p_5D_permute_vec[3]]);
+
+            ae_valign a_out = AE_ZALIGN64();
+            for(itr4 = 0; itr4 < (out_dim4 >> 2); itr4++)
+            {
+              ae_int16x4 d0, d1, d2, d3;
+              ae_int16x4 tmp0;
+
+              d1 = AE_L16_X ((ae_int16*)p_inp4, inp_stride[p_5D_permute_vec[4]]<<1);
+              d2 = AE_L16_X ((ae_int16*)p_inp4, 2*inp_stride[p_5D_permute_vec[4]]<<1);
+              d3 = AE_L16_X ((ae_int16*)p_inp4, 3*inp_stride[p_5D_permute_vec[4]]<<1);
+              AE_L16_XP(d0, (ae_int16*)p_inp4, 4*inp_stride[p_5D_permute_vec[4]]<<1);
+
+              tmp0 = AE_SEL16_6543(d0, d1);
+              tmp0 = AE_SEL16_6543(tmp0, d2);
+              tmp0 = AE_SEL16_6543(tmp0, d3);
+
+              AE_SA16X4_IP(tmp0, a_out, (ae_int16x4 *)p_out);
+            }
+            AE_SA64POS_FP(a_out, p_out);
+#pragma loop_count max=3
+            for(itr4 = 0; itr4 < (out_dim4 & 3); itr4++)
+            {
+              ae_int16x4 d0;
+              AE_L16_XP(d0, (ae_int16*)p_inp4, inp_stride[p_5D_permute_vec[4]]<<1);
+              AE_S16_0_IP(d0, (ae_int16 *)p_out, 2);
+            }
+          }
+        }
+      }
+    }
+  }
+
+  return 0;
+}
+
+
diff --git a/algo/kernels/reorg/hifi5/xa_nn_transpose_8.c b/algo/kernels/reorg/hifi5/xa_nn_transpose_8.c
index 62fce74..65706e9 100644
--- a/algo/kernels/reorg/hifi5/xa_nn_transpose_8.c
+++ b/algo/kernels/reorg/hifi5/xa_nn_transpose_8.c
@@ -72,6 +72,44 @@ WORD32 xa_nn_transpose_8_8(WORD8 * __restrict__ p_out
   XA_NNLIB_ARG_CHK_ALIGN(p_out_shape, sizeof(WORD32), -1);
   XA_NNLIB_ARG_CHK_ALIGN(p_inp_shape, sizeof(WORD32), -1);
 
+  /* Shift all dim with 1 in the outer part */
+  int eff_output_shape[5];
+  int eff_permute_vec[5];
+
+  for(int i = 0; i < num_out_dims; i++)
+  {
+    eff_output_shape[i] = p_out_shape[i];
+    eff_permute_vec[i] = p_permute_vec[i];
+  }
+
+  int one_i=num_out_dims-1, non_one_i=num_out_dims-1;
+  while(one_i > 0 && non_one_i >=0){
+    while(one_i > 0 && eff_output_shape[one_i]!=1){
+      one_i--;
+    }
+    non_one_i = one_i;
+    while(non_one_i >= 0 && eff_output_shape[non_one_i]==1)
+    {
+      non_one_i--;
+    }
+    if(one_i > 0 && non_one_i >=0){
+      int temp;
+      /*swap output_shape*/
+      {
+        temp = eff_output_shape[one_i];
+        eff_output_shape[one_i] = eff_output_shape[non_one_i];
+        eff_output_shape[non_one_i] = temp;
+      }
+      /*swap permute_vec*/
+      {
+        temp = eff_permute_vec[one_i];
+        eff_permute_vec[one_i] = eff_permute_vec[non_one_i];
+        eff_permute_vec[non_one_i] = temp;
+      }
+    }
+  }
+
+
   /* Promoting lesser dim tensors to 5D tensors. 
    * Also updating the permute_vec and shapes as needed for optimization */
   int p_5D_inp_shape[5] = {1, 1, 1, 1, 1};
@@ -83,8 +121,8 @@ WORD32 xa_nn_transpose_8_8(WORD8 * __restrict__ p_out
   itr = num_inp_dims - 1;
   while(itr >= 0)
   {
-    last_n_same_dim = (last_dim_same && (p_permute_vec[itr] == itr)) ? (last_n_same_dim + 1) : last_n_same_dim;
-    last_dim_same = (p_permute_vec[itr] == itr) ? last_dim_same & 1 : last_dim_same & 0;
+    last_n_same_dim = (last_dim_same && (eff_permute_vec[itr] == itr)) ? (last_n_same_dim + 1) : last_n_same_dim;
+    last_dim_same = (eff_permute_vec[itr] == itr) ? last_dim_same & 1 : last_dim_same & 0;
     itr--;
   }
   
@@ -95,7 +133,7 @@ WORD32 xa_nn_transpose_8_8(WORD8 * __restrict__ p_out
   while(itr >= 0)
   {
     p_5D_inp_shape[count] = (same_count > 0) ? p_5D_inp_shape[count]*p_inp_shape[itr] : p_inp_shape[itr];
-    p_5D_out_shape[count] = (same_count > 0) ? p_5D_out_shape[count]*p_out_shape[itr] : p_out_shape[itr];
+    p_5D_out_shape[count] = (same_count > 0) ? p_5D_out_shape[count]*eff_output_shape[itr] : eff_output_shape[itr];
     same_count--;
     itr--;
     count = (same_count > 0) ? count : count - 1;
@@ -106,7 +144,7 @@ WORD32 xa_nn_transpose_8_8(WORD8 * __restrict__ p_out
   count = 4;
   while(itr >= 0)
   {
-    p_5D_permute_vec[count] = (same_count > 0) ? p_permute_vec[itr-(last_n_same_dim - 1)] + dims_added + last_n_same_dim - 1 : p_permute_vec[itr] + dims_added;
+    p_5D_permute_vec[count] = (same_count > 0) ? eff_permute_vec[itr-(last_n_same_dim - 1)] + dims_added + last_n_same_dim - 1 : eff_permute_vec[itr] + dims_added;
     same_count--;
     itr--;
     count--;
diff --git a/include/nnlib/xa_nnlib_kernels_api.h b/include/nnlib/xa_nnlib_kernels_api.h
index c8a0bbb..569cc29 100644
--- a/include/nnlib/xa_nnlib_kernels_api.h
+++ b/include/nnlib/xa_nnlib_kernels_api.h
@@ -22,7 +22,6 @@
 #ifndef __XA_NNLIB_KERNELS_API_H__
 #define __XA_NNLIB_KERNELS_API_H__
 
-
 /**
  * @file xa_nnlib_kernels_api.h
  * @brief This file gives the API definition for the HiFi NNLIB
@@ -128,6 +127,38 @@
 {
 #endif
 
+#ifdef ENABLE_SCRATCH_SIZE_API_ONLY
+
+#if defined(hifi5)
+#define get_softmax_scratch_size                get_softmax_scratch_size_hifi5
+#define xa_nn_avgpool_getsize                   xa_nn_avgpool_getsize_hifi5
+#define xa_nn_conv2d_depthwise_getsize          xa_nn_conv2d_depthwise_getsize_hifi5
+#define xa_nn_conv2d_getsize                    xa_nn_conv2d_getsize_hifi5
+#define xa_nn_conv2d_std_getsize                xa_nn_conv2d_std_getsize_hifi5
+#define xa_nn_conv2d_std_getsize_sym4s          xa_nn_conv2d_std_getsize_sym4s_hifi5
+#define xa_nn_dilated_conv2d_depthwise_getsize  xa_nn_dilated_conv2d_depthwise_getsize_hifi5
+#define xa_nn_dilated_conv2d_std_getsize        xa_nn_dilated_conv2d_std_getsize_hifi5
+#define xa_nn_maxpool_getsize                   xa_nn_maxpool_getsize_hifi5
+#define xa_nn_reduce_getsize_nhwc               xa_nn_reduce_getsize_nhwc_hifi5
+#define xa_nn_transpose_conv_getsize            xa_nn_transpose_conv_getsize_hifi5
+
+#elif defined(hifi4)
+#define get_softmax_scratch_size                get_softmax_scratch_size_hifi4
+#define xa_nn_avgpool_getsize                   xa_nn_avgpool_getsize_hifi4
+#define xa_nn_conv2d_depthwise_getsize          xa_nn_conv2d_depthwise_getsize_hifi4
+#define xa_nn_conv2d_getsize                    xa_nn_conv2d_getsize_hifi4
+#define xa_nn_conv2d_std_getsize                xa_nn_conv2d_std_getsize_hifi4
+#define xa_nn_conv2d_std_getsize_sym4s          xa_nn_conv2d_std_getsize_sym4s_hifi4
+#define xa_nn_dilated_conv2d_depthwise_getsize  xa_nn_dilated_conv2d_depthwise_getsize_hifi4
+#define xa_nn_dilated_conv2d_std_getsize        xa_nn_dilated_conv2d_std_getsize_hifi4
+#define xa_nn_maxpool_getsize                   xa_nn_maxpool_getsize_hifi4
+#define xa_nn_reduce_getsize_nhwc               xa_nn_reduce_getsize_nhwc_hifi4
+#define xa_nn_transpose_conv_getsize            xa_nn_transpose_conv_getsize_hifi4
+
+#endif
+
+#endif /* #ifdef ENABLE_SCRATCH_SIZE_API_ONLY */
+
 	WORD32 xa_nn_matXvec_16x16_16(
 			WORD16 * __restrict__ p_out,                /*!< [out] 16b result: rows x 1 */
 			WORD16 * __restrict__ p_mat1,               /*!< [in] 16b mat1: rows x cols1 */
@@ -817,9 +848,9 @@
 			int filter_height, int filter_width,
 			int output_height, int output_width,
 			int num_elements,
-      int input_offset, int output_offset,
+			int input_offset, int output_offset,
 			int *output_shift, int *output_multiplier,
-			WORD32* scratch_buffer);
+			void* scratch_buffer);
 
 	WORD32 xa_nn_transpose_conv_sym8sxsym16s(
 			WORD16* output_data,
@@ -834,7 +865,7 @@
 			int output_height, int output_width,
 			int num_elements,
 			int *output_shift, int *output_multiplier,
-			WORD64* scratch_buffer);
+			void* scratch_buffer);
 
     WORD32 xa_nn_transpose_conv_f32(
             FLOAT32* output_data,
@@ -848,7 +879,7 @@
             int filter_height, int filter_width,
             int output_height, int output_width,
             int num_elements,
-            FLOAT32* scratch_buffer);
+            void* scratch_buffer);
 
 	WORD32 xa_nn_conv1d_std_getsize(
 			WORD32 kernel_height,
@@ -1676,6 +1707,12 @@
 			WORD32  vec_length,
 			pVOID   p_scratch);
 
+  WORD32 xa_nn_vec_softmax_sym16s_16( WORD16 * __restrict__ p_out,
+      const   WORD16 * __restrict__ p_vec,
+      WORD32  input_beta_left_shift,
+      WORD32  input_beta_multiplier,
+      WORD32  vec_length);
+
 	WORD32 xa_nn_vec_sigmoid_asym8u_asym8u(UWORD8 *p_out,
 			const UWORD8 *p_vec,
 			WORD32 zero_point,
@@ -2571,6 +2608,14 @@
             WORD32 clip,
             WORD32 num_elms);
 
+  WORD32 xa_nn_elm_requantize_asym8u_asym8s(WORD8 * __restrict__ p_out,
+      const UWORD8 * __restrict__ p_inp,
+      WORD32 inp_zero_bias,
+      WORD32 out_zero_bias,
+      WORD32 out_shift,
+      WORD32 out_multiplier,
+      WORD32 num_elm);
+
 	WORD32 xa_nn_elm_requantize_asym16s_asym8s(WORD8 * __restrict__ p_out,
 			const WORD16 * __restrict__ p_inp,
 			WORD32  inp_zero_bias,
@@ -3039,6 +3084,14 @@
                     ,WORD32 num_out_dims
                     ,WORD32 num_inp_dims);
 
+  WORD32 xa_nn_transpose_16_16(WORD16 * __restrict__ p_out
+                    ,const WORD32 *const p_out_shape
+                    ,const WORD16 * __restrict__ p_inp
+                    ,const WORD32 *const p_inp_shape
+                    ,const WORD32 * __restrict__ p_permute_vec
+                    ,WORD32 num_out_dims
+                    ,WORD32 num_inp_dims);
+
   WORD32 xa_nn_batch_norm_3D_8_8(WORD8 * __restrict__ p_out
                     ,const WORD8 * __restrict__ p_inp
                     ,const WORD16 * __restrict__ p_alpha
@@ -3083,6 +3136,23 @@ WORD32 xa_nn_resize_nearest_neighbour_8_8(pWORD8 __restrict__ p_out
                     ,FLOAT32 width_offset
                     ,WORD32  align_corners);
 
+WORD32 xa_nn_concat_8_8(WORD8 * __restrict__ p_out
+        ,const WORD32 *const p_out_shape
+        ,const WORD8 **p_inps
+        ,const WORD32 *const *pp_inps_shape
+        ,WORD32 num_out_dims
+        ,WORD32 num_inp
+        ,WORD32 num_inp_dims
+        ,WORD32 axis);
+
+WORD32 xa_nn_split_v_8_8(WORD8 ** __restrict__ pp_outs
+                        ,const WORD32 *const *pp_outs_shape
+                        ,const WORD8 *p_inp
+                        ,const WORD32 *const p_inp_shape
+                        ,WORD32 num_out
+                        ,WORD32 num_out_dims
+                        ,WORD32 num_inp_dims
+                        ,WORD32 axis);
 
 	/* Mapping the functions names from previous naming convension for backward compatibility */
 #define xa_nn_matXvec_asym8xasym8_asym8 xa_nn_matXvec_asym8uxasym8u_asym8u
diff --git a/include/nnlib/xa_nnlib_standards.h b/include/nnlib/xa_nnlib_standards.h
index 88c619a..fb91967 100644
--- a/include/nnlib/xa_nnlib_standards.h
+++ b/include/nnlib/xa_nnlib_standards.h
@@ -22,7 +22,9 @@
 #ifndef __STANDARDS_H__
 #define __STANDARDS_H__
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include <xtensa/config/core-isa.h>
+#endif
 
 #if defined(__cplusplus)
 extern "C"
@@ -151,7 +153,7 @@ typedef struct _xa_nnlib_shape_t{
 
 enum xa_error_severity {
   xa_severity_nonfatal = 0,
-  xa_severity_fatal    = (int)0xffffffff
+  xa_severity_fatal    = (unsigned int)0xffffffff
 };
 
 enum xa_error_class {
