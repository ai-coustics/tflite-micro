diff --git a/algo/common/include/xa_nn_common.h b/algo/common/include/xa_nn_common.h
index 5a45d27..4fa7215 100644
--- a/algo/common/include/xa_nn_common.h
+++ b/algo/common/include/xa_nn_common.h
@@ -64,8 +64,10 @@
 #define STRINGIZE(A) STRINGIZE_NX(A)    /*  Turn A into a string literal after macro-expanding it. */
 //#include STRINGIZE(PPCAT(cstub,XTENSA_CORE).h)
 //#include STRINGIZE(PPCAT(PPCAT(cstub,XTENSA_CORE),c.h))
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include "xtensa/tie/xt_hifi3.h"
 #include "xtensa/config/core-isa.h"
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 #endif
 
 //-----------------------------------------------------
@@ -89,6 +91,7 @@
 #define INV_TBL_BITS 7
 extern const int32_t tab_invQ30[128];
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #if XCHAL_HAVE_NSA
   #define NSA(n) XT_NSA(n)
 #else
@@ -100,6 +103,7 @@ extern const int32_t tab_invQ30[128];
     return AE_NSAQ56S(t)-8;
   }
 #endif
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 
 #ifdef COMPILER_XTENSA
   #define ATTRIBUTE_ALWAYS_INLINE __attribute__((always_inline))
@@ -123,11 +127,13 @@ extern const int32_t tab_invQ30[128];
 #define return_int64(x) {  union {ae_int64  ai;int64_t   i; } r; r.ai = x;  return r.i; }
 #endif
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #if  defined (__cplusplus) || defined(COMPILER_XTENSA)
 
 #else
 #error sorry, C compiler is not supported excluding the XCC
 #endif
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 
 
 #ifdef COMPILER_MSVC
diff --git a/algo/common/include/xa_nnlib_common.h b/algo/common/include/xa_nnlib_common.h
index fcf379a..d66f2a0 100644
--- a/algo/common/include/xa_nnlib_common.h
+++ b/algo/common/include/xa_nnlib_common.h
@@ -21,12 +21,16 @@
 ******************************************************************************/
 #ifndef __XA_NNLIB_LEGACY_COMPAT_H__
 #define __XA_NNLIB_LEGACY_COMPAT_H__
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include <xtensa/config/core-isa.h>
 #include "xtensa/tie/xt_hifi2.h"
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 #include "xa_nnlib_api.h"
 #include "xa_nnlib_standards.h"
 #include "xa_nnlib_err_chk.h"
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include "xa_nnlib_hifi_isa_compat.h"
+#endif
 #include "xa_nn_common.h"
 #include "xa_nnlib_common_internal.h"
 #endif /* __XA_NNLIB_LEGACY_COMPAT_H__ */
@@ -36,4 +40,4 @@
 #define XA_HAVE_HIFI3_CORE 1
 #else
 #define XA_HAVE_HIFI3_CORE 0
-#endif
\ No newline at end of file
+#endif
diff --git a/algo/common/include/xa_nnlib_common_macros.h b/algo/common/include/xa_nnlib_common_macros.h
index a2d1867..4b3f0c5 100644
--- a/algo/common/include/xa_nnlib_common_macros.h
+++ b/algo/common/include/xa_nnlib_common_macros.h
@@ -23,7 +23,9 @@
 #ifndef __XA_NNLIB_COMMON_MACROS_H__
 #define __XA_NNLIB_COMMON_MACROS_H__
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include <xtensa/config/core-isa.h>
+#endif
 #include <stddef.h>
 #include "xa_nnlib_quant_macros.h"
 #include "xa_nnlib_common_internal.h"
@@ -32,6 +34,8 @@
 #define NULL (void *)0
 #endif /* NULL */
 
+#define MAX(a, b)   (((a) > (b)) ? (a) : (b))
+
 #if XCHAL_HAVE_HIFI1
 
 #if (XCHAL_HW_VERSION >= 281090)
diff --git a/algo/kernels/activations/hifi4/xa_nn_softmax_sym16s_16.c b/algo/kernels/activations/hifi4/xa_nn_softmax_sym16s_16.c
new file mode 100644
index 0000000..9817fd6
--- /dev/null
+++ b/algo/kernels/activations/hifi4/xa_nn_softmax_sym16s_16.c
@@ -0,0 +1,397 @@
+/*******************************************************************************
+* Copyright (c) 2018-2024 Cadence Design Systems, Inc.
+*
+* Permission is hereby granted, free of charge, to any person obtaining
+* a copy of this software and associated documentation files (the
+* "Software"), to use this Software with Cadence processor cores only and
+* not with any other processors and platforms, subject to
+* the following conditions:
+*
+* The above copyright notice and this permission notice shall be included
+* in all copies or substantial portions of the Software.
+*
+* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+* IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+* SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+******************************************************************************/
+#include "xa_type_def.h"
+#include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common.h"
+#include "xa_nnlib_common_macros.h"
+
+WORD16 exp_lut[513] = {
+      2,     2,     2,     2,     2,     2,     2,     2,
+      2,     2,     2,     2,     2,     2,     2,     2,
+      2,     2,     2,     2,     2,     2,     2,     2,
+      2,     2,     2,     3,     3,     3,     3,     3,
+      3,     3,     3,     3,     3,     3,     3,     3,
+      3,     3,     3,     3,     4,     4,     4,     4,
+      4,     4,     4,     4,     4,     4,     4,     4,
+      4,     5,     5,     5,     5,     5,     5,     5,
+      5,     5,     5,     6,     6,     6,     6,     6,
+      6,     6,     6,     7,     7,     7,     7,     7,
+      7,     7,     7,     8,     8,     8,     8,     8,
+      8,     9,     9,     9,     9,     9,     9,    10,
+     10,    10,    10,    10,    11,    11,    11,    11,
+     11,    12,    12,    12,    12,    13,    13,    13,
+     13,    14,    14,    14,    14,    15,    15,    15,
+     16,    16,    16,    17,    17,    17,    18,    18,
+     18,    19,    19,    19,    20,    20,    21,    21,
+     21,    22,    22,    23,    23,    24,    24,    25,
+     25,    26,    26,    27,    27,    28,    28,    29,
+     29,    30,    30,    31,    32,    32,    33,    34,
+     34,    35,    36,    36,    37,    37,    38,    39,
+     40,    40,    42,    42,    43,    44,    45,    45,
+     46,    47,    48,    49,    50,    51,    52,    53,
+     54,    55,    56,    57,    59,    60,    60,    62,
+     63,    65,    65,    67,    68,    69,    71,    73,
+     74,    75,    77,    78,    80,    81,    83,    85,
+     86,    88,    90,    92,    93,    95,    97,    99,
+    101,   103,   105,   107,   109,   112,   114,   116,
+    118,   121,   123,   126,   128,   131,   133,   135,
+    139,   141,   144,   147,   149,   152,   155,   158,
+    162,   165,   168,   171,   174,   178,   181,   185,
+    189,   192,   196,   200,   204,   208,   212,   217,
+    221,   225,   230,   234,   239,   243,   248,   253,
+    258,   263,   268,   273,   279,   284,   290,   296,
+    302,   308,   314,   320,   327,   333,   340,   346,
+    353,   360,   366,   374,   381,   389,   397,   404,
+    413,   421,   429,   437,   446,   455,   464,   473,
+    482,   492,   501,   511,   522,   532,   543,   553,
+    564,   575,   586,   598,   610,   622,   634,   646,
+    659,   672,   685,   699,   713,   727,   741,   756,
+    771,   786,   801,   817,   833,   850,   866,   884,
+    901,   919,   937,   955,   974,   993,  1013,  1033,
+   1053,  1074,  1095,  1117,  1139,  1161,  1184,  1207,
+   1232,  1256,  1281,  1306,  1332,  1358,  1385,  1412,
+   1440,  1468,  1497,  1527,  1557,  1587,  1619,  1651,
+   1683,  1716,  1750,  1785,  1820,  1856,  1892,  1930,
+   1968,  2006,  2046,  2087,  2128,  2170,  2212,  2256,
+   2300,  2346,  2392,  2439,  2488,  2537,  2587,  2638,
+   2690,  2743,  2796,  2852,  2908,  2966,  3024,  3084,
+   3145,  3207,  3270,  3334,  3400,  3467,  3535,  3605,
+   3677,  3749,  3822,  3898,  3975,  4053,  4133,  4214,
+   4297,  4383,  4469,  4557,  4647,  4739,  4833,  4927,
+   5024,  5124,  5225,  5328,  5433,  5541,  5649,  5761,
+   5875,  5991,  6109,  6230,  6352,  6477,  6605,  6736,
+   6868,  7004,  7141,  7282,  7427,  7572,  7722,  7874,
+   8030,  8188,  8350,  8514,  8683,  8854,  9028,  9206,
+   9387,  9572,  9762,  9954, 10151, 10351, 10555, 10763,
+  10976, 11191, 11412, 11637, 11867, 12102, 12341, 12583,
+  12831, 13085, 13342, 13606, 13874, 14148, 14427, 14711,
+  15002, 15297, 15599, 15907, 16221, 16541, 16867, 17199,
+  17539, 17884, 18237, 18597, 18964, 19338, 19719, 20108,
+  20505, 20909, 21322, 21742, 22171, 22608, 23054, 23509,
+  23973, 24445, 24928, 25419, 25921, 26432, 26953, 27485,
+  28027, 28580, 29143, 29718, 30304, 30902, 31512, 32133,
+  32767};
+
+WORD16 one_over_one_plus_x_lut[513] = {
+  32767, 32704, 32640, 32578, 32514, 32451, 32388, 32326,
+  32264, 32202, 32141, 32079, 32018, 31957, 31896, 31835,
+  31775, 31715, 31655, 31596, 31537, 31476, 31418, 31359,
+  31301, 31242, 31184, 31127, 31069, 31011, 30954, 30897,
+  30840, 30784, 30727, 30671, 30615, 30560, 30504, 30449,
+  30394, 30339, 30283, 30229, 30175, 30121, 30067, 30013,
+  29960, 29906, 29853, 29800, 29746, 29694, 29642, 29589,
+  29537, 29486, 29434, 29382, 29331, 29280, 29229, 29177,
+  29127, 29076, 29026, 28976, 28926, 28877, 28827, 28777,
+  28728, 28679, 28630, 28581, 28532, 28484, 28436, 28388,
+  28340, 28292, 28244, 28197, 28150, 28103, 28056, 28008,
+  27962, 27915, 27869, 27823, 27777, 27731, 27685, 27640,
+  27594, 27549, 27504, 27459, 27413, 27369, 27324, 27280,
+  27236, 27192, 27148, 27104, 27060, 27016, 26973, 26930,
+  26887, 26844, 26801, 26758, 26715, 26673, 26630, 26588,
+  26546, 26504, 26463, 26421, 26380, 26338, 26297, 26255,
+  26214, 26174, 26132, 26092, 26051, 26011, 25971, 25931,
+  25891, 25851, 25811, 25772, 25732, 25693, 25653, 25614,
+  25575, 25536, 25497, 25458, 25420, 25381, 25343, 25305,
+  25267, 25229, 25191, 25153, 25116, 25078, 25041, 25003,
+  24966, 24928, 24892, 24855, 24818, 24781, 24745, 24709,
+  24672, 24636, 24600, 24564, 24528, 24492, 24457, 24421,
+  24385, 24350, 24315, 24280, 24245, 24210, 24175, 24140,
+  24105, 24070, 24036, 24002, 23967, 23933, 23899, 23865,
+  23831, 23798, 23764, 23730, 23697, 23664, 23630, 23597,
+  23564, 23530, 23498, 23465, 23432, 23399, 23366, 23334,
+  23302, 23269, 23237, 23205, 23173, 23141, 23109, 23077,
+  23046, 23014, 22982, 22951, 22920, 22888, 22857, 22826,
+  22795, 22764, 22733, 22703, 22672, 22641, 22611, 22580,
+  22550, 22520, 22490, 22459, 22429, 22400, 22370, 22340,
+  22310, 22281, 22251, 22221, 22192, 22163, 22134, 22104,
+  22075, 22046, 22017, 21988, 21959, 21931, 21902, 21874,
+  21845, 21817, 21788, 21760, 21732, 21704, 21676, 21648,
+  21620, 21592, 21565, 21537, 21509, 21482, 21455, 21427,
+  21400, 21372, 21345, 21318, 21291, 21264, 21237, 21210,
+  21183, 21157, 21130, 21103, 21077, 21050, 21024, 20998,
+  20971, 20945, 20919, 20893, 20867, 20841, 20816, 20790,
+  20764, 20738, 20713, 20687, 20662, 20636, 20611, 20586,
+  20560, 20535, 20510, 20485, 20460, 20435, 20410, 20385,
+  20360, 20336, 20311, 20287, 20262, 20238, 20213, 20189,
+  20165, 20141, 20117, 20092, 20068, 20044, 20021, 19997,
+  19973, 19949, 19926, 19902, 19878, 19855, 19832, 19808,
+  19784, 19762, 19738, 19715, 19692, 19668, 19645, 19622,
+  19600, 19577, 19553, 19531, 19508, 19485, 19463, 19440,
+  19418, 19395, 19373, 19351, 19328, 19306, 19284, 19262,
+  19240, 19218, 19196, 19174, 19152, 19130, 19109, 19087,
+  19065, 19044, 19022, 19000, 18979, 18958, 18936, 18915,
+  18893, 18872, 18851, 18830, 18809, 18787, 18766, 18745,
+  18725, 18704, 18682, 18662, 18641, 18620, 18600, 18579,
+  18559, 18538, 18518, 18497, 18477, 18457, 18436, 18416,
+  18396, 18376, 18356, 18336, 18316, 18296, 18276, 18256,
+  18236, 18216, 18197, 18177, 18157, 18138, 18118, 18099,
+  18079, 18059, 18040, 18021, 18001, 17982, 17963, 17944,
+  17924, 17905, 17886, 17867, 17848, 17829, 17810, 17791,
+  17772, 17754, 17735, 17716, 17697, 17679, 17660, 17641,
+  17623, 17604, 17586, 17568, 17549, 17531, 17513, 17494,
+  17476, 17458, 17440, 17422, 17404, 17386, 17368, 17350,
+  17332, 17314, 17296, 17278, 17261, 17243, 17225, 17208,
+  17190, 17172, 17155, 17137, 17120, 17102, 17085, 17067,
+  17050, 17033, 17015, 16999, 16981, 16964, 16947, 16930,
+  16913, 16895, 16878, 16862, 16845, 16828, 16810, 16794,
+  16777, 16760, 16743, 16727, 16710, 16693, 16677, 16660,
+  16644, 16627, 16611, 16594, 16578, 16562, 16545, 16529,
+  16513, 16497, 16480, 16464, 16448, 16432, 16416, 16400,
+  16384};
+
+static inline ae_int16x4 LUTLookUpX4(ae_int16x4 value, WORD16* lut)
+{
+  ae_int16x4 shifted_value = AE_SRAI16(value, 7);
+  ae_int16x4 index = AE_ADD16S(AE_MOVDA16(256), shifted_value);
+  ae_int16x4 offset_ls8 = AE_SLAI16S(AE_AND16(value, AE_MOVDA16(0x7f)), 8);
+
+  WORD32 index0, index1, index2, index3;
+  index0 = AE_MOVAD16_3(index);
+  index1 = AE_MOVAD16_2(index);
+  index2 = AE_MOVAD16_1(index);
+  index3 = AE_MOVAD16_0(index);
+
+  ae_int16 *p_ae_lut = (ae_int16 *)lut;
+  ae_int16x4 base0123 = p_ae_lut[index0];
+  base0123 = AE_SEL16_6543(base0123, p_ae_lut[index1]);
+  base0123 = AE_SEL16_6543(base0123, p_ae_lut[index2]);
+  base0123 = AE_SEL16_6543(base0123, p_ae_lut[index3]);
+
+  ae_int16x4 slope0123 = p_ae_lut[index0 + 1];
+  slope0123 = AE_SEL16_6543(slope0123, p_ae_lut[index1 + 1]);
+  slope0123 = AE_SEL16_6543(slope0123, p_ae_lut[index2 + 1]);
+  slope0123 = AE_SEL16_6543(slope0123, p_ae_lut[index3 + 1]);
+  slope0123 = AE_SUB16S(slope0123, base0123);
+
+  ae_int16x4 delta0123;
+  delta0123 = AE_MULFP16X4RAS(slope0123, offset_ls8);
+  ae_int16x4 result0123 = AE_ADD16S(base0123, delta0123);
+
+  return result0123;
+}
+
+static inline ae_int16x4 LUTLookUp(ae_int16x4 value, WORD16* lut)
+{
+  ae_int16x4 shifted_value = AE_SRAI16(value, 7);
+  ae_int16x4 index = AE_ADD16S(AE_MOVDA16(256), shifted_value);
+  ae_int16x4 offset_ls8 = AE_SLAI16S(AE_AND16(value, AE_MOVDA16(0x7f)), 8);
+
+  WORD32 index0;
+  index0 = AE_MOVAD16_0(index);
+
+  ae_int16 *p_ae_lut = (ae_int16 *)lut;
+  ae_int16x4 base = p_ae_lut[index0];
+
+  ae_int16x4 slope = p_ae_lut[index0 + 1];
+  slope = AE_SUB16S(slope, base);
+
+  ae_int16x4 delta;
+  delta = AE_MULFP16X4RAS(slope, offset_ls8);
+  ae_int16x4 result = AE_ADD16S(base, delta);
+
+  return result;
+}
+
+// Computes exp(input - max_input)
+
+#define SOFTMAX_CALCULATE_EXP(result, left_shift, right_shift, multiplier, d_inp, max_in_row) \
+{ \
+  ae_int32x2 input_diff1, input_diff2, scaled_diff1, scaled_diff2; \
+  ae_int32x2 sym_scaled_diff1, sym_scaled_diff2; \
+  ae_int16x4 d_one16 = AE_MOVDA16(1); \
+  AE_MUL16X4(input_diff1, input_diff2, d_inp, d_one16); \
+  AE_MULS16X4(input_diff1, input_diff2, max_in_row, d_one16); \
+  MPY_BY_QUANT_MULT_SLS_X2_OUT32(scaled_diff1, input_diff1, input_beta_multiplier, left_shift, right_shift); \
+  MPY_BY_QUANT_MULT_SLS_X2_OUT32(scaled_diff2, input_diff2, input_beta_multiplier, left_shift, right_shift); \
+  ae_int32x2 max_int16s = AE_MOVDA32(32767); \
+  sym_scaled_diff1 = AE_ADD32S(scaled_diff1, max_int16s); \
+  sym_scaled_diff2 = AE_ADD32S(scaled_diff2, max_int16s); \
+  ae_int16x4 sat_sym_shifted_sum = AE_SAT16X4(sym_scaled_diff1, sym_scaled_diff2); \
+  result = LUTLookUpX4(sat_sym_shifted_sum, exp_lut); \
+} \
+
+/* In Matlab this taking input as uint32_t hence need +1 with AE_NSAZ32_L */
+WORD32 count_leading_zeros(ae_int32x2 integer_input)
+{
+  WORD32 value = AE_MOVDA32(integer_input);
+  if(value == 0)
+  {
+    return 32;
+  }
+  return AE_NSAZ32_L(integer_input) + 1;
+}
+
+WORD32 xa_nn_vec_softmax_sym16s_16( WORD16 * __restrict__ p_out,
+                    const   WORD16 * __restrict__ p_vec,
+                            WORD32  input_beta_left_shift,
+                            WORD32  input_beta_multiplier,
+                            WORD32  vec_length)
+{
+  /* NULL pointer checks */
+  XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_vec, -1);
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_out, sizeof(WORD16), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_vec, sizeof(WORD16), -1);
+  /* Basic Parameter checks */
+  XA_NNLIB_ARG_CHK_COND((vec_length <= 0), -1);
+  XA_NNLIB_ARG_CHK_COND(((input_beta_left_shift < -31) || (input_beta_left_shift > 31)), -1);
+  XA_NNLIB_ARG_CHK_COND((input_beta_multiplier < 0), -1);
+
+  // Calculating Max
+  ae_int16x4 d_max;
+  int i;
+  {
+    ae_int16x4 d0;
+    xtbool4 b4;
+    ae_int16x4 *p_inp = (ae_int16x4 *)p_vec;
+    ae_valign align_inp = AE_LA64_PP(p_inp);
+    d_max = AE_MOVDA16(0x8000);
+
+    for(i = 0; i < (vec_length >> 2); i++)
+    {
+      AE_LA16X4_IP(d0, align_inp, p_inp);
+      b4 = AE_LT16(d_max, d0);
+      AE_MOVT16X4(d_max, d0, b4);
+    }
+    {
+      d0 = AE_SEL16_5432(d_max, d_max);
+      b4 = AE_LT16(d_max, d0);
+      AE_MOVT16X4(d_max, d0, b4);
+      d0 = AE_SEL16_6543(d_max, d_max);
+      b4 = AE_LT16(d_max, d0);
+      AE_MOVT16X4(d_max, d0, b4);
+    }
+
+    for(i = 0; i < (vec_length & 3); i++)
+    {
+      AE_L16_IP(d0, (ae_int16 *)p_inp, sizeof(ae_int16));
+      b4 = AE_LT16(d_max, d0);
+      AE_MOVT16X4(d_max, d0, b4);
+    }
+  }
+
+#if TFLITE_SINGLE_ROUNDING
+  int left_shift  = input_beta_left_shift;
+  int right_shift = input_beta_left_shift;
+  /* Single rounding macro doesn't need two shifts so this is not used */
+  (void)right_shift;
+#else /* #if TFLITE_SINGLE_ROUNDING */
+  int left_shift  = input_beta_left_shift < 0 ? 0 : input_beta_left_shift;
+  int right_shift = input_beta_left_shift > 0 ? 0 : -input_beta_left_shift;
+#endif /* #if TFLITE_SINGLE_ROUNDING */
+
+  //Compute exp and sum_of_exp
+  ae_int32x2 sum_of_exps;
+  {
+    ae_int16x4 *temp_out = (ae_int16x4 *)p_out;
+    ae_int16x4 *p_inp = (ae_int16x4 *)p_vec;
+    ae_int16x4 d_inp1;
+    ae_valign align_input64 = AE_LA64_PP(p_inp);
+    ae_int32x2 acc1, acc2, acc;
+    acc1 = AE_ZERO32();
+    acc2 = AE_ZERO32();
+    ae_valign align_output64 = AE_ZALIGN64();
+    ae_int16x4 exp1;
+    for(i = 0; i < (vec_length >> 2); i++)
+    {
+      AE_LA16X4_IP(d_inp1, align_input64, p_inp);
+      SOFTMAX_CALCULATE_EXP(exp1, left_shift, right_shift, input_beta_multiplier, d_inp1, d_max);
+      AE_SA16X4_IP(exp1, align_output64, temp_out);
+      AE_MULA16X4(acc1, acc2, exp1, AE_MOVDA16(1));
+    }
+    AE_SA64POS_FP(align_output64,(void *)temp_out);
+
+    int rem_length = vec_length & 3;
+
+    {
+      d_inp1 = AE_ZERO16();
+      ae_int16x4 d_exp_and = d_inp1;
+      for(i = 0; i < rem_length; i++)
+      {
+        ae_int16x4 d_tmp;
+        AE_L16_IP(d_tmp, (ae_int16 *)p_inp, 2);
+        d_inp1 = AE_SEL16_6543(d_inp1, d_tmp);
+        d_exp_and = AE_SEL16_6543(d_exp_and, AE_MOVDA16(0xffff));
+      }
+      SOFTMAX_CALCULATE_EXP(exp1, left_shift, right_shift, input_beta_multiplier, d_inp1, d_max);
+      exp1 = AE_AND16(exp1, d_exp_and);
+      AE_MULA16X4(acc1, acc2, exp1, AE_MOVDA16(1));
+      ae_int16 *temp_out_ua = (ae_int16 *)temp_out + (rem_length - 1);
+      for(i = 0; i < rem_length; i++)
+      {
+        AE_S16_0_IP(exp1, (ae_int16 *)temp_out_ua, -2);
+        exp1 = AE_SEL16_4321(exp1, exp1);
+      }
+    }
+
+    acc = AE_ADD32S(acc1, acc2);
+    sum_of_exps = AE_ADD32S(acc, AE_SEL32_LH(acc, acc));
+  }
+
+  // Calculate 1/sum_of_exps
+  WORD32 headroom_plus_one = count_leading_zeros(sum_of_exps);
+  ae_int32x2 shifted_sum = AE_SRAA32RS(sum_of_exps, 14 - (headroom_plus_one - 1));
+  ae_int32x2 plus_one_sym = AE_MOVDA32(-((1<<15) + (1<<16)));
+  ae_int32x2 sym_shifted_sum = AE_ADD32S(shifted_sum, plus_one_sym);
+  ae_int16x4 sat_sym_shifted_sum = AE_SAT16X4(sym_shifted_sum, sym_shifted_sum);
+  ae_int16x4 reciprocal_scale_q015 = LUTLookUp(sat_sym_shifted_sum, one_over_one_plus_x_lut);
+
+  // Compute exp*1/sum_of_exps
+  {
+    ae_int16x4 *temp_out1 = (ae_int16x4 *)p_out;
+    WORD32 right_shift = 31 - headroom_plus_one;
+    ae_int16x4 exp1;
+    ae_valign exp_align = AE_LA64_PP(temp_out1);
+    ae_int32x2 sfmx1, sfmx2;
+    ae_int32x2 shifted_sfmx1, shifted_sfmx2;
+    ae_int16x4 sfmx12;
+    ae_int16x4 *temp_out2 = (ae_int16x4 *)p_out;
+    ae_valign align_output = AE_ZALIGN64();
+    ae_int32x2 zero32 = AE_ZERO32();
+
+    for(i=0; i<(vec_length >> 2); i++)
+    {
+      AE_LA16X4_IP(exp1, exp_align, temp_out1);
+      AE_MUL16X4(sfmx1, sfmx2, exp1, reciprocal_scale_q015);
+      shifted_sfmx1 = AE_SRAA32RS(sfmx1, right_shift);
+      shifted_sfmx2 = AE_SRAA32RS(sfmx2, right_shift);
+      shifted_sfmx1 = AE_MAX32(shifted_sfmx1, zero32);
+      shifted_sfmx2 = AE_MAX32(shifted_sfmx2, zero32);
+      sfmx12 = AE_SAT16X4(shifted_sfmx1, shifted_sfmx2);
+      AE_SA16X4_IP(sfmx12, align_output, temp_out2);
+    }
+    AE_SA64POS_FP(align_output, (void *)temp_out2);
+    int rem_length = vec_length & 3;
+    for(i = 0; i < rem_length; i++)
+    {
+      AE_L16_IP(exp1, (ae_int16 *)temp_out1, 2);
+      AE_MUL16X4(sfmx1, sfmx2, exp1, reciprocal_scale_q015);
+      shifted_sfmx1 = AE_SRAA32RS(sfmx1, right_shift);
+      shifted_sfmx1 = AE_MAX32(shifted_sfmx1, zero32);
+      sfmx12 = AE_SAT16X4(shifted_sfmx1, shifted_sfmx1);
+      AE_S16_0_IP(sfmx12, (ae_int16 *)temp_out2, 2);
+    }
+  }
+
+  return 0;
+}
diff --git a/algo/kernels/basic/hifi4/xa_nn_elm_quantize.c b/algo/kernels/basic/hifi4/xa_nn_elm_quantize.c
index fd0cc13..194c0b1 100644
--- a/algo/kernels/basic/hifi4/xa_nn_elm_quantize.c
+++ b/algo/kernels/basic/hifi4/xa_nn_elm_quantize.c
@@ -647,6 +647,150 @@ WORD32 xa_nn_elm_requantize_asym8s_asym8s(WORD8 * __restrict__ p_out,
   return 0;
 }
 
+WORD32 xa_nn_elm_requantize_asym8u_asym8s(WORD8 * __restrict__ p_out,
+                                    const UWORD8 * __restrict__ p_inp,
+                                    WORD32  inp_zero_bias,
+                                    WORD32  out_zero_bias,
+                                    WORD32  out_shift,
+                                    WORD32  out_multiplier,
+                                    WORD32  num_elm)
+{
+  /* NULL pointer checks */
+  XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_out, sizeof(WORD8), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_inp, sizeof(WORD8), -1);
+  /* Basic Parameter checks */
+  XA_NNLIB_ARG_CHK_COND((num_elm <= 0), -1);
+  XA_NNLIB_ARG_CHK_COND(((out_zero_bias < -128) || (out_zero_bias > 127)), -1);
+  XA_NNLIB_ARG_CHK_COND(((inp_zero_bias < 0) || (inp_zero_bias > 255)), -1);
+  XA_NNLIB_ARG_CHK_COND(((out_shift < -31) || (out_shift > 31)), -1);
+  XA_NNLIB_ARG_CHK_COND((out_multiplier < 0), -1);
+
+  int i;
+  int left_shift, right_shift;
+#if TFLITE_SINGLE_ROUNDING
+  left_shift = out_shift;
+  /* Single rounding doesn't need two shifts */
+  (void)right_shift;
+#else /* #if TFLITE_SINGLE_ROUNDING */
+  left_shift  = (out_shift < 0)?0:out_shift;
+  right_shift = (out_shift > 0)?0:-out_shift;
+#endif /* #if TFLITE_SINGLE_ROUNDING */
+
+  WORD8 *p_i = (WORD8 *)p_inp;
+  WORD8 *p_o = (WORD8 *)p_out;
+
+  ae_int16x4 ONE = AE_MOVDA16(1);
+  ae_int16x4 d_inp_zero_bias  = AE_MOVDA16(inp_zero_bias);
+  ae_int32x2 d_out_multiplier = AE_MOVDA32(out_multiplier);
+  ae_int16x4 d_inp0;
+  ae_int32x2 d_inp32_0, d_inp32_1;
+  ae_int32x2 d_out0_32, d_out1_32;
+  ae_int32x2 quant_min = AE_MOVDA32(-128);
+  ae_int32x2 quant_max = AE_MOVDA32(127);
+
+  xtbool io_pointers_aligned = ((((uintptr_t)p_inp) & 3) == 0);
+#if XCHAL_HAVE_HIFI1
+  ALIGN_REGISTER_TYPE align_out = AE_ZALIGN64();
+#endif
+  if(io_pointers_aligned){
+    for(i = 0; i < num_elm >> 2; i++)
+    {
+#if XCHAL_HAVE_HIFI1
+      AE_L8X4U_IP(d_inp0, p_i, 4);
+#else
+      AE_L8X4F_IP(d_inp0, p_i, 4);
+      d_inp0 = AE_SRAI16(d_inp0, 8);
+      d_inp0 = AE_AND16(d_inp0, AE_MOVDA16(0x00ff));
+#endif
+      d_inp0 = AE_SUB16S(d_inp0, d_inp_zero_bias);
+
+      AE_MUL16X4(d_inp32_0 , d_inp32_1 , d_inp0 , ONE);
+      MPY_BY_QUANT_MULT_SLS_X2_OUT32(d_out0_32, d_inp32_0, d_out_multiplier, left_shift, right_shift);
+      MPY_BY_QUANT_MULT_SLS_X2_OUT32(d_out1_32, d_inp32_1, d_out_multiplier, left_shift, right_shift);
+      d_out0_32 = AE_ADD32S(d_out0_32, AE_MOVDA32(out_zero_bias));
+      d_out1_32 = AE_ADD32S(d_out1_32, AE_MOVDA32(out_zero_bias));
+#if XCHAL_HAVE_HIFI1
+#if ( XCHAL_HW_VERSION >= RI9_HWVERSION )
+      // clamped_out
+      ae_int8x8 clamped_01 = AE_SAT8X4X32_H(d_out0_32, d_out1_32);
+      // Store Output
+      AE_SAV8X8_XP(clamped_01, align_out, (ae_int8x8 *)p_o, 4);
+#else
+      CLAMP_VAL(d_out0_32, d_out0_32, quant_min, quant_max)
+      CLAMP_VAL(d_out1_32, d_out1_32, quant_min, quant_max)
+      ae_int16x4 out = AE_SEL16_6420(AE_MOVF16X4_FROMF32X2(d_out0_32), AE_MOVF16X4_FROMF32X2(d_out1_32));
+      AE_SA8X4U_IP(out, align_out, (ae_int32 *)p_o);
+#endif
+#else
+      CLAMP_VAL(d_out0_32, d_out0_32, quant_min, quant_max)
+      CLAMP_VAL(d_out1_32, d_out1_32, quant_min, quant_max)
+      STORE_8X4_FROM_32X4(p_o, d_out0_32, d_out1_32);
+#endif
+    }
+#if XCHAL_HAVE_HIFI1
+    AE_SA64POS_FP(align_out, p_o);
+#endif
+  }
+  else{
+    ALIGN_REGISTER_TYPE align_inp;
+    PRIME_8X4F(p_i, align_inp);
+    for(i = 0; i < num_elm >> 2; i++)
+    {
+#if XCHAL_HAVE_HIFI1
+      AE_LA8X4U_IP(d_inp0, align_inp, p_i);
+#else
+      AE_LA8X4F_IP(d_inp0, align_inp, p_i);
+      d_inp0 = AE_SRAI16(d_inp0, 8);
+      d_inp0 = AE_AND16(d_inp0, AE_MOVDA16(0x00ff));
+#endif
+      d_inp0 = AE_SUB16S(d_inp0, d_inp_zero_bias);
+
+      AE_MUL16X4(d_inp32_0 , d_inp32_1 , d_inp0 , ONE);
+      MPY_BY_QUANT_MULT_SLS_X2_OUT32(d_out0_32, d_inp32_0, d_out_multiplier, left_shift, right_shift);
+      MPY_BY_QUANT_MULT_SLS_X2_OUT32(d_out1_32, d_inp32_1, d_out_multiplier, left_shift, right_shift);
+      d_out0_32 = AE_ADD32S(d_out0_32, AE_MOVDA32(out_zero_bias));
+      d_out1_32 = AE_ADD32S(d_out1_32, AE_MOVDA32(out_zero_bias));
+#if XCHAL_HAVE_HIFI1
+#if ( XCHAL_HW_VERSION >= RI9_HWVERSION )
+      // clamped_out
+      ae_int8x8 clamped_01 = AE_SAT8X4X32_H(d_out0_32, d_out1_32);
+      // Store Output
+      AE_SAV8X8_XP(clamped_01, align_out, (ae_int8x8 *)p_o, 4);
+#else
+      CLAMP_VAL(d_out0_32, d_out0_32, quant_min, quant_max)
+      CLAMP_VAL(d_out1_32, d_out1_32, quant_min, quant_max)
+      ae_int16x4 out = AE_SEL16_6420(AE_MOVF16X4_FROMF32X2(d_out0_32), AE_MOVF16X4_FROMF32X2(d_out1_32));
+      AE_SA8X4U_IP(out, align_out, (ae_int32 *)p_o);
+#endif
+#else
+      CLAMP_VAL(d_out0_32, d_out0_32, quant_min, quant_max)
+      CLAMP_VAL(d_out1_32, d_out1_32, quant_min, quant_max)
+      STORE_8X4_FROM_32X4(p_o, d_out0_32, d_out1_32);
+#endif
+    }
+#if XCHAL_HAVE_HIFI1
+    AE_SA64POS_FP(align_out, p_o);
+#endif
+  }
+#pragma loop_count max=3
+  for(i = 0; i < (num_elm & 3) ; i++)
+  {
+    ae_int16x4 d_inp0 = AE_MOVDA16((WORD32)((UWORD8 *)p_i)[i] - inp_zero_bias);
+
+    d_inp32_0 = AE_SEXT32X2D16_10(d_inp0);
+    MPY_BY_QUANT_MULT_SLS_X2_OUT32(d_out0_32, d_inp32_0, d_out_multiplier, left_shift, right_shift);
+    d_out0_32 = AE_ADD32S(d_out0_32, AE_MOVDA32(out_zero_bias));
+
+    CLAMP_VAL(d_out0_32, d_out0_32, quant_min, quant_max)
+    *p_o = (WORD8)AE_MOVAD32_L(d_out0_32);
+    p_o++;
+  }
+  return 0;
+}
+
 #if !HAVE_VFPU
 DISCARD_FUN_FOR_NONVOID_RETURN(WORD32, xa_nn_elm_dequantize_asym8s_f32,
                                (FLOAT32 * __restrict__ p_out,
@@ -827,6 +971,10 @@ WORD32 xa_nn_elm_quantize_f32_asym8s(WORD8 * __restrict__ p_out,
   ae_valign align_inp = AE_LA64_PP(p_inp);
   ae_int32x2 quant_max = AE_MOVDA32(127);
   ae_int32x2 quant_min = AE_MOVDA32(-128);
+  xtfloatx2 d_out_scale = (xtfloatx2)*out_scale_ptr;
+  xtfloatx2 d_one = XT_FLOAT_SX2(AE_MOVDA32(1), 0);
+  xtfloatx2 d_one_over_out_scale = XT_DIV_SX2(d_one, d_out_scale);
+
 #if (XCHAL_HAVE_HIFI1 &( XCHAL_HW_VERSION >= RI9_HWVERSION ))
   ALIGN_REGISTER_TYPE align_out = AE_ZALIGN64();
 #endif
@@ -835,12 +983,11 @@ WORD32 xa_nn_elm_quantize_f32_asym8s(WORD8 * __restrict__ p_out,
     xtfloatx2 d_inp0, d_inp1;
     xtfloatx2 d_inp0_t, d_inp1_t;
     ae_int32x2 d_out32_0, d_out32_1;
-    xtfloatx2 d_out_scale = (xtfloatx2)*out_scale_ptr;
 
     XT_LASX2IP(d_inp0, align_inp, p_i);
     XT_LASX2IP(d_inp1, align_inp, p_i);
-    d_inp0_t = XT_DIV_SX2(d_inp0, d_out_scale);
-    d_inp1_t = XT_DIV_SX2(d_inp1, d_out_scale);
+    d_inp0_t = XT_MUL_SX2(d_inp0, d_one_over_out_scale);
+    d_inp1_t = XT_MUL_SX2(d_inp1, d_one_over_out_scale);
     d_inp0_t = XT_FIROUND_SX2(d_inp0_t);
     d_inp1_t = XT_FIROUND_SX2(d_inp1_t);    
     d_out32_0 = XT_TRUNC_SX2(d_inp0_t, 0);
@@ -863,12 +1010,11 @@ WORD32 xa_nn_elm_quantize_f32_asym8s(WORD8 * __restrict__ p_out,
 #endif
   for(i = 0; i < (num_elm & 3) ; i++)
   {
-    xtfloat d_out_scale = (xtfloat)*out_scale_ptr;
     xtfloat d_inp0;
     xtfloat d_inp0_t;
     ae_int32x2 d_out32_0;
     XT_LSIP(d_inp0, (xtfloat *)p_i, sizeof(FLOAT32));
-    d_inp0_t = XT_DIV_S(d_inp0, d_out_scale);
+    d_inp0_t = XT_MUL_S(d_inp0, d_one_over_out_scale);
     d_inp0_t = XT_FIROUND_S(d_inp0_t);
     d_out32_0 = XT_TRUNC_S(d_inp0_t, 0);    
     d_out32_0 = AE_ADD32S(d_out32_0, d_out_zero_bias);
@@ -915,18 +1061,20 @@ WORD32 xa_nn_elm_quantize_f32_asym16s(WORD16 * __restrict__ p_out,
   ae_valign align_out = AE_ZALIGN64();
   ae_int32x2 quant_max = AE_MOVDA32(32767);
   ae_int32x2 quant_min = AE_MOVDA32(-32768);
+  xtfloatx2 d_out_scale = (xtfloatx2)*out_scale_ptr;
+  xtfloatx2 d_one = XT_FLOAT_SX2(AE_MOVDA32(1), 0);
+  xtfloatx2 d_one_over_out_scale = XT_DIV_SX2(d_one, d_out_scale);
 
   for(i = 0; i < (num_elm >> 2); i++)
   {
     xtfloatx2 d_inp0, d_inp1;
     xtfloatx2 d_inp0_t, d_inp1_t;
     ae_int32x2 d_out32_0, d_out32_1;
-    xtfloatx2 d_out_scale = (xtfloatx2)*out_scale_ptr;
 
     XT_LASX2IP(d_inp0, align_inp, p_i);
     XT_LASX2IP(d_inp1, align_inp, p_i);
-    d_inp0_t = XT_DIV_SX2(d_inp0, d_out_scale);
-    d_inp1_t = XT_DIV_SX2(d_inp1, d_out_scale);
+    d_inp0_t = XT_MUL_SX2(d_inp0, d_one_over_out_scale);
+    d_inp1_t = XT_MUL_SX2(d_inp1, d_one_over_out_scale);
     d_inp0_t = XT_FIROUND_SX2(d_inp0_t);
     d_inp1_t = XT_FIROUND_SX2(d_inp1_t);    
     d_out32_0 = XT_TRUNC_SX2(d_inp0_t, 0);
@@ -943,12 +1091,11 @@ WORD32 xa_nn_elm_quantize_f32_asym16s(WORD16 * __restrict__ p_out,
 
   for(i = 0; i < (num_elm & 3) ; i++)
   {
-    xtfloat d_out_scale = (xtfloat)*out_scale_ptr;
     xtfloat d_inp0;
     xtfloat d_inp0_t;
     ae_int32x2 d_out32_0;
     XT_LSIP(d_inp0, (xtfloat *)p_i, sizeof(FLOAT32));
-    d_inp0_t = XT_DIV_S(d_inp0, d_out_scale);
+    d_inp0_t = XT_MUL_S(d_inp0, d_one_over_out_scale);
     d_inp0_t = XT_FIROUND_S(d_inp0_t);
     d_out32_0 = XT_TRUNC_S(d_inp0_t, 0);    
     d_out32_0 = AE_ADD32S(d_out32_0, d_out_zero_bias);
diff --git a/algo/kernels/basic/hifi4/xa_nn_elm_rsqrt_f32.c b/algo/kernels/basic/hifi4/xa_nn_elm_rsqrt_f32.c
index 5d14ba4..e8c8d74 100644
--- a/algo/kernels/basic/hifi4/xa_nn_elm_rsqrt_f32.c
+++ b/algo/kernels/basic/hifi4/xa_nn_elm_rsqrt_f32.c
@@ -138,7 +138,9 @@ WORD32 xa_nn_elm_rsqrt_f32_f32(FLOAT32 * __restrict__ p_out,
   {
     xtfloat a1, a;
     XT_LSIP(a1, (xtfloat *)inp, 0);
-    a = XT_RSQRT_S(a1);
+    x1 = a1;
+    y1 = XT_RSQRT_SX2(x1);
+    a = y1;
     XT_SSI(a, (xtfloat *)out, 0);
   }
 
diff --git a/algo/kernels/basic/hifi4/xa_nn_reduce_asym8s_asym8s.c b/algo/kernels/basic/hifi4/xa_nn_reduce_asym8s_asym8s.c
index 47b0956..b400bcb 100644
--- a/algo/kernels/basic/hifi4/xa_nn_reduce_asym8s_asym8s.c
+++ b/algo/kernels/basic/hifi4/xa_nn_reduce_asym8s_asym8s.c
@@ -33,17 +33,6 @@
 
 #define BUS_WIDTH_8 7
 
-#define STORE_8X4_FROM_16X4(out_ptr, val){\
-    int o1, o2, o3, o4;\
-    o1 = AE_MOVAD16_3(val);\
-    o2 = AE_MOVAD16_2(val);\
-    o3 = AE_MOVAD16_1(val);\
-    o4 = AE_MOVAD16_0(val);\
-    *out_ptr++ = (WORD8)o1;\
-    *out_ptr++ = (WORD8)o2;\
-    *out_ptr++ = (WORD8)o3;\
-    *out_ptr++ = (WORD8)o4;\
-}
 
 WORD32 xa_nn_reduce_getsize_nhwc(WORD32 inp_precision
                                  ,const WORD32 *const p_inp_shape
@@ -96,6 +85,20 @@ WORD32 xa_nn_reduce_getsize_nhwc(WORD32 inp_precision
     return 0;
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
+
+#define STORE_8X4_FROM_16X4(out_ptr, val){\
+    int o1, o2, o3, o4;\
+    o1 = AE_MOVAD16_3(val);\
+    o2 = AE_MOVAD16_2(val);\
+    o3 = AE_MOVAD16_1(val);\
+    o4 = AE_MOVAD16_0(val);\
+    *out_ptr++ = (WORD8)o1;\
+    *out_ptr++ = (WORD8)o2;\
+    *out_ptr++ = (WORD8)o3;\
+    *out_ptr++ = (WORD8)o4;\
+}
+
 static void vecmax8_inpx3_aligned(const WORD8 *p_src1, const WORD8* p_src2, const WORD8* p_src3, WORD8 *p_dst, int N){
     int i = 0;
 #if XCHAL_HAVE_HIFI1
@@ -1992,3 +1995,4 @@ WORD32 xa_nn_reduce_mean_4D_asym8s_asym8s(WORD8 * __restrict__ p_out
 
   return 0;
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
diff --git a/algo/kernels/cnn/hifi4/xa_nn_circ_buf.c b/algo/kernels/cnn/hifi4/xa_nn_circ_buf.c
index dace733..7e9856d 100644
--- a/algo/kernels/cnn/hifi4/xa_nn_circ_buf.c
+++ b/algo/kernels/cnn/hifi4/xa_nn_circ_buf.c
@@ -46,7 +46,7 @@ int xa_nn_circ_buf_nchw_getsize(
   }
 
   circ_buf_width = kernel_width + ((output_width - 1) * x_stride);
-  circ_buf_width = XT_MAX(circ_buf_width, x_padding + input_width);
+  circ_buf_width = MAX(circ_buf_width, x_padding + input_width);
 
   /* Aligned size independent of bytewidth */
   circ_buf_width = ALIGNED_SIZE(circ_buf_width, 4);
@@ -388,8 +388,7 @@ int xa_nn_dilated_circ_buf_nhwc_getsize(
   int size_in_bytes;
 
   circ_buf_height = kernel_height + ((output_height - 1) * y_stride);
-//  circ_buf_height = XT_MAX(circ_buf_height, y_padding + input_height);
-  circ_buf_height = XT_MAX(circ_buf_height, (y_padding + input_height + dilation_height - 1)/dilation_height);
+  circ_buf_height = MAX(circ_buf_height, (y_padding + input_height + dilation_height - 1)/dilation_height);
 
   if(bytewidth == 4)
   {
diff --git a/algo/kernels/cnn/hifi4/xa_nn_circ_buf.h b/algo/kernels/cnn/hifi4/xa_nn_circ_buf.h
index 4315be5..5d1979b 100644
--- a/algo/kernels/cnn/hifi4/xa_nn_circ_buf.h
+++ b/algo/kernels/cnn/hifi4/xa_nn_circ_buf.h
@@ -23,6 +23,11 @@
 #ifndef __XA_NN_CIRC_BUF_H__
 #define __XA_NN_CIRC_BUF_H__
 
+#ifdef ENABLE_SCRATCH_SIZE_API_ONLY
+#define xa_nn_circ_buf_nchw_getsize     xa_nn_circ_buf_nchw_getsize_hifi4
+#define xa_nn_circ_buf_nhwc_getsize     xa_nn_circ_buf_nhwc_getsize_hifi4
+#endif
+
 #define OUT_HEIGHT_PER_ITER 2
 
 #define ALIGNMENT   8   /* 8 bytes alignment */
diff --git a/algo/kernels/cnn/hifi4/xa_nn_conv2d_depthwise.c b/algo/kernels/cnn/hifi4/xa_nn_conv2d_depthwise.c
index ac6ab07..04aa355 100644
--- a/algo/kernels/cnn/hifi4/xa_nn_conv2d_depthwise.c
+++ b/algo/kernels/cnn/hifi4/xa_nn_conv2d_depthwise.c
@@ -76,7 +76,7 @@ static WORD32 xa_nn_dilated_conv2d_depthwise_nchw_getsize
     circ_buf_size = ALIGNED_SIZE(circ_buf_size, ALIGNMENT);
 
     circ_buf_width = dilated_kernel_width + ((output_width - 1) * x_stride);
-    circ_buf_width = XT_MAX(circ_buf_width, x_padding+input_width);
+    circ_buf_width = MAX(circ_buf_width, x_padding+input_width);
     circ_buf_width = ALIGNED_SIZE(circ_buf_width, 4);
 
     /* Please note for future output_width_for_x_stride_1 calculation for getting output_width_for_x_stride_1
@@ -107,6 +107,7 @@ static WORD32 xa_nn_dilated_conv2d_depthwise_nchw_getsize
     }
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 static VOID xa_nn_conv2d_depthwise_nchw_init
 (pVOID p_scratch
  ,WORD32 input_width
@@ -197,6 +198,7 @@ static VOID xa_nn_dilated_conv2d_depthwise_nchw_init
     p_mem = (p_mem + circ_buf_size);
     p_state->p_scratch = (pVOID)p_mem;
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 
 static WORD32 gcd(WORD32 a, WORD32 b)
 {
@@ -266,6 +268,7 @@ static WORD32 xa_nn_dilated_conv2d_depthwise_nhwc_getsize
     }
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 static VOID xa_nn_conv2d_depthwise_nhwc_init
 (pVOID p_scratch
  ,WORD32 input_height
@@ -353,6 +356,7 @@ static VOID xa_nn_dilated_conv2d_depthwise_nhwc_init
     p_mem = (p_mem + circ_buf_size);
     p_state->p_scratch = (pVOID)p_mem;
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 
 static WORD32 xa_nn_dilated_conv2d_depthwise_getsize_generic
 (WORD32 input_height
@@ -492,6 +496,8 @@ WORD32 xa_nn_conv2d_depthwise_getsize
 
   return total_size;
 }
+
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 VOID xa_nn_conv2d_depthwise_init
 (pVOID p_scratch
  ,WORD32 input_height
@@ -632,6 +638,7 @@ VOID xa_nn_dilated_conv2d_depthwise_init
                 ,p_pad_val);
     }
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
 
 WORD32 xa_nn_dilated_conv2d_depthwise_getsize
 (WORD32 input_height
diff --git a/algo/kernels/cnn/hifi4/xa_nn_conv2d_std_sym8sxasym8s.c b/algo/kernels/cnn/hifi4/xa_nn_conv2d_std_sym8sxasym8s.c
index c4f5804..e78cd47 100644
--- a/algo/kernels/cnn/hifi4/xa_nn_conv2d_std_sym8sxasym8s.c
+++ b/algo/kernels/cnn/hifi4/xa_nn_conv2d_std_sym8sxasym8s.c
@@ -64,17 +64,27 @@ static WORD32 conv_x_left_pad(
 #else /* #if TFLITE_SINGLE_ROUNDING */
         left_shift  = p_out_shift[k] < 0 ? 0 : p_out_shift[k];
         right_shift = p_out_shift[k] > 0 ? 0 : -p_out_shift[k];
-#endif /* #if TFLITE_SINGLE_ROUNDING */          
+#endif /* #if TFLITE_SINGLE_ROUNDING */
+        ae_int32x2 acc;
 #if XCHAL_HAVE_HIFI1
-        ae_int32x2 acc = AE_L32_I((ae_int32*)&p_bias[k], 0);
+        if(p_bias != NULL){
+          acc = AE_L32_I((ae_int32*)&p_bias[k], 0);
+        }
+        else{
+          acc = AE_MOVDA32(0);
+        }
         MPY_BY_QUANT_MULT_SLS_X2_OUT32(acc, acc, p_out_multiplier[k], left_shift, right_shift);
         acc = AE_ADD32S(acc, AE_MOVDA32(out_zero_bias));
         acc = AE_MAX32(acc, min_int8);
         acc = AE_MIN32(acc, max_int8);
         AE_S8_0_X_HIFI1( AE_MOVINT16X4_FROMINT32X2(acc), (WORD8 *)p_out, (i * out_height_offset + j * out_width_offset + k * out_channels_offset));
 #else
-
-        ae_int32x2 acc = AE_MOVDA32(p_bias[k]);
+        if(p_bias != NULL){
+          acc = AE_MOVDA32(p_bias[k]);
+        }
+        else{
+          acc = AE_MOVDA32(0);
+        }
         MPY_BY_QUANT_MULT_SLS_X2_OUT32(acc, acc, p_out_multiplier[k], left_shift, right_shift);
         acc = AE_ADD32S(acc, AE_MOVDA32(out_zero_bias));
 #if 0
@@ -129,16 +139,27 @@ static WORD32 conv_x_right_pad(
 #else /* #if TFLITE_SINGLE_ROUNDING */
         left_shift  = p_out_shift[k] < 0 ? 0 : p_out_shift[k];
         right_shift = p_out_shift[k] > 0 ? 0 : -p_out_shift[k];
-#endif /* #if TFLITE_SINGLE_ROUNDING */          
+#endif /* #if TFLITE_SINGLE_ROUNDING */
+        ae_int32x2 acc;
 #if XCHAL_HAVE_HIFI1
-        ae_int32x2 acc = AE_L32_I((ae_int32*)&p_bias[k], 0);
+        if(p_bias != NULL){
+           acc = AE_L32_I((ae_int32*)&p_bias[k], 0);
+        }
+        else{
+          acc = AE_MOVDA32(0);
+        }
         MPY_BY_QUANT_MULT_SLS_X2_OUT32(acc, acc, p_out_multiplier[k], left_shift, right_shift);
         acc = AE_ADD32S(acc, AE_MOVDA32(out_zero_bias));
         acc = AE_MAX32(acc, min_int8);
         acc = AE_MIN32(acc, max_int8);
         AE_S8_0_X_HIFI1( AE_MOVINT16X4_FROMINT32X2(acc), (WORD8 *)p_out, (i * out_height_offset + j * out_width_offset + k * out_channels_offset));
 #else
-        ae_int32x2 acc = AE_MOVDA32(p_bias[k]);
+        if(p_bias != NULL){
+          acc = AE_MOVDA32(p_bias[k]);
+        }
+        else{
+          acc = AE_MOVDA32(0);
+        }
         MPY_BY_QUANT_MULT_SLS_X2_OUT32(acc, acc, p_out_multiplier[k], left_shift, right_shift);
         acc = AE_ADD32S(acc, AE_MOVDA32(out_zero_bias));
 #if 0
@@ -565,7 +586,6 @@ WORD32 xa_nn_conv2d_std_per_chan_sym8sxasym8s(
   XA_NNLIB_ARG_CHK_PTR(p_out, -1);
   XA_NNLIB_ARG_CHK_PTR(p_kernel, -1);
   XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
-  XA_NNLIB_ARG_CHK_PTR(p_bias, -1);
   XA_NNLIB_ARG_CHK_PTR(p_scratch, -1);
   /* Pointer alignment checks */
   //XA_NNLIB_ARG_CHK_ALIGN(p_out, sizeof(UWORD8), -1);
@@ -630,7 +650,7 @@ WORD32 xa_nn_conv2d_std_per_chan_sym8sxasym8s(
       ,inp_h
       ,input_channels
       ,ker_h
-      ,kernel_width
+      ,ker_w
       ,y_str
       ,y_pad
       ,out_h
diff --git a/algo/kernels/cnn/hifi4/xa_nn_conv2d_std_sym8sxsym16s.c b/algo/kernels/cnn/hifi4/xa_nn_conv2d_std_sym8sxsym16s.c
index a8d82fd..31bedf2 100644
--- a/algo/kernels/cnn/hifi4/xa_nn_conv2d_std_sym8sxsym16s.c
+++ b/algo/kernels/cnn/hifi4/xa_nn_conv2d_std_sym8sxsym16s.c
@@ -171,7 +171,12 @@ static WORD32 conv_x_left_pad(
       ae_int64 q1;
       for(k = 0; k < out_channels; k++)
       {
-	AE_L64_IP(q1, pbias, 8);
+        if(p_bias != NULL){
+          AE_L64_IP(q1, pbias, 8);
+        }
+        else{
+          q1 = 0;
+        }
 	ae_int32x2 acc = MultiplyByQuantizedMultiplier_ref(q1, p_out_multiplier[k], p_out_shift[k]);
 	d1 = AE_SAT16X4(acc, acc);
 	AE_S16_0_XP(d1, ptrout, out_channels_offset*sizeof(WORD16));
@@ -211,7 +216,12 @@ static WORD32 conv_x_right_pad(
       ae_int64 q1;
       for(k = 0; k < out_channels; k++)
       {
-	AE_L64_IP(q1, pbias, 8);
+        if(p_bias != NULL){
+          AE_L64_IP(q1, pbias, 8);
+        }
+        else{
+          q1 = 0;
+        }
 	ae_int32x2 acc = MultiplyByQuantizedMultiplier_ref(q1, p_out_multiplier[k], p_out_shift[k]);
 	d1 = AE_SAT16X4(acc, acc);
 	AE_S16_0_XP(d1, ptrout, out_channels_offset*sizeof(WORD16));
@@ -404,7 +414,6 @@ WORD32 xa_nn_conv2d_std_per_chan_sym8sxsym16s(
   XA_NNLIB_ARG_CHK_PTR(p_out, -1);
   XA_NNLIB_ARG_CHK_PTR(p_kernel, -1);
   XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
-  XA_NNLIB_ARG_CHK_PTR(p_bias, -1);
   XA_NNLIB_ARG_CHK_PTR(p_scratch, -1);
   /* Pointer alignment checks */
   XA_NNLIB_ARG_CHK_ALIGN(p_bias, sizeof(WORD64), -1);
@@ -427,7 +436,7 @@ WORD32 xa_nn_conv2d_std_per_chan_sym8sxsym16s(
     XA_NNLIB_ARG_CHK_COND((p_out_shift[itr] < -31 || p_out_shift[itr] > 31), -1);
   }
 
-  if ( !(x_padding) && !(input_channels & 0x3) && !(out_channels & 0x3) && !(out_width & 0x1) && (out_data_format == 0) && ((out_width-1)*x_stride <=(input_width-kernel_width) ) )
+  if ( !(x_padding) && !(input_channels & 0x3) && !(out_channels & 0x3) && !(out_width & 0x1) && (out_data_format == 0) && ((out_width-1)*x_stride <=(input_width-kernel_width) ) && p_bias)
   {
     int ret_val=0;
     ret_val=xa_nn_conv2d_std_per_chan_sym8sxsym16s_no_circ_buf(p_out,
@@ -497,7 +506,7 @@ WORD32 inp_h, inp_w, ker_h, ker_w, x_str, y_str, x_pad, y_pad, out_h, out_w;
       ,inp_h
       ,input_channels
       ,ker_h
-      ,kernel_width
+      ,ker_w
       ,y_str
       ,y_pad
       ,out_h
@@ -510,7 +519,7 @@ WORD32 inp_h, inp_w, ker_h, ker_w, x_str, y_str, x_pad, y_pad, out_h, out_w;
       ,inp_h
       ,input_channels
       ,ker_h
-      ,kernel_width
+      ,ker_w
       ,y_str
       ,y_pad
       ,out_h
diff --git a/algo/kernels/cnn/hifi4/xa_nn_matXvec_sym8sxasym8s_asym8s_circ.c b/algo/kernels/cnn/hifi4/xa_nn_matXvec_sym8sxasym8s_asym8s_circ.c
index 504e9da..88dfbca 100644
--- a/algo/kernels/cnn/hifi4/xa_nn_matXvec_sym8sxasym8s_asym8s_circ.c
+++ b/algo/kernels/cnn/hifi4/xa_nn_matXvec_sym8sxasym8s_asym8s_circ.c
@@ -634,13 +634,15 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
 
   int left_shift, right_shift;
   int m_itr, vec_itr;
-
+  ae_int32 zeros[4] = {0, 0, 0, 0};
   /* vec, mat and bias 4-byte aigned */
-  if(p_mat1 && p_vec1 && p_bias &&
-      (((unsigned int)p_mat1&0x3)==0) && (((unsigned int)p_vec1&0x3)==0) && (((unsigned int)p_bias&0x3) == 0) &&
+  if(p_mat1 && p_vec1 && (((unsigned int)p_mat1&0x3)==0) && (((unsigned int)p_vec1&0x3)==0) && (((unsigned int)p_bias&0x3) == 0) &&
       ((cols1&0x3)==0) && ((vec_stride&0x3)==0) && ((row_stride1&0x3)==0))
   {
     ae_int32 *bias_ptr = (ae_int32*)p_bias;
+    if(p_bias == NULL){
+      bias_ptr = zeros;
+    }
     for(vec_itr = 0; vec_itr < ((vec_count>>2)<<2); vec_itr+=4)
     {
       ae_int32x2 acc_row0_vec0, acc_row0_vec1, acc_row0_vec2, acc_row0_vec3;
@@ -813,7 +815,9 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
 #endif
       }
       /* dummy load, just to increment the pointer */
-      AE_L32_IP(acc_row0_vec0, bias_ptr, 16);
+      if(p_bias != NULL){
+        AE_L32_IP(acc_row0_vec0, bias_ptr, 16);
+      }
     }
 
     /* for vec_count=2 */
@@ -930,7 +934,9 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
       }
       /* dummy load, just to increment the pointer */
       vec_itr+=2;
-      AE_L32_IP(acc_row0_vec0, bias_ptr, 8);
+      if(p_bias != NULL){
+        AE_L32_IP(acc_row0_vec0, bias_ptr, 8);
+      }
     }
 
     /* for vec_count=1 */
@@ -1014,11 +1020,17 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
 #endif
       }
       /* dummy load, just to increment the pointer */
-      AE_L32_IP(acc_row0_vec0, bias_ptr, 4);
+      if(p_bias != NULL){
+        AE_L32_IP(acc_row0_vec0, bias_ptr, 4);
+      }
     }
   }
-  else if(p_mat1 && p_vec1 && p_bias)
+  else if(p_mat1 && p_vec1)
   {
+    ae_int32 *bias_ptr = (ae_int32*)p_bias;
+    if(p_bias == NULL){
+      bias_ptr = zeros;
+    }
 //#if !ENABLE_PADDING_CONV2D_STD
 #if HW_AE_ADDCIRC16X4_XC
     vec_itr = 0; 
@@ -1050,14 +1062,14 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
       for (; m_itr < (rows&~3); m_itr+=4)
       {
         int c_itr;
-        ae_int32x2 acc_row01_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
-        ae_int32x2 acc_row23_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
-        ae_int32x2 acc_row01_vec1 = AE_MOVDA32(p_bias[vec_itr + 1]);
-        ae_int32x2 acc_row23_vec1 = AE_MOVDA32(p_bias[vec_itr + 1]);
-        ae_int32x2 acc_row01_vec2 = AE_MOVDA32(p_bias[vec_itr + 2]);
-        ae_int32x2 acc_row23_vec2 = AE_MOVDA32(p_bias[vec_itr + 2]);
-        ae_int32x2 acc_row01_vec3 = AE_MOVDA32(p_bias[vec_itr + 3]);
-        ae_int32x2 acc_row23_vec3 = AE_MOVDA32(p_bias[vec_itr + 3]);
+        ae_int32x2 acc_row01_vec0 = AE_L32_I(bias_ptr, 0);
+        ae_int32x2 acc_row23_vec0 = AE_L32_I(bias_ptr, 0);
+        ae_int32x2 acc_row01_vec1 = AE_L32_I(bias_ptr, 4);
+        ae_int32x2 acc_row23_vec1 = AE_L32_I(bias_ptr, 4);
+        ae_int32x2 acc_row01_vec2 = AE_L32_I(bias_ptr, 8);
+        ae_int32x2 acc_row23_vec2 = AE_L32_I(bias_ptr, 8);
+        ae_int32x2 acc_row01_vec3 = AE_L32_I(bias_ptr, 12);
+        ae_int32x2 acc_row23_vec3 = AE_L32_I(bias_ptr, 12);
 
         WORD8* p_vec_0  = (WORD8*)(p_vec1 + (vec_itr+0) * vec_stride);
         WORD8* p_vec_1  = (WORD8*)(p_vec1 + (vec_itr+1) * vec_stride);
@@ -1179,10 +1191,10 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
       /* Remainder Loop */
       for (; m_itr < rows; m_itr++)
       {
-        ae_int32x2 acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
-        ae_int32x2 acc_row0_vec1 = AE_MOVDA32(p_bias[vec_itr + 1]);
-        ae_int32x2 acc_row0_vec2 = AE_MOVDA32(p_bias[vec_itr + 2]);
-        ae_int32x2 acc_row0_vec3 = AE_MOVDA32(p_bias[vec_itr + 3]);
+        ae_int32x2 acc_row0_vec0 = AE_L32_I(bias_ptr, 0);
+        ae_int32x2 acc_row0_vec1 = AE_L32_I(bias_ptr, 4);
+        ae_int32x2 acc_row0_vec2 = AE_L32_I(bias_ptr, 8);
+        ae_int32x2 acc_row0_vec3 = AE_L32_I(bias_ptr, 12);
 
         WORD8* p_vec_0  = (WORD8*)(p_vec1 + (vec_itr+0) * vec_stride);
         WORD8* p_vec_1  = (WORD8*)(p_vec1 + (vec_itr+1) * vec_stride);
@@ -1238,6 +1250,10 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
         AE_S8_FROM32_WITHSTRIDE(AE_MOVAD32_L(acc_row0_vec2), p_dst2, out_stride);
         AE_S8_FROM32_WITHSTRIDE(AE_MOVAD32_L(acc_row0_vec3), p_dst3, out_stride);
       }
+      if(p_bias != NULL){
+        ae_int32x2 dummy;
+        AE_L32_IP(dummy, bias_ptr, 16);
+      }
     }
     for(; vec_itr < vec_count; vec_itr++)
     {
@@ -1256,8 +1272,8 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
       for (m_itr = 0; m_itr < (rows&~3); m_itr+=4)
       {
         int c_itr;
-        ae_int32x2 acc_row01_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
-        ae_int32x2 acc_row23_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+        ae_int32x2 acc_row01_vec0 = AE_L32_I(bias_ptr, 0);
+        ae_int32x2 acc_row23_vec0 = AE_L32_I(bias_ptr, 0);
 
         WORD8* p_vec_0  = (WORD8*)(p_vec1 + vec_itr * vec_stride);
         WORD8 *p_mat1_0 = (WORD8*)p_mat1;
@@ -1311,7 +1327,7 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
 
       for (; m_itr < rows; m_itr++)
       {
-        ae_int32x2 acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+        ae_int32x2 acc_row0_vec0 = AE_L32_I(bias_ptr, 0);
 
         WORD8* p_vec_0  = (WORD8*)(p_vec1 + vec_itr * vec_stride);
         WORD8 *p_mat1_0 = (WORD8*)p_mat1;
@@ -1337,6 +1353,10 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
 
         AE_S8_FROM32_WITHSTRIDE(AE_MOVAD32_L(acc_row0_vec0), p_dst0, out_stride);
       }
+      if(p_bias != NULL){
+        ae_int32x2 dummy;
+        AE_L32_IP(dummy, bias_ptr, 4);
+      }
     }
 #else
 /* Under normal mode of operation, this code is not expected to be executed. It is added only as a compliance code in case the aligend code is not executed */
@@ -1360,7 +1380,7 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
       m_itr = 0;
       for (; m_itr < rows; m_itr++)
       {
-        ae_int32x2 acc_row0_vec0 = AE_MOVDA32(p_bias[vec_itr + 0]);
+        ae_int32x2 acc_row0_vec0 = AE_L32_I(bias_ptr, 0);
 
         WORD8* p_vec_0  = (WORD8*)(p_vec1 + vec_itr * vec_stride);
         WORD8 *p_mat1_0 = (WORD8*)p_mat1;
@@ -1391,6 +1411,10 @@ WORD32 xa_nn_matXvec_sym8sxasym8s_asym8s_circ(
 
         AE_S8_FROM32_WITHSTRIDE(AE_MOVAD32_L(acc_row0_vec0), p_dst0, out_stride);
       }
+      if(p_bias != NULL){
+        ae_int32x2 dummy;
+        AE_L32_IP(dummy, bias_ptr, 4);
+      }
     }
 #endif /* ENABLE_PADDING_CONV2D_STD */
   }
diff --git a/algo/kernels/cnn/hifi4/xa_nn_matXvec_sym8sxsym16s_sym16s_circ.c b/algo/kernels/cnn/hifi4/xa_nn_matXvec_sym8sxsym16s_sym16s_circ.c
index d876754..4fd8aee 100644
--- a/algo/kernels/cnn/hifi4/xa_nn_matXvec_sym8sxsym16s_sym16s_circ.c
+++ b/algo/kernels/cnn/hifi4/xa_nn_matXvec_sym8sxsym16s_sym16s_circ.c
@@ -1234,11 +1234,14 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
   int m_itr, vec_itr;
   row_stride1 = row_stride1<<1;
 
- if(p_mat1 && p_vec1 && p_bias &&
-      (((unsigned int)p_mat1&0x7)==0) && (((unsigned int)p_vec1&0x3)==0) && (((unsigned int)p_bias&0x7) == 0) &&
+  ae_int64 zeros[4] = {0, 0, 0, 0};
+  if(p_mat1 && p_vec1 && (((unsigned int)p_mat1&0x7)==0) && (((unsigned int)p_vec1&0x3)==0) && (((unsigned int)p_bias&0x7) == 0) &&
       ((cols1&0x3)==0) && ((vec_stride&0x3)==0) && ((row_stride1&0x7)==0))
   {
     ae_int64 *pbias = (ae_int64*)p_bias;
+    if(p_bias == NULL){
+      pbias = zeros;
+    }
     for(vec_itr = 0; vec_itr<(vec_count&(~0x3)); vec_itr+=4)
     {
       ae_int32x2 acc_row0_vec0, acc_row0_vec1, acc_row0_vec2, acc_row0_vec3;
@@ -1302,6 +1305,7 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
         acc3 = AE_L64_I(pbias, 8);
         acc5 = AE_L64_I(pbias, 16);
         acc7 = AE_L64_I(pbias, 24);
+
         WORD16 *p_mat1_0 = (WORD16*)p_mat1;
         AE_ADDCIRC16X4_XC((ae_int16x4*)p_mat1_0, (m_itr+0) * row_stride1);
         _xa_nn_dot_product_1row_4vec_mat_vecs_4bytes_aligned
@@ -1325,7 +1329,9 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
 	AE_S16_0_XP(                   d1, (ae_int16*)p_dst3, out_stride);
       }
       /* dummy load, just to increment the pointer */
-      AE_L64_IP(acc1, pbias, 32);
+      if(p_bias != NULL){
+        AE_L64_IP(acc1, pbias, 32);
+      }
     }
     if(vec_count&0x2)
     {
@@ -1384,7 +1390,9 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
 	AE_S16_0_XP(	               d1, (ae_int16*)p_dst1, out_stride);
       }
       /* dummy load, just to increment the pointer */
-      AE_L64_IP(acc1, pbias, 16);
+      if(p_bias != NULL){
+        AE_L64_IP(acc1, pbias, 16);
+      }
       vec_itr += 2;
     }
     if(vec_count&0x1)
@@ -1396,7 +1404,12 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
       ae_int16x4 d1;
       for (m_itr=0; m_itr < (rows&(~0x1)); m_itr+=2)
       {
-        acc1 = acc2 = AE_L64_I(pbias, 0);
+        if(p_bias != NULL){
+          acc1 = acc2 = AE_L64_I(pbias, 0);
+        }
+        else{
+          acc1 = acc2 = 0;
+        }
         WORD16 *p_mat1_0 = (WORD16*)p_mat1;
         WORD16 *p_mat1_1 = (WORD16*)p_mat1;
         AE_ADDCIRC16X4_XC((ae_int16x4*)p_mat1_0, (m_itr+0) * row_stride1);
@@ -1430,14 +1443,18 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
 	AE_S16_0_XP(d1, (ae_int16*)p_dst0, out_stride);
       }
       /* dummy load, just to increment the pointer */
-      AE_L64_IP(acc1, pbias, 8);
+      if(p_bias != NULL){
+        AE_L64_IP(acc1, pbias, 8);
+      }
     }
   }
-  else if(p_mat1 && p_vec1 && p_bias &&
-      (((unsigned int)p_vec1&0x3)==0) && (((unsigned int)p_bias&0x7) == 0) &&
+  else if(p_mat1 && p_vec1 && (((unsigned int)p_vec1&0x3)==0) && (((unsigned int)p_bias&0x7) == 0) &&
       ((cols1&0x3)==0) && ((vec_stride&0x3)==0))
   {
     ae_int64 *pbias = (ae_int64*)p_bias;
+    if(p_bias == NULL){
+      pbias = zeros;
+    }
     for(vec_itr = 0; vec_itr<(vec_count&(~0x3)); vec_itr+=4)
     {
       ae_int32x2 acc_row0_vec0, acc_row0_vec1, acc_row0_vec2, acc_row0_vec3;
@@ -1524,7 +1541,9 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
 	AE_S16_0_XP(                   d1, (ae_int16*)p_dst3, out_stride);
       }
       /* dummy load, just to increment the pointer */
-      AE_L64_IP(acc1, pbias, 32);
+      if(p_bias != NULL){
+        AE_L64_IP(acc1, pbias, 32);
+      }
     }
     if(vec_count&0x2)
     {
@@ -1583,7 +1602,9 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
 	AE_S16_0_XP(	               d1, (ae_int16*)p_dst1, out_stride);
       }
       /* dummy load, just to increment the pointer */
-      AE_L64_IP(acc1, pbias, 16);
+      if(p_bias != NULL){
+        AE_L64_IP(acc1, pbias, 16);
+      }
       vec_itr += 2;
     }
     if(vec_count&0x1)
@@ -1629,7 +1650,9 @@ WORD32 xa_nn_matXvec_sym8sxsym16s_sym16s_circ(
 	AE_S16_0_XP(d1, (ae_int16*)p_dst0, out_stride);
       }
       /* dummy load, just to increment the pointer */
-      AE_L64_IP(acc1, pbias, 8);
+      if(p_bias != NULL){
+        AE_L64_IP(acc1, pbias, 8);
+      }
     }
   }
   else
diff --git a/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_circ_buf.c b/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_circ_buf.c
index 4a2f875..36903ee 100644
--- a/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_circ_buf.c
+++ b/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_circ_buf.c
@@ -117,6 +117,7 @@ WORD32 xa_nn_transpose_conv_getsize
     return total_size;
 }
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 VOID xa_nn_transpose_conv_init_state(
     VOID *p_scratch,
     VOID *p_kernel,
@@ -182,3 +183,5 @@ VOID xa_nn_transpose_conv_init_state(
   AE_SETCBEGIN0(p_state->cir_buf.p_begin);
   AE_SETCEND0(p_state->cir_buf.p_end);
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
+
diff --git a/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_f32.c b/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_f32.c
index c8f3998..31de98d 100644
--- a/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_f32.c
+++ b/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_f32.c
@@ -619,7 +619,7 @@ DISCARD_FUN_FOR_NONVOID_RETURN(WORD32, xa_nn_transpose_conv_f32, (FLOAT32* outpu
             int filter_height, int filter_width,
             int output_height, int output_width,
             int num_elements,
-            FLOAT32* scratch_buffer))
+            void* scratch_buffer))
 #else
 WORD32 xa_nn_transpose_conv_f32(FLOAT32* output_data,
 		const FLOAT32* input_data,
@@ -632,7 +632,7 @@ WORD32 xa_nn_transpose_conv_f32(FLOAT32* output_data,
 		int filter_height, int filter_width,
 		int output_height, int output_width,
 		int num_elements,
-		FLOAT32* scratch_buffer)
+		void* scratch_buffer)
 {
 	/* NULL pointer checks */
 	XA_NNLIB_ARG_CHK_PTR(output_data, -1);
diff --git a/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_sym8sxasym8s.c b/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_sym8sxasym8s.c
index 1fcf93f..a50a8e9 100644
--- a/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_sym8sxasym8s.c
+++ b/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_sym8sxasym8s.c
@@ -641,7 +641,7 @@ int xa_nn_transpose_conv_sym8sxasym8s(WORD8* output_data,
     int num_elements,
     int input_offset, int output_offset,
     int *output_shift, int *output_multiplier,
-    int32_t* scratch_buffer)
+    void* scratch_buffer)
 {
   /* NULL pointer checks */
   XA_NNLIB_ARG_CHK_PTR(output_data, -1);
diff --git a/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_sym8sxsym16s.c b/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_sym8sxsym16s.c
index 026166b..438f597 100644
--- a/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_sym8sxsym16s.c
+++ b/algo/kernels/cnn/hifi4/xa_nn_transpose_conv_sym8sxsym16s.c
@@ -630,18 +630,18 @@ static inline void transpose_conv2d_std_sym8sxsym16s(WORD16* output_data,
 }
 
 int xa_nn_transpose_conv_sym8sxsym16s(WORD16* output_data,
-		const WORD16* input_data,
-		const WORD8* filter_data,
-		const WORD64* bias_data,
-		int stride_width, int stride_height,
-		int pad_width, int pad_height,
-		int input_depth, int output_depth,
-		int input_height, int input_width,
-		int filter_height, int filter_width,
-		int output_height, int output_width,
-		int num_elements,
-		int *output_shift, int *output_multiplier,
-		int64_t* scratch_buffer)
+    const WORD16* input_data,
+    const WORD8* filter_data,
+    const WORD64* bias_data,
+    int stride_width, int stride_height,
+    int pad_width, int pad_height,
+    int input_depth, int output_depth,
+    int input_height, int input_width,
+    int filter_height, int filter_width,
+    int output_height, int output_width,
+    int num_elements,
+    int *output_shift, int *output_multiplier,
+    void* scratch_buffer)
 {
 	/* NULL pointer checks */
 	XA_NNLIB_ARG_CHK_PTR(output_data, -1);
diff --git a/algo/kernels/pool/hifi4/xa_nn_avgpool.c b/algo/kernels/pool/hifi4/xa_nn_avgpool.c
index d7481e8..351965f 100644
--- a/algo/kernels/pool/hifi4/xa_nn_avgpool.c
+++ b/algo/kernels/pool/hifi4/xa_nn_avgpool.c
@@ -24,8 +24,9 @@
 #include "xa_nnlib_kernels_api.h"
 #include "xa_nn_avgpool_state.h"
 #include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common_macros.h"
 
-WORD32 xa_nn_avgpool_getsize_nchw(
+static WORD32 xa_nn_avgpool_getsize_nchw(
     WORD32 inp_precision,
     WORD32 input_width,
     WORD32 kernel_height,
@@ -86,7 +87,7 @@ WORD32 xa_nn_avgpool_getsize_nchw(
         den_array_size = 0;
     /* Output scratch buffer size */
     full_buf_width = kernel_width + (out_width - 1)*x_stride;
-    full_buf_width = XT_MAX(full_buf_width, ALIGNED_SIZE(x_padding, 2)+input_width);
+    full_buf_width = MAX(full_buf_width, ALIGNED_SIZE(x_padding, 2)+input_width);
     full_buf_width = ALIGNED_SIZE(full_buf_width, ALIGNMENT/2);
     /* Need 2 rows of padded input width as acratch for temp output */
     full_out_width = ALIGNED_SIZE(full_buf_width + kernel_width, 4);
@@ -97,7 +98,7 @@ WORD32 xa_nn_avgpool_getsize_nchw(
     return total_size;
 }
 
-WORD32 xa_nn_avgpool_getsize_nhwc(
+static WORD32 xa_nn_avgpool_getsize_nhwc(
     WORD32 inp_precision,
     WORD32 input_channels,
     WORD32 input_width,
@@ -150,7 +151,7 @@ WORD32 xa_nn_avgpool_getsize_nhwc(
 
         if(kernel_height <= (int)MAX_HEIGHT_16_BIT_ACC) // Accumulation in 16 bit container
         {
-            zero_mem_bytes = XT_MAX(sizeof(UWORD8)*cw_plane_size, sizeof(WORD16)*input_channels);
+            zero_mem_bytes = MAX((int)(sizeof(UWORD8)*cw_plane_size), (int)(sizeof(WORD16)*input_channels));
 
             total_size = ALIGNED_SIZE(sizeof(WORD32)* out_height, ALIGNMENT) +
                          ALIGNED_SIZE(sizeof(WORD32)* out_width, ALIGNMENT) +
@@ -162,7 +163,7 @@ WORD32 xa_nn_avgpool_getsize_nhwc(
         }
         else  // Accumulation in 32 bit container
         {
-            zero_mem_bytes = XT_MAX(sizeof(UWORD8)*cw_plane_size, sizeof(WORD32)*input_channels);
+            zero_mem_bytes = MAX((int)(sizeof(UWORD8)*cw_plane_size), (int)(sizeof(WORD32)*input_channels));
 
             total_size = ALIGNED_SIZE(sizeof(WORD32)*out_height, ALIGNMENT) +
                          ALIGNED_SIZE(sizeof(WORD32)*out_width, ALIGNMENT) +
@@ -179,7 +180,7 @@ WORD32 xa_nn_avgpool_getsize_nhwc(
         int zero_mem_bytes;
 
         cw_plane_size = input_width*input_channels;
-        zero_mem_bytes = XT_MAX(sizeof(WORD16)*cw_plane_size, sizeof(WORD32)*input_channels);
+        zero_mem_bytes = MAX((int)(sizeof(WORD16)*cw_plane_size), (int)(sizeof(WORD32)*input_channels));
 
         total_size = ALIGNED_SIZE(sizeof(WORD32)*out_height, ALIGNMENT) +
             ALIGNED_SIZE(sizeof(WORD32)*out_width, ALIGNMENT) +
@@ -259,7 +260,7 @@ WORD32 xa_nn_avgpool_getsize(
         den_array_size = 0;
     /* Output scratch buffer size */
     full_buf_width = kernel_width + (out_width - 1)*x_stride;
-    full_buf_width = XT_MAX(full_buf_width, ALIGNED_SIZE(x_padding, 2)+input_width);
+    full_buf_width = MAX(full_buf_width, ALIGNED_SIZE(x_padding, 2)+input_width);
     full_buf_width = ALIGNED_SIZE(full_buf_width, ALIGNMENT/2);
     /* Need 2 rows of padded input width as acratch for temp output */
     full_out_width = ALIGNED_SIZE(full_buf_width + kernel_width, 4);
diff --git a/algo/kernels/pool/hifi4/xa_nn_maxpool.c b/algo/kernels/pool/hifi4/xa_nn_maxpool.c
index 5ffd08a..b765988 100644
--- a/algo/kernels/pool/hifi4/xa_nn_maxpool.c
+++ b/algo/kernels/pool/hifi4/xa_nn_maxpool.c
@@ -24,8 +24,9 @@
 #include "xa_nnlib_kernels_api.h"
 #include "xa_nn_maxpool_state.h"
 #include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common_macros.h"
 
-WORD32 xa_nn_maxpool_getsize_nchw(
+static WORD32 xa_nn_maxpool_getsize_nchw(
     WORD32 inp_precision,
     WORD32 input_width,
     WORD32 kernel_height,
@@ -76,7 +77,7 @@ WORD32 xa_nn_maxpool_getsize_nchw(
     state_size = ALIGNED_SIZE(sizeof(xa_nn_maxpool_state_t), ALIGNMENT);
     /* Output scratch buffer size */
     full_buf_width = kernel_width + (out_width - 1)*x_stride;
-    full_buf_width = XT_MAX(full_buf_width, x_padding + input_width);
+    full_buf_width = MAX(full_buf_width, x_padding + input_width);
     full_buf_width = ALIGNED_SIZE(full_buf_width, ALIGNMENT/2);
     /* maxpool: Need 2 rows of padded input width as acratch for temp output */
     full_out_width = ALIGNED_SIZE(full_buf_width + kernel_width, 4);
@@ -87,15 +88,15 @@ WORD32 xa_nn_maxpool_getsize_nchw(
     return total_size;
 }
 
-WORD32 xa_nn_maxpool_getsize_nhwc(WORD32  inp_precision,
-                                  WORD32  input_width,
-                                  WORD32  input_channels,
-                                  WORD32 kernel_height,
-                                  WORD32 kernel_width,
-                                  WORD32 x_stride,
-                                  WORD32 y_stride,
-                                  WORD32 x_padding,
-                                  WORD32 out_width)
+static WORD32 xa_nn_maxpool_getsize_nhwc(WORD32  inp_precision,
+                                         WORD32  input_width,
+                                         WORD32  input_channels,
+                                         WORD32 kernel_height,
+                                         WORD32 kernel_width,
+                                         WORD32 x_stride,
+                                         WORD32 y_stride,
+                                         WORD32 x_padding,
+                                         WORD32 out_width)
 {
     int scratch_bytewidth, scratch_size;
 
@@ -264,6 +265,7 @@ WORD32 xa_nn_maxpool_getsize(
 }
 #endif
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 WORD32 xa_nn_maxpool_init(
     WORD32 inp_precision,
     pVOID  p_scratch)
@@ -300,3 +302,4 @@ WORD32 xa_nn_maxpool_init(
     p_state->p_scratch = (pVOID)p_mem;
     return 0;
 }
+#endif /* #ifndef ENABLE_SCRATCH_SIZE_API_ONLY */
diff --git a/algo/kernels/reorg/hifi4/xa_nn_concat_8.c b/algo/kernels/reorg/hifi4/xa_nn_concat_8.c
new file mode 100644
index 0000000..a4c5cf6
--- /dev/null
+++ b/algo/kernels/reorg/hifi4/xa_nn_concat_8.c
@@ -0,0 +1,193 @@
+/*******************************************************************************
+* Copyright (c) 2018-2024 Cadence Design Systems, Inc.
+*
+* Permission is hereby granted, free of charge, to any person obtaining
+* a copy of this software and associated documentation files (the
+* "Software"), to use this Software with Cadence processor cores only and
+* not with any other processors and platforms, subject to
+* the following conditions:
+*
+* The above copyright notice and this permission notice shall be included
+* in all copies or substantial portions of the Software.
+*
+* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+* IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+* SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+******************************************************************************/
+#include "xa_type_def.h"
+#include "xa_nn_common.h"
+#include "xa_nnlib_kernels_api.h"
+#include "xa_nnlib_common_macros.h"
+#include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common.h"
+
+WORD32 xa_nn_concat_8_8(WORD8 * __restrict__ p_out
+                        ,const WORD32 *const p_out_shape
+                        ,const WORD8 **pp_inps
+                        ,const WORD32 *const *pp_inps_shape
+                        ,WORD32 num_out_dims
+                        ,WORD32 num_inp
+                        ,WORD32 num_inp_dims
+                        ,WORD32 axis)
+{
+  XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_out_shape, -1);
+  XA_NNLIB_ARG_CHK_PTR(pp_inps, -1);
+  XA_NNLIB_ARG_CHK_PTR(pp_inps_shape, -1);
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_out_shape, sizeof(WORD32), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(pp_inps, sizeof(WORD8 *), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(pp_inps_shape, sizeof(WORD32 *), -1);
+  //Validate Arguments
+  XA_NNLIB_ARG_CHK_COND((num_out_dims <= 0 || num_out_dims > 6), -1);
+  XA_NNLIB_ARG_CHK_COND((num_inp <= 0 || num_inp > 10), -1);
+  XA_NNLIB_ARG_CHK_COND((num_inp_dims != num_out_dims), -1);
+  XA_NNLIB_ARG_CHK_COND((axis < -num_out_dims || axis >= num_out_dims), -1);
+
+  int i = 0, j = 0;
+  for(i = 0; i < num_out_dims; i++)
+  {
+    XA_NNLIB_ARG_CHK_COND((p_out_shape[i] <= 0), -1);
+  }
+
+  if(axis < 0)
+    axis = num_out_dims + axis;
+
+  WORD32 concat_size = 0;
+  for (i = 0; i < num_inp; i++)
+  {
+    XA_NNLIB_ARG_CHK_PTR(pp_inps[i], -1);
+    XA_NNLIB_ARG_CHK_PTR(pp_inps_shape[i], -1);
+    XA_NNLIB_ARG_CHK_ALIGN(pp_inps_shape[i], sizeof(WORD32), -1);
+#pragma loop_count min=1
+    for(j = 0; j < num_out_dims; j++)
+    {
+      XA_NNLIB_ARG_CHK_COND((pp_inps_shape[i][j] != p_out_shape[j] && j != axis), -1);
+    }
+    XA_NNLIB_ARG_CHK_COND((pp_inps_shape[i][axis] <= 0), -1);
+    concat_size += pp_inps_shape[i][axis];
+  }
+
+  XA_NNLIB_ARG_CHK_COND((p_out_shape[axis] != concat_size), -1);
+
+  //Calculate outer and inner size for axis
+  WORD32 outer_size = 1;
+#pragma no_simd
+  for(int i = 0; i < axis; i++)
+  {
+    outer_size *= p_out_shape[i];
+  }
+
+  WORD32 base_inner_size = 1;
+#pragma no_simd
+  for(int i = axis + 1; i < num_out_dims; i++)
+  {
+    base_inner_size *= p_out_shape[i];
+  }
+
+  WORD8 *ptmp_out = p_out;
+  for(int i = 0; i < num_inp; i++)
+  {
+    const WORD32 copy_size = pp_inps_shape[i][axis] * base_inner_size;
+    WORD8 *output_ptr = ptmp_out;
+    const WORD8* input_ptr = pp_inps[i];
+
+    if(((copy_size & 1) == 0) && (((concat_size * base_inner_size) & 1) == 0)
+      && (((unsigned)input_ptr & 1) == 0) && (((unsigned)output_ptr & 1) == 0))
+    {
+      if(copy_size <= 8)
+      {
+        const ae_int16 *pae_inp = (const ae_int16 *)input_ptr;
+        for(int k = 0; k < outer_size; k++)
+        {
+          ae_int16 *pae_out = (ae_int16 *)output_ptr;
+#pragma concurrent
+#pragma no_simd
+          for(int ic = 0; ic < (copy_size >> 1); ic++)
+          {
+            *pae_out++ = *pae_inp++;
+          }
+          output_ptr += concat_size * base_inner_size;
+        }
+      }
+      else
+      {
+        for(int k = 0; k < outer_size; k++)
+        {
+          const ae_int16x4 *pae_inp = (const ae_int16x4 *)input_ptr;
+          ae_int16x4 *pae_out = (ae_int16x4 *)output_ptr;
+          ae_valign inp_a, out_a;
+          inp_a = AE_LA64_PP(pae_inp);
+          out_a = AE_ZALIGN64();
+          for(int ic = 0; ic < (copy_size >> 3); ic++)
+          {
+            ae_int16x4 d0;
+            AE_LA16X4_IP(d0, inp_a, pae_inp);
+            AE_SA16X4_IP(d0, out_a, pae_out);
+          }
+          AE_SA64POS_FP(out_a, pae_out);
+          const ae_int16 *puae_inp = (const ae_int16 *)pae_inp;
+          ae_int16 *puae_out = (ae_int16 *)pae_out;
+#pragma concurrent
+          for(int ic = 0; ic < ((copy_size >> 1) & 3); ic++)
+          {
+            puae_out[ic] = puae_inp[ic];
+          }
+          input_ptr += copy_size;
+          output_ptr += concat_size * base_inner_size;
+        }
+      }
+    }
+    else
+    {
+      if(copy_size <= 6)
+      {
+        for(int k = 0; k < outer_size; k++)
+        {
+#pragma concurrent
+#pragma no_unroll
+          for(int ic = 0; ic < copy_size; ic++)
+          {
+            output_ptr[ic] = *input_ptr++;
+          }
+          output_ptr += concat_size * base_inner_size;
+        }
+      }
+      else
+      {
+        for(int k = 0; k < outer_size; k++)
+        {
+          const ae_int24x2 *pae_inp = (const ae_int24x2 *)input_ptr;
+          ae_int24x2 *pae_out = (ae_int24x2 *)output_ptr;
+          ae_valign inp_a, out_a;
+          inp_a = AE_LA64_PP(pae_inp);
+          out_a = AE_ZALIGN64();
+
+          int copy_size_by6 = AE_MOVAD32_H(AE_MOVINT32X2_FROMINT64(AE_MUL32_LL(copy_size, 0x2AAAAAAB)));
+          int copy_size_rem_start = 6*copy_size_by6;
+#pragma concurrent
+          for(int ic = 0; ic < copy_size_by6; ic++)
+          {
+            ae_int24x2 d0;
+            AE_LA24X2_IP(d0, inp_a, pae_inp);
+            AE_SA24X2_IP(d0, out_a, pae_out);
+          }
+          AE_SA64POS_FP(out_a, pae_out);
+          for(int ic = copy_size_rem_start; ic < copy_size; ic++)
+          {
+            output_ptr[ic] = input_ptr[ic];
+          }
+          input_ptr += copy_size;
+          output_ptr += concat_size * base_inner_size;
+        }
+      }
+    }
+    ptmp_out += copy_size;
+  }
+  return 0;
+}
diff --git a/algo/kernels/reorg/hifi4/xa_nn_split_v_8.c b/algo/kernels/reorg/hifi4/xa_nn_split_v_8.c
new file mode 100644
index 0000000..ee9b16b
--- /dev/null
+++ b/algo/kernels/reorg/hifi4/xa_nn_split_v_8.c
@@ -0,0 +1,193 @@
+/*******************************************************************************
+* Copyright (c) 2018-2024 Cadence Design Systems, Inc.
+*
+* Permission is hereby granted, free of charge, to any person obtaining
+* a copy of this software and associated documentation files (the
+* "Software"), to use this Software with Cadence processor cores only and
+* not with any other processors and platforms, subject to
+* the following conditions:
+*
+* The above copyright notice and this permission notice shall be included
+* in all copies or substantial portions of the Software.
+*
+* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+* IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+* SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+******************************************************************************/
+#include "xa_type_def.h"
+#include "xa_nn_common.h"
+#include "xa_nnlib_kernels_api.h"
+#include "xa_nnlib_common_macros.h"
+#include "xa_nnlib_err_chk.h"
+#include "xa_nnlib_common.h"
+
+WORD32 xa_nn_split_v_8_8(WORD8 ** __restrict__ pp_outs
+                         ,const WORD32 *const *pp_outs_shape
+                         ,const WORD8 * __restrict__ p_inp
+                         ,const WORD32 *const p_inp_shape
+                         ,WORD32 num_out
+                         ,WORD32 num_out_dims
+                         ,WORD32 num_inp_dims
+                         ,WORD32 axis)
+{
+  XA_NNLIB_ARG_CHK_PTR(pp_outs, -1);
+  XA_NNLIB_ARG_CHK_PTR(pp_outs_shape, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp_shape, -1);
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_inp_shape, sizeof(WORD32), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(pp_outs, sizeof(WORD8 *), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(pp_outs_shape, sizeof(WORD32 *), -1);
+  //Validate Arguments
+  XA_NNLIB_ARG_CHK_COND((num_out <= 0 || num_out > 10), -1);
+  XA_NNLIB_ARG_CHK_COND((num_out_dims <= 0 || num_out_dims > 6), -1);
+  XA_NNLIB_ARG_CHK_COND((num_inp_dims != num_out_dims), -1);
+  XA_NNLIB_ARG_CHK_COND((axis < -num_inp_dims || axis >= num_inp_dims), -1);
+
+  int i = 0, j = 0;
+  for(i = 0; i < num_inp_dims; i++)
+  {
+    XA_NNLIB_ARG_CHK_COND((p_inp_shape[i] <= 0), -1);
+  }
+
+  if(axis < 0)
+    axis = num_inp_dims + axis;
+
+  WORD32 concat_size = 0;
+  for (i = 0; i < num_out; i++)
+  {
+    XA_NNLIB_ARG_CHK_PTR(pp_outs[i], -1);
+    XA_NNLIB_ARG_CHK_PTR(pp_outs_shape[i], -1);
+    XA_NNLIB_ARG_CHK_ALIGN(pp_outs_shape[i], sizeof(WORD32), -1);
+#pragma loop_count min=1
+    for(j = 0; j < num_inp_dims; j++)
+    {
+      XA_NNLIB_ARG_CHK_COND((pp_outs_shape[i][j] != p_inp_shape[j] && j != axis), -1);
+    }
+    XA_NNLIB_ARG_CHK_COND((pp_outs_shape[i][axis] <= 0), -1);
+    concat_size += pp_outs_shape[i][axis];
+  }
+
+  XA_NNLIB_ARG_CHK_COND((p_inp_shape[axis] != concat_size), -1);
+
+  //Calculate outer and inner size for axis
+  WORD32 outer_size = 1;
+#pragma no_simd
+  for(int i = 0; i < axis; i++)
+  {
+    outer_size *= p_inp_shape[i];
+  }
+
+  WORD32 base_inner_size = 1;
+#pragma no_simd
+  for(int i = axis + 1; i < num_inp_dims; i++)
+  {
+    base_inner_size *= p_inp_shape[i];
+  }
+
+  const WORD8 *ptmp_inp = p_inp;
+  for(int i = 0; i < num_out; i++)
+  {
+    const WORD32 copy_size = pp_outs_shape[i][axis] * base_inner_size;
+    const WORD8 *input_ptr = ptmp_inp;
+    WORD8* output_ptr = pp_outs[i];
+
+    if(((copy_size & 1) == 0) && (((concat_size * base_inner_size) & 1) == 0)
+      && (((unsigned)input_ptr & 1) == 0) && (((unsigned)output_ptr & 1) == 0))
+    {
+      if(copy_size <= 8)
+      {
+        ae_int16 *pae_out = (ae_int16 *)output_ptr;
+        for(int k = 0; k < outer_size; k++)
+        {
+          const ae_int16 *pae_inp = (const ae_int16 *)input_ptr;
+#pragma concurrent
+#pragma no_simd
+          for(int ic = 0; ic < (copy_size >> 1); ic++)
+          {
+            *pae_out++ = *pae_inp++;
+          }
+          input_ptr += concat_size * base_inner_size;
+        }
+      }
+      else
+      {
+        for(int k = 0; k < outer_size; k++)
+        {
+          const ae_int16x4 *pae_inp = (const ae_int16x4 *)input_ptr;
+          ae_int16x4 *pae_out = (ae_int16x4 *)output_ptr;
+          ae_valign inp_a, out_a;
+          inp_a = AE_LA64_PP(pae_inp);
+          out_a = AE_ZALIGN64();
+          for(int ic = 0; ic < (copy_size >> 3); ic++)
+          {
+            ae_int16x4 d0;
+            AE_LA16X4_IP(d0, inp_a, pae_inp);
+            AE_SA16X4_IP(d0, out_a, pae_out);
+          }
+          AE_SA64POS_FP(out_a, pae_out);
+          const ae_int16 *puae_inp = (const ae_int16 *)pae_inp;
+          ae_int16 *puae_out = (ae_int16 *)pae_out;
+#pragma concurrent
+          for(int ic = 0; ic < ((copy_size >> 1) & 3); ic++)
+          {
+            puae_out[ic] = puae_inp[ic];
+          }
+          output_ptr += copy_size;
+          input_ptr += concat_size * base_inner_size;
+        }
+      }
+    }
+    else
+    {
+      if(copy_size <= 6)
+      {
+        for(int k = 0; k < outer_size; k++)
+        {
+#pragma concurrent
+#pragma no_unroll
+          for(int ic = 0; ic < copy_size; ic++)
+          {
+            *output_ptr++ = input_ptr[ic];
+          }
+          input_ptr += concat_size * base_inner_size;
+        }
+      }
+      else
+      {
+        for(int k = 0; k < outer_size; k++)
+        {
+          const ae_int24x2 *pae_inp = (const ae_int24x2 *)input_ptr;
+          ae_int24x2 *pae_out = (ae_int24x2 *)output_ptr;
+          ae_valign inp_a, out_a;
+          inp_a = AE_LA64_PP(pae_inp);
+          out_a = AE_ZALIGN64();
+
+          int copy_size_by6 = AE_MOVAD32_H(AE_MOVINT32X2_FROMINT64(AE_MUL32_LL(copy_size, 0x2AAAAAAB)));
+          int copy_size_rem_start = 6*copy_size_by6;
+#pragma concurrent
+          for(int ic = 0; ic < copy_size_by6; ic++)
+          {
+            ae_int24x2 d0;
+            AE_LA24X2_IP(d0, inp_a, pae_inp);
+            AE_SA24X2_IP(d0, out_a, pae_out);
+          }
+          AE_SA64POS_FP(out_a, pae_out);
+          for(int ic = copy_size_rem_start; ic < copy_size; ic++)
+          {
+            output_ptr[ic] = input_ptr[ic];
+          }
+          output_ptr += copy_size;
+          input_ptr += concat_size * base_inner_size;
+        }
+      }
+    }
+    ptmp_inp += copy_size;
+  }
+  return 0;
+}
diff --git a/algo/kernels/reorg/hifi4/xa_nn_transpose_16.c b/algo/kernels/reorg/hifi4/xa_nn_transpose_16.c
new file mode 100644
index 0000000..ff24b43
--- /dev/null
+++ b/algo/kernels/reorg/hifi4/xa_nn_transpose_16.c
@@ -0,0 +1,267 @@
+/*******************************************************************************
+* Copyright (c) 2018-2024 Cadence Design Systems, Inc.
+*
+* Permission is hereby granted, free of charge, to any person obtaining
+* a copy of this software and associated documentation files (the
+* "Software"), to use this Software with Cadence processor cores only and
+* not with any other processors and platforms, subject to
+* the following conditions:
+*
+* The above copyright notice and this permission notice shall be included
+* in all copies or substantial portions of the Software.
+*
+* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+* IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+* CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+* TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+* SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+******************************************************************************/
+#include "xa_nnlib_common.h"
+
+/*
+ * Currently only supports upto 5D input tensors.
+ * 1/2/3/4 D input tensors will be scaled up to 5D.
+ * For example, 2x3 -> 1x1x1x2x3.
+ */
+
+WORD32 xa_nn_transpose_16_16(WORD16 * __restrict__ p_out
+                    ,const WORD32 *const p_out_shape
+                    ,const WORD16 * __restrict__ p_inp
+                    ,const WORD32 *const p_inp_shape
+                    ,const WORD32 * __restrict__ p_permute_vec
+                    ,WORD32 num_out_dims
+                    ,WORD32 num_inp_dims)
+{
+  /* NULL pointer checks */
+  XA_NNLIB_ARG_CHK_PTR(p_out, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_permute_vec, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_out_shape, -1);
+  XA_NNLIB_ARG_CHK_PTR(p_inp_shape, -1);
+
+  /* Invalid input checks */
+  XA_NNLIB_ARG_CHK_COND(((num_inp_dims <= 0) || (num_inp_dims > 5)), -1);
+  XA_NNLIB_ARG_CHK_COND((num_out_dims != num_inp_dims), -1);
+
+  int itr = 0;
+  for(itr=0; itr < num_inp_dims; itr++)
+  {
+    XA_NNLIB_ARG_CHK_COND((p_inp_shape[itr] <= 0), -1);
+  }
+  for(itr=0; itr < num_out_dims; itr++)
+  {
+    XA_NNLIB_ARG_CHK_COND((p_out_shape[itr] <= 0), -1);
+  }
+
+
+  /* Output shape provided must be correct based on input
+   * shape and permute values */
+  for(itr=0; itr < num_out_dims; itr++)
+  {
+    int output_dim = p_out_shape[itr];
+    int expected_dim = p_inp_shape[p_permute_vec[itr]];
+    XA_NNLIB_ARG_CHK_COND((output_dim != expected_dim), -1);
+  }
+
+  /* Pointer alignment checks */
+  XA_NNLIB_ARG_CHK_ALIGN(p_out, sizeof(WORD16), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_inp, sizeof(WORD16), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_permute_vec, sizeof(WORD32), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_out_shape, sizeof(WORD32), -1);
+  XA_NNLIB_ARG_CHK_ALIGN(p_inp_shape, sizeof(WORD32), -1);
+
+  /* Shift all dim with 1 in the outer part */
+  int eff_output_shape[5];
+  int eff_permute_vec[5];
+
+  for(int i = 0; i < num_out_dims; i++)
+  {
+    eff_output_shape[i] = p_out_shape[i];
+    eff_permute_vec[i] = p_permute_vec[i];
+  }
+
+  int one_i=num_out_dims-1, non_one_i=num_out_dims-1;
+  while(one_i > 0 && non_one_i >=0){
+    while(one_i > 0 && eff_output_shape[one_i]!=1){
+      one_i--;
+    }
+    non_one_i = one_i;
+    while(non_one_i >= 0 && eff_output_shape[non_one_i]==1)
+    {
+      non_one_i--;
+    }
+    if(one_i > 0 && non_one_i >=0){
+      int temp;
+      /*swap output_shape*/
+      {
+        temp = eff_output_shape[one_i];
+        eff_output_shape[one_i] = eff_output_shape[non_one_i];
+        eff_output_shape[non_one_i] = temp;
+      }
+      /*swap permute_vec*/
+      {
+        temp = eff_permute_vec[one_i];
+        eff_permute_vec[one_i] = eff_permute_vec[non_one_i];
+        eff_permute_vec[non_one_i] = temp;
+      }
+
+    }
+  }
+
+  /* Promoting lesser dim tensors to 5D tensors.
+   * Also updating the permute_vec and shapes as needed for optimization */
+  int p_5D_inp_shape[5] = {1, 1, 1, 1, 1};
+  int p_5D_out_shape[5] = {1, 1, 1, 1, 1};
+  int p_5D_permute_vec[5] = {0, 1, 2, 3, 4};
+
+  /* Check if any inner inp dimension is same in the output */
+  int last_dim_same = 1, last_n_same_dim = 0;
+  itr = num_inp_dims - 1;
+  while(itr >= 0)
+  {
+    last_n_same_dim = (last_dim_same && (eff_permute_vec[itr] == itr)) ? (last_n_same_dim + 1) : last_n_same_dim;
+    last_dim_same = (eff_permute_vec[itr] == itr) ? last_dim_same & 1 : last_dim_same & 0;
+    itr--;
+  }
+
+  int dims_added = 5 - num_inp_dims;
+  itr = num_inp_dims - 1;
+  int same_count = last_n_same_dim;
+  int count = 4;
+  while(itr >= 0)
+  {
+    p_5D_inp_shape[count] = (same_count > 0) ? p_5D_inp_shape[count]*p_inp_shape[itr] : p_inp_shape[itr];
+    p_5D_out_shape[count] = (same_count > 0) ? p_5D_out_shape[count]*eff_output_shape[itr] : eff_output_shape[itr];
+    same_count--;
+    itr--;
+    count = (same_count > 0) ? count : count - 1;
+  }
+
+  itr = num_inp_dims - 1;
+  same_count = (last_n_same_dim) ? num_inp_dims - (last_n_same_dim - 1) : 0;
+  count = 4;
+  while(itr >= 0)
+  {
+    p_5D_permute_vec[count] = (same_count > 0) ? eff_permute_vec[itr-(last_n_same_dim - 1)] + dims_added + last_n_same_dim - 1 : eff_permute_vec[itr] + dims_added;
+    same_count--;
+    itr--;
+    count--;
+  }
+
+  int out_dim0, out_dim1, out_dim2, out_dim3, out_dim4;
+  int inp_dim1, inp_dim2, inp_dim3, inp_dim4;
+  int inp_stride[5];
+
+  out_dim0 = p_5D_out_shape[0];
+  out_dim1 = p_5D_out_shape[1];
+  out_dim2 = p_5D_out_shape[2];
+  out_dim3 = p_5D_out_shape[3];
+  out_dim4 = p_5D_out_shape[4];
+
+  inp_dim1 = p_5D_inp_shape[1];
+  inp_dim2 = p_5D_inp_shape[2];
+  inp_dim3 = p_5D_inp_shape[3];
+  inp_dim4 = p_5D_inp_shape[4];
+
+  inp_stride[0] = inp_dim1*inp_dim2*inp_dim3*inp_dim4;
+  inp_stride[1] = inp_dim2*inp_dim3*inp_dim4;
+  inp_stride[2] = inp_dim3*inp_dim4;
+  inp_stride[3] = inp_dim4;
+  inp_stride[4] = 1;
+
+  if(last_n_same_dim)
+  {
+    int itr0, itr1, itr2, itr3, itr4;
+    WORD16 *p_inp0 = (WORD16*)p_inp;
+    for(itr0 = 0; itr0 < out_dim0; itr0++)
+    {
+      WORD16 *p_inp1 = p_inp0+(itr0*inp_stride[p_5D_permute_vec[0]]);
+#pragma loop_count min=1
+      for(itr1 = 0; itr1 < out_dim1; itr1++)
+      {
+        WORD16 *p_inp2 = p_inp1+(itr1*inp_stride[p_5D_permute_vec[1]]);
+#pragma loop_count min=1
+        for(itr2 = 0; itr2 < out_dim2; itr2++)
+        {
+          WORD16 *p_inp3 = p_inp2+(itr2*inp_stride[p_5D_permute_vec[2]]);
+#pragma loop_count min=1
+          for(itr3 = 0; itr3 < out_dim3; itr3++, p_out+=out_dim4)
+          {
+            WORD16 *p_inp4 = p_inp3+(itr3*inp_stride[p_5D_permute_vec[3]]);
+            ae_int16x4 *__restrict__ pae_i = (ae_int16x4 *)(p_inp4);
+            ae_int16x4 *__restrict__ pae_o = (ae_int16x4 *)(p_out);
+            ae_valign a_inp = AE_LA64_PP(pae_i);
+            ae_valign a_out = AE_ZALIGN64();
+            ae_int16x4 d0;
+            for(itr4 = 0; itr4 < (out_dim4 >> 2); itr4++)
+            {
+              AE_LA16X4_IP(d0, a_inp, pae_i);
+              AE_SA16X4_IP(d0, a_out, pae_o);
+            }
+            AE_SA64POS_FP(a_out, pae_o);
+            ae_int16 *__restrict__ puae_i = (ae_int16 *)(pae_i);
+            ae_int16 *__restrict__ puae_o = (ae_int16 *)(pae_o);
+#pragma loop_count max=3
+            for(itr4 = 0; itr4 < (out_dim4 & 3); itr4++)
+            {
+              puae_o[itr4] = puae_i[itr4];
+            }
+          }
+        }
+      }
+    }
+  }
+  else
+  {
+    int itr0, itr1, itr2, itr3, itr4;
+    WORD16 *p_inp0 = (WORD16*)p_inp;
+    for(itr0 = 0; itr0 < out_dim0; itr0++)
+    {
+      WORD16 *p_inp1 = p_inp0+(itr0*inp_stride[p_5D_permute_vec[0]]);
+      for(itr1 = 0; itr1 < out_dim1; itr1++)
+      {
+        WORD16 *p_inp2 = p_inp1+(itr1*inp_stride[p_5D_permute_vec[1]]);
+        for(itr2 = 0; itr2 < out_dim2; itr2++)
+        {
+          WORD16 *p_inp3 = p_inp2+(itr2*inp_stride[p_5D_permute_vec[2]]);
+          for(itr3 = 0; itr3 < out_dim3; itr3++)
+          {
+            WORD16 *p_inp4 = p_inp3+(itr3*inp_stride[p_5D_permute_vec[3]]);
+
+            ae_valign a_out = AE_ZALIGN64();
+            for(itr4 = 0; itr4 < (out_dim4 >> 2); itr4++)
+            {
+              ae_int16x4 d0, d1, d2, d3;
+              ae_int16x4 tmp0;
+
+              d1 = AE_L16_X ((ae_int16*)p_inp4, inp_stride[p_5D_permute_vec[4]]<<1);
+              d2 = AE_L16_X ((ae_int16*)p_inp4, 2*inp_stride[p_5D_permute_vec[4]]<<1);
+              d3 = AE_L16_X ((ae_int16*)p_inp4, 3*inp_stride[p_5D_permute_vec[4]]<<1);
+              AE_L16_XP(d0, (ae_int16*)p_inp4, 4*inp_stride[p_5D_permute_vec[4]]<<1);
+
+              tmp0 = AE_SEL16_6543(d0, d1);
+              tmp0 = AE_SEL16_6543(tmp0, d2);
+              tmp0 = AE_SEL16_6543(tmp0, d3);
+
+              AE_SA16X4_IP(tmp0, a_out, (ae_int16x4 *)p_out);
+            }
+            AE_SA64POS_FP(a_out, p_out);
+#pragma loop_count max=3
+            for(itr4 = 0; itr4 < (out_dim4 & 3); itr4++)
+            {
+              ae_int16x4 d0;
+              AE_L16_XP(d0, (ae_int16*)p_inp4, inp_stride[p_5D_permute_vec[4]]<<1);
+              AE_S16_0_IP(d0, (ae_int16 *)p_out, 2);
+            }
+          }
+        }
+      }
+    }
+  }
+
+  return 0;
+}
+
diff --git a/algo/kernels/reorg/hifi4/xa_nn_transpose_8.c b/algo/kernels/reorg/hifi4/xa_nn_transpose_8.c
index 350ccb3..d7311c3 100644
--- a/algo/kernels/reorg/hifi4/xa_nn_transpose_8.c
+++ b/algo/kernels/reorg/hifi4/xa_nn_transpose_8.c
@@ -74,6 +74,45 @@ WORD32 xa_nn_transpose_8_8(WORD8 * __restrict__ p_out
   XA_NNLIB_ARG_CHK_ALIGN(p_out_shape, sizeof(WORD32), -1);
   XA_NNLIB_ARG_CHK_ALIGN(p_inp_shape, sizeof(WORD32), -1);
 
+  /* Shift all dim with 1 in the outer part */
+  int eff_output_shape[5];
+  int eff_permute_vec[5];
+
+  for(int i = 0; i < num_out_dims; i++)
+  {
+    eff_output_shape[i] = p_out_shape[i];
+    eff_permute_vec[i] = p_permute_vec[i];
+  }
+
+  int one_i=num_out_dims-1, non_one_i=num_out_dims-1;
+  while(one_i > 0 && non_one_i >=0){
+    while(one_i > 0 && eff_output_shape[one_i]!=1){
+      one_i--;
+    }
+    non_one_i = one_i;
+    while(non_one_i >= 0 && eff_output_shape[non_one_i]==1)
+    {
+      non_one_i--;
+    }
+    if(one_i > 0 && non_one_i >=0){
+      int temp;
+      /*swap output_shape*/
+      {
+        temp = eff_output_shape[one_i];
+        eff_output_shape[one_i] = eff_output_shape[non_one_i];
+        eff_output_shape[non_one_i] = temp;
+      }
+      /*swap permute_vec*/
+      {
+        temp = eff_permute_vec[one_i];
+        eff_permute_vec[one_i] = eff_permute_vec[non_one_i];
+        eff_permute_vec[non_one_i] = temp;
+      }
+
+    }
+  }
+
+
   /* Promoting lesser dim tensors to 5D tensors. 
    * Also updating the permute_vec and shapes as needed for optimization */
   int p_5D_inp_shape[5] = {1, 1, 1, 1, 1};
@@ -85,8 +124,8 @@ WORD32 xa_nn_transpose_8_8(WORD8 * __restrict__ p_out
   itr = num_inp_dims - 1;
   while(itr >= 0)
   {
-    last_n_same_dim = (last_dim_same && (p_permute_vec[itr] == itr)) ? (last_n_same_dim + 1) : last_n_same_dim;
-    last_dim_same = (p_permute_vec[itr] == itr) ? last_dim_same & 1 : last_dim_same & 0;
+    last_n_same_dim = (last_dim_same && (eff_permute_vec[itr] == itr)) ? (last_n_same_dim + 1) : last_n_same_dim;
+    last_dim_same = (eff_permute_vec[itr] == itr) ? last_dim_same & 1 : last_dim_same & 0;
     itr--;
   }
   
@@ -97,7 +136,7 @@ WORD32 xa_nn_transpose_8_8(WORD8 * __restrict__ p_out
   while(itr >= 0)
   {
     p_5D_inp_shape[count] = (same_count > 0) ? p_5D_inp_shape[count]*p_inp_shape[itr] : p_inp_shape[itr];
-    p_5D_out_shape[count] = (same_count > 0) ? p_5D_out_shape[count]*p_out_shape[itr] : p_out_shape[itr];
+    p_5D_out_shape[count] = (same_count > 0) ? p_5D_out_shape[count]*eff_output_shape[itr] : eff_output_shape[itr];
     same_count--;
     itr--;
     count = (same_count > 0) ? count : count - 1;
@@ -108,7 +147,7 @@ WORD32 xa_nn_transpose_8_8(WORD8 * __restrict__ p_out
   count = 4;
   while(itr >= 0)
   {
-    p_5D_permute_vec[count] = (same_count > 0) ? p_permute_vec[itr-(last_n_same_dim - 1)] + dims_added + last_n_same_dim - 1 : p_permute_vec[itr] + dims_added;
+    p_5D_permute_vec[count] = (same_count > 0) ? eff_permute_vec[itr-(last_n_same_dim - 1)] + dims_added + last_n_same_dim - 1 : eff_permute_vec[itr] + dims_added;
     same_count--;
     itr--;
     count--;
diff --git a/include/nnlib/xa_nnlib_kernels_api.h b/include/nnlib/xa_nnlib_kernels_api.h
index c8a0bbb..569cc29 100644
--- a/include/nnlib/xa_nnlib_kernels_api.h
+++ b/include/nnlib/xa_nnlib_kernels_api.h
@@ -22,7 +22,6 @@
 #ifndef __XA_NNLIB_KERNELS_API_H__
 #define __XA_NNLIB_KERNELS_API_H__
 
-
 /**
  * @file xa_nnlib_kernels_api.h
  * @brief This file gives the API definition for the HiFi NNLIB
@@ -128,6 +127,38 @@
 {
 #endif
 
+#ifdef ENABLE_SCRATCH_SIZE_API_ONLY
+
+#if defined(hifi5)
+#define get_softmax_scratch_size                get_softmax_scratch_size_hifi5
+#define xa_nn_avgpool_getsize                   xa_nn_avgpool_getsize_hifi5
+#define xa_nn_conv2d_depthwise_getsize          xa_nn_conv2d_depthwise_getsize_hifi5
+#define xa_nn_conv2d_getsize                    xa_nn_conv2d_getsize_hifi5
+#define xa_nn_conv2d_std_getsize                xa_nn_conv2d_std_getsize_hifi5
+#define xa_nn_conv2d_std_getsize_sym4s          xa_nn_conv2d_std_getsize_sym4s_hifi5
+#define xa_nn_dilated_conv2d_depthwise_getsize  xa_nn_dilated_conv2d_depthwise_getsize_hifi5
+#define xa_nn_dilated_conv2d_std_getsize        xa_nn_dilated_conv2d_std_getsize_hifi5
+#define xa_nn_maxpool_getsize                   xa_nn_maxpool_getsize_hifi5
+#define xa_nn_reduce_getsize_nhwc               xa_nn_reduce_getsize_nhwc_hifi5
+#define xa_nn_transpose_conv_getsize            xa_nn_transpose_conv_getsize_hifi5
+
+#elif defined(hifi4)
+#define get_softmax_scratch_size                get_softmax_scratch_size_hifi4
+#define xa_nn_avgpool_getsize                   xa_nn_avgpool_getsize_hifi4
+#define xa_nn_conv2d_depthwise_getsize          xa_nn_conv2d_depthwise_getsize_hifi4
+#define xa_nn_conv2d_getsize                    xa_nn_conv2d_getsize_hifi4
+#define xa_nn_conv2d_std_getsize                xa_nn_conv2d_std_getsize_hifi4
+#define xa_nn_conv2d_std_getsize_sym4s          xa_nn_conv2d_std_getsize_sym4s_hifi4
+#define xa_nn_dilated_conv2d_depthwise_getsize  xa_nn_dilated_conv2d_depthwise_getsize_hifi4
+#define xa_nn_dilated_conv2d_std_getsize        xa_nn_dilated_conv2d_std_getsize_hifi4
+#define xa_nn_maxpool_getsize                   xa_nn_maxpool_getsize_hifi4
+#define xa_nn_reduce_getsize_nhwc               xa_nn_reduce_getsize_nhwc_hifi4
+#define xa_nn_transpose_conv_getsize            xa_nn_transpose_conv_getsize_hifi4
+
+#endif
+
+#endif /* #ifdef ENABLE_SCRATCH_SIZE_API_ONLY */
+
 	WORD32 xa_nn_matXvec_16x16_16(
 			WORD16 * __restrict__ p_out,                /*!< [out] 16b result: rows x 1 */
 			WORD16 * __restrict__ p_mat1,               /*!< [in] 16b mat1: rows x cols1 */
@@ -817,9 +848,9 @@
 			int filter_height, int filter_width,
 			int output_height, int output_width,
 			int num_elements,
-      int input_offset, int output_offset,
+			int input_offset, int output_offset,
 			int *output_shift, int *output_multiplier,
-			WORD32* scratch_buffer);
+			void* scratch_buffer);
 
 	WORD32 xa_nn_transpose_conv_sym8sxsym16s(
 			WORD16* output_data,
@@ -834,7 +865,7 @@
 			int output_height, int output_width,
 			int num_elements,
 			int *output_shift, int *output_multiplier,
-			WORD64* scratch_buffer);
+			void* scratch_buffer);
 
     WORD32 xa_nn_transpose_conv_f32(
             FLOAT32* output_data,
@@ -848,7 +879,7 @@
             int filter_height, int filter_width,
             int output_height, int output_width,
             int num_elements,
-            FLOAT32* scratch_buffer);
+            void* scratch_buffer);
 
 	WORD32 xa_nn_conv1d_std_getsize(
 			WORD32 kernel_height,
@@ -1676,6 +1707,12 @@
 			WORD32  vec_length,
 			pVOID   p_scratch);
 
+  WORD32 xa_nn_vec_softmax_sym16s_16( WORD16 * __restrict__ p_out,
+      const   WORD16 * __restrict__ p_vec,
+      WORD32  input_beta_left_shift,
+      WORD32  input_beta_multiplier,
+      WORD32  vec_length);
+
 	WORD32 xa_nn_vec_sigmoid_asym8u_asym8u(UWORD8 *p_out,
 			const UWORD8 *p_vec,
 			WORD32 zero_point,
@@ -2571,6 +2608,14 @@
             WORD32 clip,
             WORD32 num_elms);
 
+  WORD32 xa_nn_elm_requantize_asym8u_asym8s(WORD8 * __restrict__ p_out,
+      const UWORD8 * __restrict__ p_inp,
+      WORD32 inp_zero_bias,
+      WORD32 out_zero_bias,
+      WORD32 out_shift,
+      WORD32 out_multiplier,
+      WORD32 num_elm);
+
 	WORD32 xa_nn_elm_requantize_asym16s_asym8s(WORD8 * __restrict__ p_out,
 			const WORD16 * __restrict__ p_inp,
 			WORD32  inp_zero_bias,
@@ -3039,6 +3084,14 @@
                     ,WORD32 num_out_dims
                     ,WORD32 num_inp_dims);
 
+  WORD32 xa_nn_transpose_16_16(WORD16 * __restrict__ p_out
+                    ,const WORD32 *const p_out_shape
+                    ,const WORD16 * __restrict__ p_inp
+                    ,const WORD32 *const p_inp_shape
+                    ,const WORD32 * __restrict__ p_permute_vec
+                    ,WORD32 num_out_dims
+                    ,WORD32 num_inp_dims);
+
   WORD32 xa_nn_batch_norm_3D_8_8(WORD8 * __restrict__ p_out
                     ,const WORD8 * __restrict__ p_inp
                     ,const WORD16 * __restrict__ p_alpha
@@ -3083,6 +3136,23 @@ WORD32 xa_nn_resize_nearest_neighbour_8_8(pWORD8 __restrict__ p_out
                     ,FLOAT32 width_offset
                     ,WORD32  align_corners);
 
+WORD32 xa_nn_concat_8_8(WORD8 * __restrict__ p_out
+        ,const WORD32 *const p_out_shape
+        ,const WORD8 **p_inps
+        ,const WORD32 *const *pp_inps_shape
+        ,WORD32 num_out_dims
+        ,WORD32 num_inp
+        ,WORD32 num_inp_dims
+        ,WORD32 axis);
+
+WORD32 xa_nn_split_v_8_8(WORD8 ** __restrict__ pp_outs
+                        ,const WORD32 *const *pp_outs_shape
+                        ,const WORD8 *p_inp
+                        ,const WORD32 *const p_inp_shape
+                        ,WORD32 num_out
+                        ,WORD32 num_out_dims
+                        ,WORD32 num_inp_dims
+                        ,WORD32 axis);
 
 	/* Mapping the functions names from previous naming convension for backward compatibility */
 #define xa_nn_matXvec_asym8xasym8_asym8 xa_nn_matXvec_asym8uxasym8u_asym8u
diff --git a/include/nnlib/xa_nnlib_standards.h b/include/nnlib/xa_nnlib_standards.h
index 88c619a..fb91967 100644
--- a/include/nnlib/xa_nnlib_standards.h
+++ b/include/nnlib/xa_nnlib_standards.h
@@ -22,7 +22,9 @@
 #ifndef __STANDARDS_H__
 #define __STANDARDS_H__
 
+#ifndef ENABLE_SCRATCH_SIZE_API_ONLY
 #include <xtensa/config/core-isa.h>
+#endif
 
 #if defined(__cplusplus)
 extern "C"
@@ -151,7 +153,7 @@ typedef struct _xa_nnlib_shape_t{
 
 enum xa_error_severity {
   xa_severity_nonfatal = 0,
-  xa_severity_fatal    = (int)0xffffffff
+  xa_severity_fatal    = (unsigned int)0xffffffff
 };
 
 enum xa_error_class {
